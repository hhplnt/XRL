{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc70cbdd",
   "metadata": {},
   "source": [
    "# Linear network training and replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38c1b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\yawning_titan\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from yawning_titan.networks.node import Node\n",
    "from yawning_titan.networks.network import Network\n",
    "\n",
    "import time\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOMlp\n",
    "\n",
    "from yawning_titan.envs.generic.core.blue_interface import BlueInterface\n",
    "from yawning_titan.envs.generic.core.red_interface import RedInterface\n",
    "from yawning_titan.envs.generic.generic_env import GenericNetworkEnv\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "from yawning_titan.envs.generic.core.network_interface import NetworkInterface\n",
    "from yawning_titan.networks.network_db import default_18_node_network\n",
    "import yawning_titan.game_modes\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "\n",
    "## Using DB to retrieve game modes\n",
    "from yawning_titan.game_modes.game_mode_db import GameModeDB, GameModeSchema\n",
    "from yawning_titan.db.doc_metadata import DocMetadataSchema\n",
    "db = GameModeDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab85736a",
   "metadata": {},
   "source": [
    "### Network Setup\n",
    "\n",
    "Network of 5 nodes, connected in a line. Node 1 is the entry node and node 5 is the high value target. All nodes begin with an initial vulnerability of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b38f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUID                                  Name    High Value Node    Entry Node      Vulnerability  Position (x,y)\n",
      "------------------------------------  ------  -----------------  ------------  ---------------  ----------------\n",
      "b75a9424-5c21-4379-abba-6296ad6e49b6  PC 1    False              True                      0.8  -1.00, 0.01\n",
      "3afd6bca-8b71-451f-ac05-23c62869f31d  PC 2    False              False                     0.8  -0.50, 0.01\n",
      "f2882641-56a0-43c2-b28d-b0deed647b19  PC 3    False              False                     0.8  0.00, 0.01\n",
      "113e6418-c7d4-4f9d-830c-352cb2216bc2  PC 4    False              False                     0.8  0.50, 0.01\n",
      "f08e2045-55b8-4c23-8d11-fd85d107f15f  PC 5    True               False                     0.8  1.00, 0.01\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Network\n",
    "network = Network()\n",
    "\n",
    "# Instantiate the Node's and add them to the Network\n",
    " \n",
    "pc_1 = Node(\"PC 1\")\n",
    "network.add_node(pc_1)\n",
    "pc_1.x_pos = -1.00\n",
    "pc_1.y_pos = 0.01\n",
    "pc_1.entry_node = True\n",
    "pc_1.vulnerability = 0.8\n",
    "\n",
    "pc_2 = Node(\"PC 2\")\n",
    "network.add_node(pc_2)\n",
    "pc_2.x_pos = -0.50\n",
    "pc_2.y_pos = 0.01\n",
    "pc_2.vulnerability = 0.8\n",
    "\n",
    "pc_3 = Node(\"PC 3\")\n",
    "network.add_node(pc_3)\n",
    "pc_3.x_pos = 0.00\n",
    "pc_3.y_pos = 0.01\n",
    "pc_3.vulnerability = 0.8\n",
    "\n",
    "pc_4 = Node(\"PC 4\")\n",
    "network.add_node(pc_4)\n",
    "pc_4.x_pos = 0.50\n",
    "pc_4.y_pos = 0.01\n",
    "pc_4.vulnerability = 0.8\n",
    "\n",
    "pc_5 = Node(\"PC 5\")\n",
    "network.add_node(pc_5)\n",
    "pc_5.x_pos = 1.00\n",
    "pc_5.y_pos = 0.01\n",
    "pc_5.high_value_node = True\n",
    "pc_5.vulnerability = 0.8\n",
    "\n",
    "\n",
    "# Add the edges between Node's\n",
    "network.add_edge(pc_1, pc_2)\n",
    "network.add_edge(pc_2, pc_3)\n",
    "network.add_edge(pc_3, pc_4)\n",
    "network.add_edge(pc_4, pc_5)\n",
    "\n",
    "\n",
    "\n",
    "# Reset the entry nodes, high value nodes, and vulnerability scores by calling .setup()\n",
    "# network.reset()\n",
    "\n",
    "# View the Networks Node Details\n",
    "network.show(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee45f47",
   "metadata": {},
   "source": [
    "## Creating environments and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d5e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d048b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name               author              locked    uuid\n",
      "-----------------  ------------------  --------  ------------------------------------\n",
      "DCBO Agent Config  dstl/YAWNING-TITAN  True      bac2cb9d-b24b-426c-88a5-5edd0c2de413\n",
      "Default Game Mode  dstl/YAWNING-TITAN  True      900a704f-6271-4994-ade7-40b74d3199b1\n",
      "Low skill red      dstl/YAWNING-TITAN  True      3ccd9988-8781-4c3e-9c75-44cc987ae6af\n",
      "simple_mode        Hannah Harrison     False     919da33c-7bc9-4d29-99eb-097a7e9bb016\n",
      "no_zero_day        Hannah Harrison     False     fe76bb6c-4806-41af-aaf3-ac78d2942021\n"
     ]
    }
   ],
   "source": [
    "db.show(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61acc85f",
   "metadata": {},
   "source": [
    "### Simple mode:\n",
    "simple_mode \n",
    "\n",
    "#### Red: \n",
    "\n",
    "Can attack from any node it controls \n",
    "\n",
    "Only basic attack and zero day enabled. \n",
    "\n",
    "Starts with one zero day attack and gains another every 5 timesteps \n",
    "\n",
    "No natural spreading \n",
    "\n",
    "Target mechanism: (prioritise vulnerable nodes – sorts nodes it can attack and selects most vulnerable) changed to random \n",
    "\n",
    "#### Blue: \n",
    "\n",
    "Action set: reduce vulnerability, restore node (considering taking away restore node as kept winning by immediately restoring first node red attacks) \n",
    "\n",
    "100% chance of immediately discovering intrusions \n",
    "\n",
    "#### Game rules: \n",
    "\n",
    "Max steps: 30, no grace period \n",
    "\n",
    "Blue loss if high value node lost\n",
    "\n",
    "#### Observation space \n",
    "\n",
    "Compromised status, vulnerability scores and node connections. I removed special nodes as this is kind of inferred by the rewards and adds a lot of dimensions.\n",
    "\n",
    "#### Rewards \n",
    "\n",
    "-100 for loss, 100 for reaching end.  \n",
    "\n",
    "Negative reward reduced for closer fails – if closer to end of timesteps \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543a6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_mode = db.get(\"919da33c-7bc9-4d29-99eb-097a7e9bb016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52138506",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build network interface\n",
    "s_network_interface = NetworkInterface(game_mode=simple_mode, network=network)\n",
    "\n",
    "## Name agents\n",
    "red = RedInterface(s_network_interface)\n",
    "blue = BlueInterface(s_network_interface)\n",
    "\n",
    "## Create environment\n",
    "s_env = GenericNetworkEnv(red, blue, s_network_interface)\n",
    "\n",
    "## Check compliant with OpenAI gym\n",
    "check_env(s_env, warn=True)\n",
    "_ = s_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb49df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/ppo_linear_tensorboard1/PPO_14\n",
      "Eval num_timesteps=1000, episode_reward=-54.84 +/- 18.95\n",
      "Episode length: 18.20 +/- 6.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | -54.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-75.74 +/- 12.81\n",
      "Episode length: 13.40 +/- 4.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.4     |\n",
      "|    mean_reward     | -75.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16       |\n",
      "|    ep_rew_mean     | -57.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 415      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-64.12 +/- 24.81\n",
      "Episode length: 17.20 +/- 8.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.2        |\n",
      "|    mean_reward          | -64.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012756627 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | 0.00162     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 573         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 1.47e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-72.82 +/- 9.70\n",
      "Episode length: 13.00 +/- 3.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -72.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.2     |\n",
      "|    ep_rew_mean     | -58.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 485      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-66.67 +/- 17.57\n",
      "Episode length: 14.40 +/- 5.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | -66.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011365324 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | 0.000496    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 838         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 1.26e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-71.58 +/- 8.16\n",
      "Episode length: 13.00 +/- 2.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -71.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.8     |\n",
      "|    ep_rew_mean     | -65.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 513      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-31.25 +/- 55.06\n",
      "Episode length: 22.40 +/- 6.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.4        |\n",
      "|    mean_reward          | -31.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010511356 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.26       |\n",
      "|    explained_variance   | 0.000168    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 256         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 753         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-59.72 +/- 14.14\n",
      "Episode length: 21.00 +/- 6.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21       |\n",
      "|    mean_reward     | -59.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | -52.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 528      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-70.22 +/- 4.70\n",
      "Episode length: 15.00 +/- 2.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15           |\n",
      "|    mean_reward          | -70.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073443768 |\n",
      "|    clip_fraction        | 0.0828       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.24        |\n",
      "|    explained_variance   | 9.09e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 386          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00535     |\n",
      "|    value_loss           | 754          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-34.90 +/- 55.16\n",
      "Episode length: 20.20 +/- 5.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -34.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.2     |\n",
      "|    ep_rew_mean     | -52.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-32.74 +/- 50.50\n",
      "Episode length: 21.00 +/- 6.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21          |\n",
      "|    mean_reward          | -32.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010007665 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.21       |\n",
      "|    explained_variance   | 0.00214     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 430         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 881         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-61.38 +/- 17.74\n",
      "Episode length: 17.80 +/- 6.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | -61.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -39      |\n",
      "| time/              |          |\n",
      "|    fps             | 544      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-62.35 +/- 13.69\n",
      "Episode length: 16.80 +/- 4.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.8        |\n",
      "|    mean_reward          | -62.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003067175 |\n",
      "|    clip_fraction        | 0.000391    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.21       |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 584         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    value_loss           | 1.03e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-59.30 +/- 11.13\n",
      "Episode length: 17.60 +/- 4.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.6     |\n",
      "|    mean_reward     | -59.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.4     |\n",
      "|    ep_rew_mean     | -48.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 547      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-5.12 +/- 70.19\n",
      "Episode length: 23.20 +/- 6.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.2        |\n",
      "|    mean_reward          | -5.12       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004832075 |\n",
      "|    clip_fraction        | 0.000342    |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.2        |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 330         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    value_loss           | 811         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-54.35 +/- 12.10\n",
      "Episode length: 18.80 +/- 4.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | -54.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | -35.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 550      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-35.26 +/- 55.84\n",
      "Episode length: 17.80 +/- 6.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.8        |\n",
      "|    mean_reward          | -35.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004573076 |\n",
      "|    clip_fraction        | 0.00825     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.18       |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 403         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    value_loss           | 822         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-45.47 +/- 63.91\n",
      "Episode length: 15.00 +/- 7.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | -45.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | -37.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 554      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-40.26 +/- 56.51\n",
      "Episode length: 18.20 +/- 6.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.2         |\n",
      "|    mean_reward          | -40.3        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045709563 |\n",
      "|    clip_fraction        | 0.00361      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.16        |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 538          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00497     |\n",
      "|    value_loss           | 976          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-31.87 +/- 56.96\n",
      "Episode length: 19.60 +/- 6.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.6     |\n",
      "|    mean_reward     | -31.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -23.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 558      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-0.33 +/- 63.79\n",
      "Episode length: 23.00 +/- 5.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23          |\n",
      "|    mean_reward          | -0.334      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006464051 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | 0.449       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 585         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00671    |\n",
      "|    value_loss           | 1.19e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=-3.18 +/- 71.05\n",
      "Episode length: 23.00 +/- 6.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23       |\n",
      "|    mean_reward     | -3.18    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -17.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=0.54 +/- 69.28\n",
      "Episode length: 22.40 +/- 7.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.4         |\n",
      "|    mean_reward          | 0.543        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074296272 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.13        |\n",
      "|    explained_variance   | 0.419        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 573          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00822     |\n",
      "|    value_loss           | 1.26e+03     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=-2.52 +/- 67.62\n",
      "Episode length: 24.20 +/- 6.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.2     |\n",
      "|    mean_reward     | -2.52    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -12      |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-18.51 +/- 54.59\n",
      "Episode length: 23.80 +/- 5.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 23.8         |\n",
      "|    mean_reward          | -18.5        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068568503 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.07        |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 650          |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0084      |\n",
      "|    value_loss           | 1.16e+03     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-62.35 +/- 18.71\n",
      "Episode length: 16.00 +/- 7.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | -62.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.7     |\n",
      "|    ep_rew_mean     | 3.72     |\n",
      "| time/              |          |\n",
      "|    fps             | 556      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=6.77 +/- 66.22\n",
      "Episode length: 24.20 +/- 5.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | 6.77        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010256683 |\n",
      "|    clip_fraction        | 0.0758      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.02       |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 592         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 1.27e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28000, episode_reward=31.17 +/- 63.79\n",
      "Episode length: 26.80 +/- 4.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.8     |\n",
      "|    mean_reward     | 31.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.9     |\n",
      "|    ep_rew_mean     | 4.93     |\n",
      "| time/              |          |\n",
      "|    fps             | 555      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=51.78 +/- 58.54\n",
      "Episode length: 27.40 +/- 5.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.4        |\n",
      "|    mean_reward          | 51.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008867405 |\n",
      "|    clip_fraction        | 0.0539      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.41        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 546         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 1.15e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-1.97 +/- 66.79\n",
      "Episode length: 22.60 +/- 6.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -1.97    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | 16.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 551      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=32.16 +/- 68.37\n",
      "Episode length: 25.60 +/- 5.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.6        |\n",
      "|    mean_reward          | 32.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012717855 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.361       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 489         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 1.22e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=27.68 +/- 62.45\n",
      "Episode length: 25.60 +/- 5.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.6     |\n",
      "|    mean_reward     | 27.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.2     |\n",
      "|    ep_rew_mean     | 30.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 548      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=0.69 +/- 69.24\n",
      "Episode length: 22.20 +/- 7.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.2        |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008406285 |\n",
      "|    clip_fraction        | 0.0515      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.89       |\n",
      "|    explained_variance   | 0.347       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 410         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0099     |\n",
      "|    value_loss           | 1.02e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=56.29 +/- 46.29\n",
      "Episode length: 28.20 +/- 3.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.2     |\n",
      "|    mean_reward     | 56.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.3     |\n",
      "|    ep_rew_mean     | 42.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 548      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=35000, episode_reward=32.26 +/- 61.80\n",
      "Episode length: 25.20 +/- 6.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.2        |\n",
      "|    mean_reward          | 32.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010767738 |\n",
      "|    clip_fraction        | 0.0802      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 408         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 1.02e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=27.28 +/- 72.24\n",
      "Episode length: 23.80 +/- 7.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.8     |\n",
      "|    mean_reward     | 27.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.1     |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 546      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=89.41 +/- 5.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 89.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010398944 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.321       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 416         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 889         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=89.81 +/- 4.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 89.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.8     |\n",
      "|    ep_rew_mean     | 66.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 544      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=91.49 +/- 7.42\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 91.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010082174 |\n",
      "|    clip_fraction        | 0.0955      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 484         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=13.63 +/- 66.61\n",
      "Episode length: 24.20 +/- 6.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.2     |\n",
      "|    mean_reward     | 13.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.6     |\n",
      "|    ep_rew_mean     | 71.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 543      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=66.65 +/- 54.26\n",
      "Episode length: 28.00 +/- 4.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 28          |\n",
      "|    mean_reward          | 66.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008977548 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 241         |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 617         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=60.27 +/- 56.68\n",
      "Episode length: 27.00 +/- 6.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27       |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=32.00 +/- 74.98\n",
      "Episode length: 23.60 +/- 8.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.6     |\n",
      "|    mean_reward     | 32       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.2     |\n",
      "|    ep_rew_mean     | 79.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 542      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=91.03 +/- 6.60\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 91          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011928596 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 236         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00977    |\n",
      "|    value_loss           | 271         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=60.08 +/- 64.24\n",
      "Episode length: 26.60 +/- 6.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.6     |\n",
      "|    mean_reward     | 60.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | 81.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 543      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=60.87 +/- 58.98\n",
      "Episode length: 27.00 +/- 6.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27          |\n",
      "|    mean_reward          | 60.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010611737 |\n",
      "|    clip_fraction        | 0.0827      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 357         |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 365         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=47000, episode_reward=97.25 +/- 8.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 97.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.9     |\n",
      "|    ep_rew_mean     | 78.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 541      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=96.76 +/- 3.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009043428 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.2        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 495         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=98.29 +/- 5.76\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 98.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29       |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=64.27 +/- 68.01\n",
      "Episode length: 26.00 +/- 8.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26          |\n",
      "|    mean_reward          | 64.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009387093 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 313         |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00689    |\n",
      "|    value_loss           | 431         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=98.57 +/- 4.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 98.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.8     |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 540      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=91.61 +/- 5.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 91.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009298938 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 160         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 441         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=91.19 +/- 8.72\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 91.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29       |\n",
      "|    ep_rew_mean     | 78.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=98.53 +/- 2.76\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 98.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 54000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00539909 |\n",
      "|    clip_fraction        | 0.055      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.165      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 109        |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.00497   |\n",
      "|    value_loss           | 366        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=93.58 +/- 6.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 93.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.5     |\n",
      "|    ep_rew_mean     | 85.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=92.55 +/- 4.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 92.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008347895 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 304         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00896    |\n",
      "|    value_loss           | 298         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=96.78 +/- 7.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 96.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 91.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=58000, episode_reward=96.83 +/- 4.66\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 96.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009314181 |\n",
      "|    clip_fraction        | 0.0956      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.77        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00656    |\n",
      "|    value_loss           | 153         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=100.11 +/- 2.29\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 100      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 92.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 110      |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=76.74 +/- 48.75\n",
      "Episode length: 29.60 +/- 0.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.6        |\n",
      "|    mean_reward          | 76.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014036774 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 268         |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=95.96 +/- 2.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 93.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=67.72 +/- 57.25\n",
      "Episode length: 27.40 +/- 5.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.4        |\n",
      "|    mean_reward          | 67.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014774807 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.1        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00728    |\n",
      "|    value_loss           | 80.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=99.73 +/- 3.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 99.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 96.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=104.58 +/- 3.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 105        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 64000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00923688 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.25      |\n",
      "|    explained_variance   | 0.218      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 97.1       |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.00954   |\n",
      "|    value_loss           | 187        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=106.08 +/- 1.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 106      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 98.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 121      |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=101.52 +/- 1.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 102         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009660082 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.27        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00883    |\n",
      "|    value_loss           | 81.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=101.85 +/- 3.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 102      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 96       |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=103.83 +/- 3.51\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 104         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005668906 |\n",
      "|    clip_fraction        | 0.0645      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62          |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    value_loss           | 276         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=69000, episode_reward=104.27 +/- 1.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 98.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 539      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=103.32 +/- 3.53\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 103          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035270955 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.294        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 83.5         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    value_loss           | 186          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=102.49 +/- 3.23\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 102      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 103      |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=106.73 +/- 2.23\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 107         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007968318 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.408       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.41        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00744    |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73000, episode_reward=103.39 +/- 5.33\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 103      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=105.73 +/- 2.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 106          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 74000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069206017 |\n",
      "|    clip_fraction        | 0.0817       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.272        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 60.2         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00636     |\n",
      "|    value_loss           | 126          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=105.40 +/- 1.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 105      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 100      |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=108.20 +/- 1.19\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 108         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009437808 |\n",
      "|    clip_fraction        | 0.0873      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.358       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.47        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    value_loss           | 80.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=106.67 +/- 2.08\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    fps             | 538      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=106.95 +/- 2.01\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 30        |\n",
      "|    mean_reward          | 107       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 78000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0096851 |\n",
      "|    clip_fraction        | 0.119     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.06     |\n",
      "|    explained_variance   | 0.357     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.48      |\n",
      "|    n_updates            | 380       |\n",
      "|    policy_gradient_loss | -0.00931  |\n",
      "|    value_loss           | 51.4      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=74.33 +/- 61.35\n",
      "Episode length: 27.20 +/- 5.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27.2     |\n",
      "|    mean_reward     | 74.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=80000, episode_reward=108.35 +/- 1.36\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 108         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010708781 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 5.36        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=81000, episode_reward=108.12 +/- 1.66\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 537      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=105.80 +/- 2.11\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 106         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008729035 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.33        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 5.03        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=107.81 +/- 2.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    fps             | 536      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 83968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=107.62 +/- 2.51\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 108         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009973068 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 250         |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00594    |\n",
      "|    value_loss           | 209         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=108.50 +/- 1.28\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=86000, episode_reward=107.15 +/- 2.50\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.3     |\n",
      "|    ep_rew_mean     | 98.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=107.62 +/- 2.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 108          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 87000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040526474 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.984       |\n",
      "|    explained_variance   | 0.192        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.68         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 272          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=106.78 +/- 2.72\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 534      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=84.02 +/- 46.96\n",
      "Episode length: 29.20 +/- 1.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.2        |\n",
      "|    mean_reward          | 84          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 89000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010614948 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.956      |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.15        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    value_loss           | 6.18        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=106.80 +/- 2.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=107.25 +/- 2.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 107          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 91000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057201046 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.968       |\n",
      "|    explained_variance   | 0.336        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.95         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00698     |\n",
      "|    value_loss           | 71.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=92000, episode_reward=107.00 +/- 2.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    fps             | 535      |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=107.46 +/- 1.62\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 107         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 93000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005891026 |\n",
      "|    clip_fraction        | 0.0829      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.51        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00499    |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=106.64 +/- 2.60\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    fps             | 534      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=83.55 +/- 51.30\n",
      "Episode length: 28.60 +/- 2.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 28.6         |\n",
      "|    mean_reward          | 83.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 95000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064280434 |\n",
      "|    clip_fraction        | 0.0687       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | 0.34         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.71         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00408     |\n",
      "|    value_loss           | 51.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=107.69 +/- 1.08\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 534      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=107.69 +/- 3.46\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 108         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 97000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007315013 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.918      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 124         |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00389    |\n",
      "|    value_loss           | 66.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=107.48 +/- 1.60\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 107      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 533      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=109.50 +/- 1.14\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 99000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010906557 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.07        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    value_loss           | 4.76        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=108.63 +/- 1.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 533      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=109.09 +/- 1.01\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 109          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 101000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066453675 |\n",
      "|    clip_fraction        | 0.0877       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.922       |\n",
      "|    explained_variance   | 0.192        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.54         |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    value_loss           | 134          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=108.61 +/- 2.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    fps             | 532      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=103000, episode_reward=108.20 +/- 1.55\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 108          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 103000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071829525 |\n",
      "|    clip_fraction        | 0.0959       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.92        |\n",
      "|    explained_variance   | 0.337        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.99         |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00532     |\n",
      "|    value_loss           | 75.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=107.86 +/- 3.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 532      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=107.79 +/- 3.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 108          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 105000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062238476 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.88        |\n",
      "|    explained_variance   | 0.287        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 112          |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00544     |\n",
      "|    value_loss           | 76           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=109.33 +/- 1.54\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    fps             | 531      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=108.59 +/- 1.24\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 107000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006147965 |\n",
      "|    clip_fraction        | 0.0669      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.89       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.6        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00392    |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=107.87 +/- 2.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    fps             | 531      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=109.60 +/- 0.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 109000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005390028 |\n",
      "|    clip_fraction        | 0.0799      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.1         |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00437    |\n",
      "|    value_loss           | 80.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=109.37 +/- 1.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    fps             | 530      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=106.86 +/- 1.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 107         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 111000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008116173 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.858      |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=76.72 +/- 64.50\n",
      "Episode length: 26.80 +/- 6.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.8     |\n",
      "|    mean_reward     | 76.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 531      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=109.17 +/- 1.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 113000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007699845 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.854      |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 76.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=114000, episode_reward=109.04 +/- 2.29\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 530      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 215      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=109.73 +/- 0.69\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 115000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059214556 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.854       |\n",
      "|    explained_variance   | 0.306        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.74         |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00246     |\n",
      "|    value_loss           | 73.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=116000, episode_reward=109.00 +/- 0.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 530      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=87.05 +/- 43.94\n",
      "Episode length: 29.80 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 29.8         |\n",
      "|    mean_reward          | 87.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 117000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042004907 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.839       |\n",
      "|    explained_variance   | 0.33         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.78         |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 71.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=109.65 +/- 1.22\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 531      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=109.74 +/- 0.76\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 119000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005778648 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.848      |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00442    |\n",
      "|    value_loss           | 72.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=110.08 +/- 0.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 530      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=107.93 +/- 3.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 108         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 121000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004743576 |\n",
      "|    clip_fraction        | 0.0973      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.844      |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    value_loss           | 78.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=107.97 +/- 0.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.5     |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 529      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 231      |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=106.44 +/- 2.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 106         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 123000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007828072 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.809      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    value_loss           | 3.57        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=110.52 +/- 0.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 111      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 530      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=125000, episode_reward=109.76 +/- 0.70\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 125000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077389884 |\n",
      "|    clip_fraction        | 0.0551       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.807       |\n",
      "|    explained_variance   | 0.138        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.1         |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    value_loss           | 96.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=109.57 +/- 1.31\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 530      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=108.81 +/- 1.05\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 127000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006534476 |\n",
      "|    clip_fraction        | 0.0913      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.826      |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.9        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 74.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=109.20 +/- 1.40\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=108.70 +/- 2.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 529      |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 243      |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=110.30 +/- 1.17\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 130000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040001553 |\n",
      "|    clip_fraction        | 0.079        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.778       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.74         |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 3.1          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=109.54 +/- 0.58\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 529      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=110.36 +/- 0.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 132000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069494024 |\n",
      "|    clip_fraction        | 0.0893       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.798       |\n",
      "|    explained_variance   | 0.892        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.54         |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    value_loss           | 2.78         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=108.27 +/- 2.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 529      |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=109.32 +/- 1.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 134000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012305479 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.796      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00489    |\n",
      "|    value_loss           | 2.28        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=108.57 +/- 2.02\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 529      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 255      |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=109.99 +/- 1.08\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 136000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004222372 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.791      |\n",
      "|    explained_variance   | 0.284       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    value_loss           | 75.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=137000, episode_reward=108.67 +/- 2.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 528      |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=109.23 +/- 2.14\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 109          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 138000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048010787 |\n",
      "|    clip_fraction        | 0.0743       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.793       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.39         |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    value_loss           | 3.18         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=110.10 +/- 0.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 528      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=110.08 +/- 1.05\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004846423 |\n",
      "|    clip_fraction        | 0.0759      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.773      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.655       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0039     |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=109.86 +/- 0.50\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    fps             | 528      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 267      |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=89.21 +/- 42.86\n",
      "Episode length: 29.80 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 29.8         |\n",
      "|    mean_reward          | 89.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 142000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030294568 |\n",
      "|    clip_fraction        | 0.0649       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.751       |\n",
      "|    explained_variance   | 0.193        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 71.3         |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00474     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=110.73 +/- 0.27\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 111      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 528      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=107.89 +/- 1.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 108          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 144000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068709017 |\n",
      "|    clip_fraction        | 0.0822       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.757       |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.12         |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00456     |\n",
      "|    value_loss           | 3.06         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=109.43 +/- 1.52\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 275      |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=110.18 +/- 0.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 146000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009092912 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.747      |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.8        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00539    |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=110.36 +/- 0.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 279      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=148000, episode_reward=108.45 +/- 2.05\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 108         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 148000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006183111 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.733      |\n",
      "|    explained_variance   | 0.298       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.992       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00204    |\n",
      "|    value_loss           | 75.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=109.34 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 283      |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=108.06 +/- 2.62\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 108          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054966677 |\n",
      "|    clip_fraction        | 0.0482       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.722       |\n",
      "|    explained_variance   | 0.204        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.33         |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000615    |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=110.21 +/- 0.78\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=109.17 +/- 2.17\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 109          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 152000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037537194 |\n",
      "|    clip_fraction        | 0.0772       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.753       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -0.000999    |\n",
      "|    value_loss           | 2.94         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=108.90 +/- 1.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=110.31 +/- 0.66\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 154000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004745001 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=109.70 +/- 1.21\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 295      |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=108.65 +/- 1.07\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008205993 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.717      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00308    |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=108.97 +/- 1.50\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=109.94 +/- 0.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 158000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056198733 |\n",
      "|    clip_fraction        | 0.0585       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.733       |\n",
      "|    explained_variance   | 0.261        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.7         |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 78.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=159000, episode_reward=109.97 +/- 1.04\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=107.96 +/- 2.02\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 108          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 160000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046709883 |\n",
      "|    clip_fraction        | 0.0795       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.712       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    value_loss           | 2.75         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=108.45 +/- 0.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 307      |\n",
      "|    total_timesteps | 161792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=108.83 +/- 1.74\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 109          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 162000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064965542 |\n",
      "|    clip_fraction        | 0.0892       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.7         |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.35         |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00481     |\n",
      "|    value_loss           | 2.23         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=109.79 +/- 1.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 527      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=108.28 +/- 1.50\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 108          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 164000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026472583 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.655       |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.000524    |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=110.42 +/- 0.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 314      |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=109.38 +/- 1.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 166000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004699111 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.66       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00239    |\n",
      "|    value_loss           | 77.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=109.93 +/- 0.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 167936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=110.00 +/- 1.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009682852 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.89        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00539    |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=109.15 +/- 1.17\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 169984   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=170000, episode_reward=109.03 +/- 1.08\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005135554 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.683      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    value_loss           | 2.58        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=109.87 +/- 1.52\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=110.56 +/- 0.19\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 111      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 327      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=109.88 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 173000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006440767 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.649      |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 72.9        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.000819   |\n",
      "|    value_loss           | 145         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=108.74 +/- 1.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 526      |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 174080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=109.58 +/- 0.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 175000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036765751 |\n",
      "|    clip_fraction        | 0.0572       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.659       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.41         |\n",
      "|    n_updates            | 850          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 2.14         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=108.54 +/- 1.31\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 106      |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 335      |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=109.40 +/- 0.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 109          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 177000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067118695 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.683       |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45         |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=109.64 +/- 0.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=109.74 +/- 1.29\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 179000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048441244 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.643       |\n",
      "|    explained_variance   | 0.216        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.25         |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=110.18 +/- 0.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 343      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=109.98 +/- 0.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 181000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076403758 |\n",
      "|    clip_fraction        | 0.0926       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.653       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 2.79         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=182000, episode_reward=108.29 +/- 2.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 346      |\n",
      "|    total_timesteps | 182272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=110.06 +/- 0.86\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 183000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007663642 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.678      |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.96        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00173    |\n",
      "|    value_loss           | 80          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=109.94 +/- 0.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 525      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 351      |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=109.85 +/- 0.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008935568 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | 0.00025     |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=109.12 +/- 1.31\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 355      |\n",
      "|    total_timesteps | 186368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=109.29 +/- 1.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 187000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009562707 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=108.70 +/- 1.06\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 359      |\n",
      "|    total_timesteps | 188416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=110.32 +/- 0.79\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 110         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 189000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004148815 |\n",
      "|    clip_fraction        | 0.0719      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.652      |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 284         |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=109.63 +/- 1.29\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 363      |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=110.29 +/- 0.12\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 191000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062944405 |\n",
      "|    clip_fraction        | 0.0847       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.65        |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.06         |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    value_loss           | 2.65         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=109.92 +/- 0.58\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 366      |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=193000, episode_reward=109.65 +/- 1.17\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 110          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 193000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050840094 |\n",
      "|    clip_fraction        | 0.0646       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.632       |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.21         |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 2.8          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=108.42 +/- 0.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.2     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 370      |\n",
      "|    total_timesteps | 194560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=109.07 +/- 0.70\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 109          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 195000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035753422 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.626       |\n",
      "|    explained_variance   | 0.249        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 55.6         |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 60.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=107.84 +/- 2.02\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 375      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=108.49 +/- 1.29\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 108          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 197000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057494054 |\n",
      "|    clip_fraction        | 0.0686       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.614       |\n",
      "|    explained_variance   | 0.868        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.38         |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.0018      |\n",
      "|    value_loss           | 3.43         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=110.27 +/- 0.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 110      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 523      |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 379      |\n",
      "|    total_timesteps | 198656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=110.64 +/- 0.12\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 111         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 199000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008420659 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.4         |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=109.32 +/- 1.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.3     |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    fps             | 524      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 382      |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3576), started 3 days, 19:26:36 ago. (Use '!kill 3576' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3caa12bfdebfb97d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3caa12bfdebfb97d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initialise environment callback\n",
    "eval_callback = EvalCallback(Monitor(s_env), eval_freq=1000, deterministic=False, render=False)\n",
    "\n",
    "## Create agent\n",
    "s_agent = PPO(PPOMlp, s_env, verbose=1, tensorboard_log=\"./logs/ppo_linear_tensorboard2/\")\n",
    "\n",
    "## Train agent for 200,000 timesteps\n",
    "s_agent.learn(total_timesteps=200000, n_eval_episodes=1, callback=eval_callback)\n",
    "%tensorboard --logdir ./logs/ppo_linear_tensorboard2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9000a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save trained agent\n",
    "s_agent_name = \"ppo-s-linear\"\n",
    "s_agent.save(s_agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc0054c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 10:22:42,387::WARNING::py.warnings::_showwarnmsg::109::C:\\Users\\olive\\yawning_titan\\.venv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=109.29 +/- 2.3471513303222493\n"
     ]
    }
   ],
   "source": [
    "# Evaluation \n",
    "\n",
    "# Create a new environment for evaluation\n",
    "eval_env =GenericNetworkEnv(red, blue, s_network_interface)\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward = evaluate_policy(s_agent, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5ec9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file  .\\gifs\\None_01-08-2023_10-23_1.webm\n",
      "MoviePy - - Generating GIF frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - - File ready: .\\gifs\\None_01-08-2023_10-23_1.webm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[   action   rewards                                               info\n",
       " 0       4    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 1       2    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 2       4    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 3       4    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 4       5    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 5       3    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 6       5    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 7       3    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 8       2    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 9       2    0.2000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 10      3    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 11      4    0.2600  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 12      0    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 13      8    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 14      9    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 15      5    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 16      3    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 17      6    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 18      9    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 19      6    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 20      5    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 21      3    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 22      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 23      0    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 24      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 25      8    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 26      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 27      0    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 28      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...\n",
       " 29      8  100.0000  {'initial_state': {'b75a9424-5c21-4379-abba-62...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create gif\n",
    "loop = ActionLoop(eval_env, s_agent, episode_count=1)\n",
    "loop.gif_action_loop(save_gif=True, render_network=True, gif_output_directory='.\\gifs', webm_output_directory='.\\gifs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2465f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<yawning_titan.envs.generic.core.action_loops.ActionLoop object at 0x000001BD76613D00>\n"
     ]
    }
   ],
   "source": [
    "loop = ActionLoop(eval_env, s_agent, episode_count=1)\n",
    "print(loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b07f90",
   "metadata": {},
   "source": [
    "## Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac09be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "909bcab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "        complete_results = []\n",
    "        for i in range(100):\n",
    "            results = pd.DataFrame(\n",
    "                columns=[\"action\", \"rewards\", \"info\", \"new_state\"]\n",
    "            )  \n",
    "            obs = eval_env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # gets the agents prediction for the best next action to take\n",
    "                action, _states = s_agent.predict(obs, deterministic= True)\n",
    "\n",
    "                # step the env\n",
    "                obs, rewards, done, info = eval_env.step(action)\n",
    "\n",
    "                results.loc[len(results.index)] = [action, rewards, info, obs]\n",
    "\n",
    "            complete_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e01b412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>rewards</th>\n",
       "      <th>info</th>\n",
       "      <th>new_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>{'initial_state': {'b75a9424-5c21-4379-abba-62...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action   rewards                                               info  \\\n",
       "0       4    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "1       4    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "2       2    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "3       4    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "4       5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "5       2    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "6       2    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "7       5    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "8       3    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "9       5    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "10      3    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "11      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "12      4    0.2600  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "13      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "14      0    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "15      0    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "16      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "17      8    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "18      6    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "19      0    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "20      8    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "21      5    0.4231  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "22      3    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "23      6    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "24      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "25      6    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "26      8    0.3000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "27      0    0.2600  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "28      5    0.5000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "29      6  100.0000  {'initial_state': {'b75a9424-5c21-4379-abba-62...   \n",
       "\n",
       "                                            new_state  \n",
       "0   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "1   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "2   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "3   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "4   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "5   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "6   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "7   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "8   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "9   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "10  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "11  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "12  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "13  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "14  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "15  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "16  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "17  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "18  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "19  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "20  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "21  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "22  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "23  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "24  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "25  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "26  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "27  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "28  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  \n",
       "29  [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a844cf3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_state': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 0,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0},\n",
       " 'initial_blue_view': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 0,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0},\n",
       " 'initial_vulnerabilities': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 0.4000000000000001,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0.6000000000000001,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0.8,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0.8,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0.8},\n",
       " 'initial_red_location': None,\n",
       " 'initial_graph': {Node(uuid='b75a9424-5c21-4379-abba-6296ad6e49b6', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.8, x_pos=-1.0, y_pos=0.01): {Node(uuid='3afd6bca-8b71-451f-ac05-23c62869f31d', name='PC 2', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=-0.5, y_pos=0.01): {}},\n",
       "  Node(uuid='3afd6bca-8b71-451f-ac05-23c62869f31d', name='PC 2', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=-0.5, y_pos=0.01): {Node(uuid='b75a9424-5c21-4379-abba-6296ad6e49b6', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.8, x_pos=-1.0, y_pos=0.01): {},\n",
       "   Node(uuid='f2882641-56a0-43c2-b28d-b0deed647b19', name='PC 3', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=0.0, y_pos=0.01): {}},\n",
       "  Node(uuid='f2882641-56a0-43c2-b28d-b0deed647b19', name='PC 3', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=0.0, y_pos=0.01): {Node(uuid='3afd6bca-8b71-451f-ac05-23c62869f31d', name='PC 2', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=-0.5, y_pos=0.01): {},\n",
       "   Node(uuid='113e6418-c7d4-4f9d-830c-352cb2216bc2', name='PC 4', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=0.5, y_pos=0.01): {}},\n",
       "  Node(uuid='113e6418-c7d4-4f9d-830c-352cb2216bc2', name='PC 4', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=0.5, y_pos=0.01): {Node(uuid='f2882641-56a0-43c2-b28d-b0deed647b19', name='PC 3', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=0.0, y_pos=0.01): {},\n",
       "   Node(uuid='f08e2045-55b8-4c23-8d11-fd85d107f15f', name='PC 5', high_value_node=True, entry_node=False, vulnerability=0.8, x_pos=1.0, y_pos=0.01): {}},\n",
       "  Node(uuid='f08e2045-55b8-4c23-8d11-fd85d107f15f', name='PC 5', high_value_node=True, entry_node=False, vulnerability=0.8, x_pos=1.0, y_pos=0.01): {Node(uuid='113e6418-c7d4-4f9d-830c-352cb2216bc2', name='PC 4', high_value_node=False, entry_node=False, vulnerability=0.8, x_pos=0.5, y_pos=0.01): {}}},\n",
       " 'current_step': 3,\n",
       " 'red_info': {0: {'Action': 'zero_day',\n",
       "   'Attacking_Nodes': [None],\n",
       "   'Target_Nodes': [Node(uuid='b75a9424-5c21-4379-abba-6296ad6e49b6', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.8, x_pos=-1.0, y_pos=0.01)],\n",
       "   'Successes': [True]}},\n",
       " 'post_red_state': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 1,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0},\n",
       " 'post_red_blue_view': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 1,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0},\n",
       " 'post_red_vulnerabilities': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 0.4000000000000001,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0.6000000000000001,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0.8,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0.8,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0.8},\n",
       " 'post_red_isolation': {'b75a9424-5c21-4379-abba-6296ad6e49b6': False,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': False,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': False,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': False,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': False},\n",
       " 'post_red_red_location': Node(uuid='b75a9424-5c21-4379-abba-6296ad6e49b6', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.8, x_pos=-1.0, y_pos=0.01),\n",
       " 'end_blue_view': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 1,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0},\n",
       " 'end_state': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 1,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0},\n",
       " 'final_vulnerabilities': {'b75a9424-5c21-4379-abba-6296ad6e49b6': 0.20000000000000007,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': 0.6000000000000001,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': 0.8,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': 0.8,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': 0.8},\n",
       " 'final_red_location': Node(uuid='b75a9424-5c21-4379-abba-6296ad6e49b6', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.8, x_pos=-1.0, y_pos=0.01),\n",
       " 'safe_nodes': 4,\n",
       " 'blue_action': 'reduce_vulnerability',\n",
       " 'blue_node': Node(uuid='b75a9424-5c21-4379-abba-6296ad6e49b6', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.8, x_pos=-1.0, y_pos=0.01),\n",
       " 'attacks': [[None,\n",
       "   Node(uuid='b75a9424-5c21-4379-abba-6296ad6e49b6', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.8, x_pos=-1.0, y_pos=0.01)]],\n",
       " 'end_isolation': {'b75a9424-5c21-4379-abba-6296ad6e49b6': False,\n",
       "  '3afd6bca-8b71-451f-ac05-23c62869f31d': False,\n",
       "  'f2882641-56a0-43c2-b28d-b0deed647b19': False,\n",
       "  '113e6418-c7d4-4f9d-830c-352cb2216bc2': False,\n",
       "  'f08e2045-55b8-4c23-8d11-fd85d107f15f': False}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_results[1]['info'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb105428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 1. , 0. , 0. , 0. , 1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. ,\n",
       "       1. , 0. , 0. , 0. , 1. , 0. , 1. , 0. , 0. , 0. , 1. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.2, 0.2, 0.8, 0.8,\n",
       "       0.8, 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0.7],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_results[1]['new_state'][11]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
