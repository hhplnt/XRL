{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc70cbdd",
   "metadata": {},
   "source": [
    "# Linear network training and replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d38c1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yawning_titan.networks.node import Node\n",
    "from yawning_titan.networks.network import Network\n",
    "\n",
    "import time\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOMlp\n",
    "\n",
    "from yawning_titan.envs.generic.core.blue_interface import BlueInterface\n",
    "from yawning_titan.envs.generic.core.red_interface import RedInterface\n",
    "from yawning_titan.envs.generic.generic_env import GenericNetworkEnv\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "from yawning_titan.envs.generic.core.network_interface import NetworkInterface\n",
    "from yawning_titan.networks.network_db import default_18_node_network\n",
    "import yawning_titan.game_modes\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "\n",
    "## Using DB to retrieve game modes\n",
    "from yawning_titan.game_modes.game_mode_db import GameModeDB, GameModeSchema\n",
    "from yawning_titan.db.doc_metadata import DocMetadataSchema\n",
    "db = GameModeDB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab85736a",
   "metadata": {},
   "source": [
    "### Network Setup\n",
    "\n",
    "Network of 5 nodes, connected in a line. Node 1 is the entry node and node 5 is the high value target. All nodes begin with an initial vulnerability of 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b38f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUID                                  Name    High Value Node    Entry Node      Vulnerability  Position (x,y)\n",
      "------------------------------------  ------  -----------------  ------------  ---------------  ----------------\n",
      "0fdc2c71-fb51-4e14-a01d-80325bee970f  PC 1    False              True                      0.8  -1.00, 0.01\n",
      "0cf00849-52e1-42ce-bccf-56589a31c272  PC 2    False              False                     0.8  -0.50, 0.01\n",
      "4c1fc42a-18b2-4bad-8b40-159ab6cc4b5d  PC 3    False              False                     0.8  0.00, 0.01\n",
      "0c9f0109-50e9-4bb9-a163-335ae86f141d  PC 4    False              False                     0.8  0.50, 0.01\n",
      "b40d296d-3027-4e76-b06e-5d14a8870bef  PC 5    True               False                     0.8  1.00, 0.01\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Network\n",
    "network = Network()\n",
    "\n",
    "# Instantiate the Node's and add them to the Network\n",
    " \n",
    "pc_1 = Node(\"PC 1\")\n",
    "network.add_node(pc_1)\n",
    "pc_1.x_pos = -1.00\n",
    "pc_1.y_pos = 0.01\n",
    "pc_1.entry_node = True\n",
    "pc_1.vulnerability = 0.8\n",
    "\n",
    "pc_2 = Node(\"PC 2\")\n",
    "network.add_node(pc_2)\n",
    "pc_2.x_pos = -0.50\n",
    "pc_2.y_pos = 0.01\n",
    "pc_2.vulnerability = 0.8\n",
    "\n",
    "pc_3 = Node(\"PC 3\")\n",
    "network.add_node(pc_3)\n",
    "pc_3.x_pos = 0.00\n",
    "pc_3.y_pos = 0.01\n",
    "pc_3.vulnerability = 0.8\n",
    "\n",
    "pc_4 = Node(\"PC 4\")\n",
    "network.add_node(pc_4)\n",
    "pc_4.x_pos = 0.50\n",
    "pc_4.y_pos = 0.01\n",
    "pc_4.vulnerability = 0.8\n",
    "\n",
    "pc_5 = Node(\"PC 5\")\n",
    "network.add_node(pc_5)\n",
    "pc_5.x_pos = 1.00\n",
    "pc_5.y_pos = 0.01\n",
    "pc_5.high_value_node = True\n",
    "pc_5.vulnerability = 0.8\n",
    "\n",
    "\n",
    "# Add the edges between Node's\n",
    "network.add_edge(pc_1, pc_2)\n",
    "network.add_edge(pc_2, pc_3)\n",
    "network.add_edge(pc_3, pc_4)\n",
    "network.add_edge(pc_4, pc_5)\n",
    "\n",
    "\n",
    "\n",
    "# Reset the entry nodes, high value nodes, and vulnerability scores by calling .setup()\n",
    "# network.reset()\n",
    "\n",
    "# View the Networks Node Details\n",
    "network.show(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee45f47",
   "metadata": {},
   "source": [
    "## Creating environments and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d5e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d048b699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name               author              locked    uuid\n",
      "-----------------  ------------------  --------  ------------------------------------\n",
      "DCBO Agent Config  dstl/YAWNING-TITAN  True      bac2cb9d-b24b-426c-88a5-5edd0c2de413\n",
      "Default Game Mode  dstl/YAWNING-TITAN  True      900a704f-6271-4994-ade7-40b74d3199b1\n",
      "Low skill red      dstl/YAWNING-TITAN  True      3ccd9988-8781-4c3e-9c75-44cc987ae6af\n",
      "XAI_mode           H Harrison          False     f5665563-d91a-4164-9e19-c67ce3db0066\n"
     ]
    }
   ],
   "source": [
    "db.show(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61acc85f",
   "metadata": {},
   "source": [
    "### Game mode\n",
    "\n",
    "#### Red: \n",
    "\n",
    "Can attack from any node it controls \n",
    "\n",
    "Only basic attack and zero day enabled. \n",
    "\n",
    "Starts with one zero day attack and gains another every 5 timesteps \n",
    "\n",
    "No natural spreading \n",
    "\n",
    "Target mechanism: (prioritise vulnerable nodes â€“ sorts nodes it can attack and selects most vulnerable) changed to random \n",
    "\n",
    "#### Blue: \n",
    "\n",
    "Action set: reduce vulnerability, restore node (considering taking away restore node as kept winning by immediately restoring first node red attacks) \n",
    "\n",
    "100% chance of immediately discovering intrusions \n",
    "\n",
    "#### Game rules: \n",
    "\n",
    "Max steps: 30, no grace period \n",
    "\n",
    "Blue loss if high value node lost\n",
    "\n",
    "#### Observation space \n",
    "\n",
    "Compromised status, vulnerability scores and node connections. I removed special nodes as this is kind of inferred by the rewards and adds a lot of dimensions.\n",
    "\n",
    "#### Rewards \n",
    "\n",
    "-100 for loss, 100 for reaching end.  \n",
    "\n",
    "Negative reward reduced for closer fails â€“ if closer to end of timesteps \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "543a6f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 12621 (char 12620)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m simple_mode \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf5665563-d91a-4164-9e19-c67ce3db0066\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\yawning_titan\\.venv\\lib\\site-packages\\yawning_titan\\game_modes\\game_mode_db.py:172\u001b[0m, in \u001b[0;36mGameModeDB.get\u001b[1;34m(self, uuid)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03mGet a game_mode config document from its uuid.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m    otherwise :class:`None`.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# self._db.db.clear_cache()\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43muuid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc_to_game_mode(doc)\n",
      "File \u001b[1;32m~\\yawning_titan\\.venv\\lib\\site-packages\\yawning_titan\\db\\yawning_titan_db.py:178\u001b[0m, in \u001b[0;36mYawningTitanDB.get\u001b[1;34m(self, uuid)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, uuid: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Document, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    Get a doc from its uuid.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m        when the search returns multiple docs with the same uuid.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDocMetadataSchema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUUID\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muuid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb\u001b[38;5;241m.\u001b[39mclear_cache()\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "File \u001b[1;32m~\\yawning_titan\\.venv\\lib\\site-packages\\tinydb\\table.py:254\u001b[0m, in \u001b[0;36mTable.search\u001b[1;34m(self, cond)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_results[:]\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Perform the search by applying the query to all documents.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# Then, only if the document matches the query, convert it\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# to the document class and document ID class.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_class(doc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_id_class(doc_id))\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc_id, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cond(doc)\n\u001b[0;32m    256\u001b[0m ]\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Only cache cacheable queries.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;66;03m# This weird `getattr` dance is needed to make MyPy happy as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# `is_cacheable` was introduced which assumed that all queries\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# are cacheable.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m is_cacheable: Callable[[], \u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(cond, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_cacheable\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    272\u001b[0m                                            \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\yawning_titan\\.venv\\lib\\site-packages\\tinydb\\table.py:685\u001b[0m, in \u001b[0;36mTable._read_table\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124;03mRead the table data from the underlying storage.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;124;03monly one document for example.\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \u001b[38;5;66;03m# Retrieve the tables from the storage\u001b[39;00m\n\u001b[1;32m--> 685\u001b[0m tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# The database is empty\u001b[39;00m\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\yawning_titan\\.venv\\lib\\site-packages\\tinydb\\storages.py:125\u001b[0m, in \u001b[0;36mJSONStorage.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Load the JSON contents of the file\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\json\\decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 12621 (char 12620)"
     ]
    }
   ],
   "source": [
    "simple_mode = db.get(\"f5665563-d91a-4164-9e19-c67ce3db0066\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52138506",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build network interface\n",
    "s_network_interface = NetworkInterface(game_mode=simple_mode, network=network)\n",
    "\n",
    "## Name agents\n",
    "red = RedInterface(s_network_interface)\n",
    "blue = BlueInterface(s_network_interface)\n",
    "\n",
    "## Create environment\n",
    "s_env = GenericNetworkEnv(red, blue, s_network_interface)\n",
    "\n",
    "## Check compliant with OpenAI gym\n",
    "check_env(s_env, warn=True)\n",
    "_ = s_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cb49df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/tensorboard_ex/PPO_1\n",
      "Eval num_timesteps=1000, episode_reward=-34.35 +/- 8.24\n",
      "Episode length: 14.40 +/- 3.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.4     |\n",
      "|    mean_reward     | -34.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-38.90 +/- 8.04\n",
      "Episode length: 12.00 +/- 6.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | -38.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12       |\n",
      "|    ep_rew_mean     | -40.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 834      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-40.21 +/- 6.35\n",
      "Episode length: 12.40 +/- 5.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | -40.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013506388 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | 0.00353     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 121         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 454         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-43.23 +/- 5.57\n",
      "Episode length: 12.20 +/- 3.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | -43.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.1     |\n",
      "|    ep_rew_mean     | -37.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 680      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-23.97 +/- 33.16\n",
      "Episode length: 14.40 +/- 8.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 14.4         |\n",
      "|    mean_reward          | -24          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0116607305 |\n",
      "|    clip_fraction        | 0.0979       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.28        |\n",
      "|    explained_variance   | 0.0013       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 62.8         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.013       |\n",
      "|    value_loss           | 235          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-35.37 +/- 2.38\n",
      "Episode length: 13.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | -35.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.3     |\n",
      "|    ep_rew_mean     | -38.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-2.84 +/- 36.58\n",
      "Episode length: 19.40 +/- 8.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19.4        |\n",
      "|    mean_reward          | -2.84       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012474487 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.000925    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 142         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-24.25 +/- 33.54\n",
      "Episode length: 12.00 +/- 9.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | -24.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.1     |\n",
      "|    ep_rew_mean     | -34.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 635      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-26.31 +/- 29.21\n",
      "Episode length: 13.80 +/- 8.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | -26.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011432238 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.22       |\n",
      "|    explained_variance   | 0.000951    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-35.29 +/- 3.37\n",
      "Episode length: 15.20 +/- 4.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.2     |\n",
      "|    mean_reward     | -35.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.2     |\n",
      "|    ep_rew_mean     | -29.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 623      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-18.78 +/- 27.75\n",
      "Episode length: 17.80 +/- 8.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 17.8         |\n",
      "|    mean_reward          | -18.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022231385 |\n",
      "|    clip_fraction        | 0.00293      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.21        |\n",
      "|    explained_variance   | 0.373        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58           |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-22.73 +/- 29.70\n",
      "Episode length: 14.80 +/- 7.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.8     |\n",
      "|    mean_reward     | -22.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.1     |\n",
      "|    ep_rew_mean     | -25.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-42.95 +/- 5.27\n",
      "Episode length: 8.40 +/- 3.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.4         |\n",
      "|    mean_reward          | -42.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006221044 |\n",
      "|    clip_fraction        | 0.0214      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.19       |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 132         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00714    |\n",
      "|    value_loss           | 225         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-16.92 +/- 28.31\n",
      "Episode length: 19.40 +/- 6.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.4     |\n",
      "|    mean_reward     | -16.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.2     |\n",
      "|    ep_rew_mean     | -29.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-30.21 +/- 9.78\n",
      "Episode length: 13.80 +/- 6.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.8         |\n",
      "|    mean_reward          | -30.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067415214 |\n",
      "|    clip_fraction        | 0.0394       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.16        |\n",
      "|    explained_variance   | 0.529        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 70.4         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00988     |\n",
      "|    value_loss           | 182          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-32.14 +/- 3.38\n",
      "Episode length: 14.40 +/- 3.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.4     |\n",
      "|    mean_reward     | -32.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17       |\n",
      "|    ep_rew_mean     | -22.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-1.87 +/- 30.27\n",
      "Episode length: 22.00 +/- 6.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22          |\n",
      "|    mean_reward          | -1.87       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009480332 |\n",
      "|    clip_fraction        | 0.0679      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.12       |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 245         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=2.58 +/- 32.00\n",
      "Episode length: 25.40 +/- 3.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.4     |\n",
      "|    mean_reward     | 2.58     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.7     |\n",
      "|    ep_rew_mean     | -21      |\n",
      "| time/              |          |\n",
      "|    fps             | 612      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-31.39 +/- 6.29\n",
      "Episode length: 14.40 +/- 7.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | -31.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008887447 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.07       |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 275         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-12.09 +/- 28.30\n",
      "Episode length: 19.60 +/- 6.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.6     |\n",
      "|    mean_reward     | -12.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.3     |\n",
      "|    ep_rew_mean     | -13.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-23.76 +/- 36.07\n",
      "Episode length: 13.40 +/- 8.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | -23.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007958883 |\n",
      "|    clip_fraction        | 0.0292      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.02       |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 128         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00901    |\n",
      "|    value_loss           | 290         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-5.23 +/- 38.09\n",
      "Episode length: 20.40 +/- 7.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -5.23    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -13      |\n",
      "| time/              |          |\n",
      "|    fps             | 614      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-6.85 +/- 43.39\n",
      "Episode length: 17.80 +/- 10.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.8        |\n",
      "|    mean_reward          | -6.85       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007956141 |\n",
      "|    clip_fraction        | 0.0219      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.99       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 310         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=24000, episode_reward=0.76 +/- 30.91\n",
      "Episode length: 24.60 +/- 5.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.6     |\n",
      "|    mean_reward     | 0.764    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -8.76    |\n",
      "| time/              |          |\n",
      "|    fps             | 613      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-22.78 +/- 15.61\n",
      "Episode length: 16.80 +/- 7.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.8        |\n",
      "|    mean_reward          | -22.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009362611 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 172         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 321         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-28.11 +/- 11.84\n",
      "Episode length: 17.20 +/- 8.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.2     |\n",
      "|    mean_reward     | -28.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.3     |\n",
      "|    ep_rew_mean     | 1.47     |\n",
      "| time/              |          |\n",
      "|    fps             | 614      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-16.51 +/- 25.34\n",
      "Episode length: 19.80 +/- 9.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19.8        |\n",
      "|    mean_reward          | -16.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006470604 |\n",
      "|    clip_fraction        | 0.0597      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 200         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 351         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=10.21 +/- 39.17\n",
      "Episode length: 23.00 +/- 9.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23       |\n",
      "|    mean_reward     | 10.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | 4.86     |\n",
      "| time/              |          |\n",
      "|    fps             | 611      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=28.32 +/- 31.95\n",
      "Episode length: 27.00 +/- 6.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 27           |\n",
      "|    mean_reward          | 28.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058667306 |\n",
      "|    clip_fraction        | 0.061        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.84        |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 146          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00751     |\n",
      "|    value_loss           | 313          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-1.53 +/- 36.16\n",
      "Episode length: 21.80 +/- 7.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.8     |\n",
      "|    mean_reward     | -1.53    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | 0.707    |\n",
      "| time/              |          |\n",
      "|    fps             | 610      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=25.56 +/- 25.89\n",
      "Episode length: 28.40 +/- 3.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 28.4        |\n",
      "|    mean_reward          | 25.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010165172 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 157         |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 318         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=6.97 +/- 35.29\n",
      "Episode length: 22.00 +/- 9.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22       |\n",
      "|    mean_reward     | 6.97     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.5     |\n",
      "|    ep_rew_mean     | 13.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=8.63 +/- 29.85\n",
      "Episode length: 28.40 +/- 1.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 28.4        |\n",
      "|    mean_reward          | 8.63        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007781231 |\n",
      "|    clip_fraction        | 0.0778      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 149         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 351         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=0.04 +/- 43.25\n",
      "Episode length: 18.20 +/- 9.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | 0.0419   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26       |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 608      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=35000, episode_reward=11.00 +/- 42.35\n",
      "Episode length: 22.20 +/- 9.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.2        |\n",
      "|    mean_reward          | 11          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010766781 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 129         |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 254         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=48.15 +/- 4.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 48.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.9     |\n",
      "|    ep_rew_mean     | 27       |\n",
      "| time/              |          |\n",
      "|    fps             | 606      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=38.41 +/- 23.81\n",
      "Episode length: 29.60 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29.6       |\n",
      "|    mean_reward          | 38.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 37000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01177001 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.6       |\n",
      "|    explained_variance   | 0.366      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 69.5       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    value_loss           | 220        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=12.65 +/- 35.10\n",
      "Episode length: 22.60 +/- 9.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | 12.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.9     |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 605      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=5.07 +/- 40.82\n",
      "Episode length: 20.20 +/- 8.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 20.2         |\n",
      "|    mean_reward          | 5.07         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125802625 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.394        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 192          |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 238          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1.40 +/- 39.15\n",
      "Episode length: 20.00 +/- 11.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | 1.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.1     |\n",
      "|    ep_rew_mean     | 25.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=16.21 +/- 38.31\n",
      "Episode length: 22.60 +/- 9.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | 16.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008868545 |\n",
      "|    clip_fraction        | 0.0708      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 84.8        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00876    |\n",
      "|    value_loss           | 234         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=36.46 +/- 27.33\n",
      "Episode length: 27.80 +/- 4.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27.8     |\n",
      "|    mean_reward     | 36.5     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=16.96 +/- 41.09\n",
      "Episode length: 22.60 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | 17       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27       |\n",
      "|    ep_rew_mean     | 30.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=47.84 +/- 4.24\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 47.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008710815 |\n",
      "|    clip_fraction        | 0.088       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.368       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 76.7        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 204         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=32.13 +/- 32.11\n",
      "Episode length: 26.00 +/- 8.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26       |\n",
      "|    mean_reward     | 32.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.2     |\n",
      "|    ep_rew_mean     | 34.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 598      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=38.34 +/- 23.81\n",
      "Episode length: 29.00 +/- 2.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29          |\n",
      "|    mean_reward          | 38.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007081204 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 80.9        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 156         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=47000, episode_reward=3.88 +/- 44.28\n",
      "Episode length: 19.40 +/- 9.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.4     |\n",
      "|    mean_reward     | 3.88     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.4     |\n",
      "|    ep_rew_mean     | 30       |\n",
      "| time/              |          |\n",
      "|    fps             | 598      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=49.66 +/- 1.60\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 49.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071891667 |\n",
      "|    clip_fraction        | 0.0669       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0.276        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 148          |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00787     |\n",
      "|    value_loss           | 244          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=49000, episode_reward=46.96 +/- 3.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 47       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.3     |\n",
      "|    ep_rew_mean     | 38.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=53.27 +/- 3.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 53.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009348446 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 66.3        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00652    |\n",
      "|    value_loss           | 94.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51000, episode_reward=52.09 +/- 4.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 52.1     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29       |\n",
      "|    ep_rew_mean     | 43.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 599      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=37.59 +/- 32.82\n",
      "Episode length: 26.80 +/- 6.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.8        |\n",
      "|    mean_reward          | 37.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008943973 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.296       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.8        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00917    |\n",
      "|    value_loss           | 88.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=38.56 +/- 37.81\n",
      "Episode length: 25.60 +/- 8.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.6     |\n",
      "|    mean_reward     | 38.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 47.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=50.94 +/- 4.76\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 50.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011628985 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00943    |\n",
      "|    value_loss           | 56.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=55.36 +/- 1.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 55.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.3     |\n",
      "|    ep_rew_mean     | 45.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 600      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=30.28 +/- 30.20\n",
      "Episode length: 27.80 +/- 3.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 27.8         |\n",
      "|    mean_reward          | 30.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110968985 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.156        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 63.1         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00944     |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=37.04 +/- 29.52\n",
      "Episode length: 27.20 +/- 5.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27.2     |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.2     |\n",
      "|    ep_rew_mean     | 43.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 601      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=58000, episode_reward=35.92 +/- 38.96\n",
      "Episode length: 25.40 +/- 9.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.4        |\n",
      "|    mean_reward          | 35.9        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007951869 |\n",
      "|    clip_fraction        | 0.0636      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.998      |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.2        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00734    |\n",
      "|    value_loss           | 123         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=51.76 +/- 3.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 51.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.2     |\n",
      "|    ep_rew_mean     | 44       |\n",
      "| time/              |          |\n",
      "|    fps             | 601      |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=54.57 +/- 2.86\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 54.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008438434 |\n",
      "|    clip_fraction        | 0.088       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.236       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.4        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00957    |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=27.43 +/- 38.59\n",
      "Episode length: 24.40 +/- 7.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.4     |\n",
      "|    mean_reward     | 27.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 50.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=37.24 +/- 39.77\n",
      "Episode length: 25.00 +/- 10.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | 37.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024497703 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -0.129      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.8        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 27.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=51.98 +/- 5.42\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 52       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 53.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=57.59 +/- 2.27\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 57.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012939621 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.41        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 37.7        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=55.45 +/- 1.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 55.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.4     |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 602      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=56.70 +/- 0.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | 56.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 66000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01023563 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.982     |\n",
      "|    explained_variance   | 0.0873     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.2       |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.00688   |\n",
      "|    value_loss           | 35.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=38.37 +/- 38.08\n",
      "Episode length: 25.40 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.4     |\n",
      "|    mean_reward     | 38.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 52.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=42.81 +/- 31.15\n",
      "Episode length: 27.60 +/- 4.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 27.6       |\n",
      "|    mean_reward          | 42.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 68000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00532162 |\n",
      "|    clip_fraction        | 0.0775     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.994     |\n",
      "|    explained_variance   | 0.0906     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 14.9       |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | -0.0079    |\n",
      "|    value_loss           | 69.4       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=69000, episode_reward=39.79 +/- 33.02\n",
      "Episode length: 26.60 +/- 6.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.6     |\n",
      "|    mean_reward     | 39.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.8     |\n",
      "|    ep_rew_mean     | 49.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=59.18 +/- 1.36\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 59.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006010068 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.987      |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00606    |\n",
      "|    value_loss           | 140         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=71000, episode_reward=56.82 +/- 4.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 56.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.9     |\n",
      "|    ep_rew_mean     | 50.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=23.67 +/- 42.03\n",
      "Episode length: 22.80 +/- 8.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 22.8       |\n",
      "|    mean_reward          | 23.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 72000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00731547 |\n",
      "|    clip_fraction        | 0.0563     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.985     |\n",
      "|    explained_variance   | 0.0703     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.7       |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.00512   |\n",
      "|    value_loss           | 56.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=57.74 +/- 3.23\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 57.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 55.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=57.28 +/- 3.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 57.3         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 74000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067969663 |\n",
      "|    clip_fraction        | 0.0936       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.912       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46           |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 38.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=59.28 +/- 1.86\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 59.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.8     |\n",
      "|    ep_rew_mean     | 54.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=59.88 +/- 1.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 59.9         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047096736 |\n",
      "|    clip_fraction        | 0.0926       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.915       |\n",
      "|    explained_variance   | 0.0722       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.13         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00437     |\n",
      "|    value_loss           | 44.1         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=77000, episode_reward=41.76 +/- 32.52\n",
      "Episode length: 27.20 +/- 5.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27.2     |\n",
      "|    mean_reward     | 41.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 56.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=58.12 +/- 2.53\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 58.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067630652 |\n",
      "|    clip_fraction        | 0.0887       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.911       |\n",
      "|    explained_variance   | 0.153        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    value_loss           | 13.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=60.71 +/- 0.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 60.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=80000, episode_reward=60.06 +/- 1.76\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 60.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045377775 |\n",
      "|    clip_fraction        | 0.0642       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.861       |\n",
      "|    explained_variance   | 0.106        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.9         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00457     |\n",
      "|    value_loss           | 34.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=58.30 +/- 1.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 58.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 135      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=57.17 +/- 2.43\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 57.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011784021 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.872      |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.64        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00453    |\n",
      "|    value_loss           | 16.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=58.83 +/- 2.23\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 58.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30.1     |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 139      |\n",
      "|    total_timesteps | 83968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=58.74 +/- 1.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 58.7         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059362343 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.855       |\n",
      "|    explained_variance   | 0.048        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.68         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 42.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=57.73 +/- 1.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 57.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=59.81 +/- 2.28\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 59.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.4     |\n",
      "|    ep_rew_mean     | 55.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=39.18 +/- 40.49\n",
      "Episode length: 25.00 +/- 10.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 25           |\n",
      "|    mean_reward          | 39.2         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 87000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065308674 |\n",
      "|    clip_fraction        | 0.0946       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.831       |\n",
      "|    explained_variance   | 0.0768       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00465     |\n",
      "|    value_loss           | 54.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=60.18 +/- 1.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 60.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 56.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 603      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=59.45 +/- 1.86\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 59.5         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 89000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045730183 |\n",
      "|    clip_fraction        | 0.0726       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.848       |\n",
      "|    explained_variance   | 0.119        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    value_loss           | 35.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=60.28 +/- 1.15\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 60.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=59.56 +/- 1.86\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 59.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 91000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005645329 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.823      |\n",
      "|    explained_variance   | 0.0295      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    value_loss           | 56.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=92000, episode_reward=59.31 +/- 1.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 59.3     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 58.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=60.55 +/- 0.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 60.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 93000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006584077 |\n",
      "|    clip_fraction        | 0.0851      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.827      |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 59.9        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    value_loss           | 16.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=59.41 +/- 1.56\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 59.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.6     |\n",
      "|    ep_rew_mean     | 56.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=60.13 +/- 0.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 30           |\n",
      "|    mean_reward          | 60.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 95000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034780826 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.854       |\n",
      "|    explained_variance   | 0.0632       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    value_loss           | 54.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=47.84 +/- 24.48\n",
      "Episode length: 29.00 +/- 2.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 29       |\n",
      "|    mean_reward     | 47.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 30       |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 159      |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=59.43 +/- 1.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30          |\n",
      "|    mean_reward          | 59.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 97000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005310109 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.827      |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.528       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00462    |\n",
      "|    value_loss           | 19.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=60.69 +/- 0.72\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 60.7     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 54.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=41.64 +/- 32.51\n",
      "Episode length: 26.80 +/- 6.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 26.8         |\n",
      "|    mean_reward          | 41.6         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 99000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025599762 |\n",
      "|    clip_fraction        | 0.0371       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.806       |\n",
      "|    explained_variance   | 0.0407       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.1         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 81.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=60.35 +/- 1.23\n",
      "Episode length: 30.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | 60.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.3     |\n",
      "|    ep_rew_mean     | 54.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 604      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 165      |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7f6dde8707a9e552\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7f6dde8707a9e552\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initialise environment callback\n",
    "eval_callback = EvalCallback(Monitor(s_env), eval_freq=1000, deterministic=False, render=False)\n",
    "\n",
    "## Create agent\n",
    "s_agent_6 = PPO(PPOMlp, s_env, verbose=1, tensorboard_log=\"./logs/tensorboard_ex/\")\n",
    "\n",
    "## Train agent for 200,000 timesteps\n",
    "s_agent_6.learn(total_timesteps=100000, n_eval_episodes=1, callback=eval_callback)\n",
    "%tensorboard --logdir ./logs/tensorboard_ex/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save trained agent\n",
    "s_agent_name = \"ppo-s-linear\"\n",
    "s_agent.save(s_agent_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0054c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "\n",
    "# Create a new environment for evaluation\n",
    "eval_env =GenericNetworkEnv(red, blue, s_network_interface)\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward = evaluate_policy(s_agent, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
