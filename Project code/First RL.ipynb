{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa23c94-7412-40c4-9b60-b22d5f852b7f",
   "metadata": {},
   "source": [
    "## First create network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f9dff2-fc35-43db-afb8-f824d6c96e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yawning_titan.networks.node import Node\n",
    "from yawning_titan.networks.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89cb44c-2dba-40c1-9036-60f8ecd0aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUID                                  Name      High Value Node    Entry Node      Vulnerability  Position (x,y)\n",
      "------------------------------------  --------  -----------------  ------------  ---------------  ----------------\n",
      "e8b8d6b4-9078-4fff-b039-b94631ed5e57  Router 1  True               False                0.689895  0.00, 0.00\n",
      "b07f9828-0fe6-465a-8d3a-512a5feea783  Switch 1  False              False                0.430328  0.01, 0.61\n",
      "a56584f5-50d2-411f-b2be-2f216c87f7a8  Switch 2  False              False                0.839482  0.00, -0.62\n",
      "018dab3e-ed96-4bbb-a006-3255d6ccc258  PC 1      False              False                0.693289  -0.38, 0.69\n",
      "7606a163-71c7-4265-afa9-e5dcddb6b27f  PC 2      False              True                 0.254101  0.18, 1.00\n",
      "d7eec66e-b0a0-4a8e-b15e-a9026c729773  PC 3      False              False                0.475056  0.39, 0.70\n",
      "01ed5e66-b260-4dd4-9a1e-125b9fd3e31c  PC 4      False              True                 0.843232  -0.39, -0.69\n",
      "b45599e6-23a1-4457-8c94-ae99437aa69b  PC 5      False              False                0.807658  0.38, -0.69\n",
      "a7e8df8c-58ad-4d38-8e5a-d9c2675868e7  PC 6      True               False                0.82635   -0.19, -1.00\n",
      "e48a0d75-1023-441b-9cad-ccabcece731c  Server 1  False              True                 0.276198  -0.17, 0.99\n",
      "6444ff78-dbf1-4f9a-a640-e04f1b780900  Server 2  False              False                0.904936  0.17, -1.00\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Network\n",
    "network = Network(\n",
    "    set_random_entry_nodes=True,\n",
    "    num_of_random_entry_nodes=3,\n",
    "    set_random_high_value_nodes=True,\n",
    "    num_of_random_high_value_nodes=2,\n",
    "    set_random_vulnerabilities=True,\n",
    ")\n",
    "\n",
    "# Instantiate the Node's and add them to the Network\n",
    "router_1 = Node(\"Router 1\")\n",
    "network.add_node(router_1)\n",
    "\n",
    "\n",
    "switch_1 = Node(\"Switch 1\")\n",
    "network.add_node(switch_1)\n",
    "switch_1.x_pos = 0.01\n",
    "switch_1.y_pos = 0.61\n",
    "\n",
    "switch_2 = Node(\"Switch 2\")\n",
    "network.add_node(switch_2)\n",
    "switch_2.y_pos = -0.62 \n",
    "\n",
    "pc_1 = Node(\"PC 1\")\n",
    "network.add_node(pc_1)\n",
    "pc_1.x_pos = -0.38\n",
    "pc_1.y_pos = 0.69\n",
    "\n",
    "pc_2 = Node(\"PC 2\")\n",
    "network.add_node(pc_2)\n",
    "pc_2.x_pos = 0.18\n",
    "pc_2.y_pos = 1.00\n",
    "\n",
    "pc_3 = Node(\"PC 3\")\n",
    "network.add_node(pc_3)\n",
    "pc_3.x_pos = 0.39\n",
    "pc_3.y_pos = 0.70\n",
    "\n",
    "pc_4 = Node(\"PC 4\")\n",
    "network.add_node(pc_4)\n",
    "pc_4.x_pos = -0.39\n",
    "pc_4.y_pos = -0.69\n",
    "\n",
    "pc_5 = Node(\"PC 5\")\n",
    "network.add_node(pc_5)\n",
    "pc_5.x_pos = 0.38\n",
    "pc_5.y_pos = -0.69\n",
    "\n",
    "pc_6 = Node(\"PC 6\")\n",
    "network.add_node(pc_6)\n",
    "pc_6.x_pos = -0.19\n",
    "pc_6.y_pos = -1.00\n",
    "\n",
    "server_1 = Node(\"Server 1\")\n",
    "network.add_node(server_1)\n",
    "server_1.x_pos = -0.17\n",
    "server_1.y_pos = 0.99\n",
    "\n",
    "server_2 = Node(\"Server 2\")\n",
    "network.add_node(server_2)\n",
    "server_2.x_pos = 0.17\n",
    "server_2.y_pos = -1.00\n",
    "\n",
    "# Add the edges between Node's\n",
    "network.add_edge(router_1, switch_1)\n",
    "network.add_edge(switch_1, server_1)\n",
    "network.add_edge(switch_1, pc_1)\n",
    "network.add_edge(switch_1, pc_2)\n",
    "network.add_edge(switch_1, pc_3)\n",
    "network.add_edge(router_1, switch_2)\n",
    "network.add_edge(switch_2, server_2)\n",
    "network.add_edge(switch_2, pc_4)\n",
    "network.add_edge(switch_2, pc_5)\n",
    "network.add_edge(switch_2, pc_6)\n",
    "\n",
    "# Reset the entry nodes, high value nodes, and vulnerability scores by calling .setup()\n",
    "network.reset()\n",
    "\n",
    "# View the Networks Node Details\n",
    "network.show(verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a97b4-56e6-41e1-8afa-c73481eab8be",
   "metadata": {},
   "source": [
    "## Creating environment and training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30cf136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5465da69-5a2a-4ba6-b7d2-e67515f44489",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\yawning_titan\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import packages - SB3\n",
    "import time\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOMlp\n",
    "\n",
    "from yawning_titan.envs.generic.core.blue_interface import BlueInterface\n",
    "from yawning_titan.envs.generic.core.red_interface import RedInterface\n",
    "from yawning_titan.envs.generic.generic_env import GenericNetworkEnv\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "from yawning_titan.envs.generic.core.network_interface import NetworkInterface\n",
    "from yawning_titan.networks.network_db import default_18_node_network\n",
    "from yawning_titan.game_modes.game_mode_db import default_game_mode\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2da84d09-4743-46c3-b81d-42ce7da72135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Select game mode\n",
    "game_mode = default_game_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66e1add-8ee8-4d54-81e2-71e1fa62cfa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Build network interface\n",
    "network_interface = NetworkInterface(game_mode=game_mode, network=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c014789-0765-48b4-831b-46860bec4fee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Name agents\n",
    "red = RedInterface(network_interface)\n",
    "blue = BlueInterface(network_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f43d14-9df2-4e17-857b-9b6bbbf1d140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create environment\n",
    "env = GenericNetworkEnv(red, blue, network_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6fd1a8-512e-45ac-8697-4a060d912cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Check compliant with OpenAI gym\n",
    "check_env(env, warn=True)\n",
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d148b91d-f6d7-42a4-b9cc-c5dc4e888a5e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Initialise environment callback\n",
    "eval_callback = EvalCallback(Monitor(env), eval_freq=1000, deterministic=False, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37f6d185-0cf1-4584-9dee-703ca0bb0298",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "## Create agent\n",
    "agent = PPO(PPOMlp, env, verbose=1, tensorboard_log=\"./logs/ppo_YT_initial_tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a923d714-7172-48fe-9a51-74b28c0a85a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/ppo_YT_initial_tensorboard/PPO_3\n",
      "Eval num_timesteps=1000, episode_reward=-108.47 +/- 9.63\n",
      "Episode length: 9.20 +/- 9.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-107.08 +/- 7.51\n",
      "Episode length: 11.20 +/- 6.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.6     |\n",
      "|    ep_rew_mean     | -110     |\n",
      "| time/              |          |\n",
      "|    fps             | 124      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-112.40 +/- 7.66\n",
      "Episode length: 15.60 +/- 10.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.6        |\n",
      "|    mean_reward          | -112        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011068372 |\n",
      "|    clip_fraction        | 0.0518      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.29       |\n",
      "|    explained_variance   | -0.000359   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.69e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 4.06e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-108.35 +/- 4.32\n",
      "Episode length: 15.00 +/- 4.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.1     |\n",
      "|    ep_rew_mean     | -108     |\n",
      "| time/              |          |\n",
      "|    fps             | 192      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-109.04 +/- 7.75\n",
      "Episode length: 12.20 +/- 7.05\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12.2       |\n",
      "|    mean_reward          | -109       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01216401 |\n",
      "|    clip_fraction        | 0.0652     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.27      |\n",
      "|    explained_variance   | 2.72e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.56e+03   |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    value_loss           | 3.52e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-103.64 +/- 4.89\n",
      "Episode length: 12.20 +/- 6.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | -104     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.7     |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    fps             | 238      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-110.39 +/- 7.64\n",
      "Episode length: 18.40 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | -110        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011817721 |\n",
      "|    clip_fraction        | 0.0709      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.25       |\n",
      "|    explained_variance   | 2.28e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.24e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 2.79e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-107.86 +/- 8.84\n",
      "Episode length: 12.00 +/- 6.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | -108     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.1     |\n",
      "|    ep_rew_mean     | -107     |\n",
      "| time/              |          |\n",
      "|    fps             | 267      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-101.54 +/- 2.84\n",
      "Episode length: 7.00 +/- 3.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | -102        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015022835 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.22       |\n",
      "|    explained_variance   | 8.29e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.1e+03     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 2.42e+03    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-105.12 +/- 5.61\n",
      "Episode length: 11.20 +/- 5.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.3     |\n",
      "|    ep_rew_mean     | -106     |\n",
      "| time/              |          |\n",
      "|    fps             | 286      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-102.59 +/- 2.51\n",
      "Episode length: 8.20 +/- 4.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | -103        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015748974 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.19       |\n",
      "|    explained_variance   | 4.05e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 901         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 1.93e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-100.14 +/- 2.55\n",
      "Episode length: 9.00 +/- 4.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.7     |\n",
      "|    ep_rew_mean     | -105     |\n",
      "| time/              |          |\n",
      "|    fps             | 304      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-105.42 +/- 5.09\n",
      "Episode length: 6.80 +/- 5.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | -105       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01215865 |\n",
      "|    clip_fraction        | 0.0667     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.16      |\n",
      "|    explained_variance   | 4.77e-06   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 600        |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    value_loss           | 1.47e+03   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-100.06 +/- 2.37\n",
      "Episode length: 10.80 +/- 9.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.8     |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.5     |\n",
      "|    ep_rew_mean     | -104     |\n",
      "| time/              |          |\n",
      "|    fps             | 312      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 14336    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-105.91 +/- 4.04\n",
      "Episode length: 14.80 +/- 11.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.8        |\n",
      "|    mean_reward          | -106        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012965313 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.13       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 464         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 1.11e+03    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-101.58 +/- 3.22\n",
      "Episode length: 13.80 +/- 11.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.1     |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    fps             | 320      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-98.42 +/- 2.18\n",
      "Episode length: 8.60 +/- 5.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | -98.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011504029 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.09       |\n",
      "|    explained_variance   | 5.07e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 313         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 897         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-104.98 +/- 4.34\n",
      "Episode length: 10.80 +/- 6.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.8     |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.4     |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    fps             | 327      |\n",
      "|    iterations      | 9        |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 18432    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-101.76 +/- 4.30\n",
      "Episode length: 10.60 +/- 5.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 10.6         |\n",
      "|    mean_reward          | -102         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145173995 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.06        |\n",
      "|    explained_variance   | 4.71e-06     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 258          |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0163      |\n",
      "|    value_loss           | 648          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-101.86 +/- 0.98\n",
      "Episode length: 13.00 +/- 8.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.4     |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    fps             | 329      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-101.46 +/- 2.47\n",
      "Episode length: 6.40 +/- 6.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.4         |\n",
      "|    mean_reward          | -101        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011810874 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.03       |\n",
      "|    explained_variance   | 3.87e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 408         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-97.93 +/- 2.30\n",
      "Episode length: 11.00 +/- 5.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11       |\n",
      "|    mean_reward     | -97.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.4     |\n",
      "|    ep_rew_mean     | -102     |\n",
      "| time/              |          |\n",
      "|    fps             | 334      |\n",
      "|    iterations      | 11       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-99.68 +/- 3.48\n",
      "Episode length: 12.60 +/- 6.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.6        |\n",
      "|    mean_reward          | -99.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012899383 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.02       |\n",
      "|    explained_variance   | 1.73e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 98.6        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 279         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=24000, episode_reward=-101.87 +/- 3.47\n",
      "Episode length: 7.40 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.4     |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-101.68 +/- 4.25\n",
      "Episode length: 11.20 +/- 6.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.2        |\n",
      "|    mean_reward          | -102        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012320183 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.01       |\n",
      "|    explained_variance   | 4.65e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 57.2        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-100.67 +/- 3.22\n",
      "Episode length: 9.60 +/- 3.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | -101     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.3     |\n",
      "|    ep_rew_mean     | -101     |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=-100.70 +/- 3.75\n",
      "Episode length: 10.60 +/- 12.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.6        |\n",
      "|    mean_reward          | -101        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012061893 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.96       |\n",
      "|    explained_variance   | 2.8e-06     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 71.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-100.20 +/- 4.59\n",
      "Episode length: 10.60 +/- 6.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -100     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.4     |\n",
      "|    ep_rew_mean     | -99.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-100.54 +/- 4.58\n",
      "Episode length: 16.20 +/- 17.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | -101        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013514342 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 9.54e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-99.08 +/- 2.27\n",
      "Episode length: 10.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -99.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.7     |\n",
      "|    ep_rew_mean     | -99.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-98.02 +/- 1.73\n",
      "Episode length: 13.20 +/- 5.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.2        |\n",
      "|    mean_reward          | -98         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011962888 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 4.05e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.1        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 27.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-99.53 +/- 1.33\n",
      "Episode length: 5.60 +/- 3.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -99.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.5     |\n",
      "|    ep_rew_mean     | -99      |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-98.11 +/- 1.60\n",
      "Episode length: 9.60 +/- 7.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | -98.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009551888 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 4.89e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 30.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-104.72 +/- 6.75\n",
      "Episode length: 17.60 +/- 14.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.6     |\n",
      "|    mean_reward     | -105     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.1     |\n",
      "|    ep_rew_mean     | -98.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 17       |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 34816    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=35000, episode_reward=-99.22 +/- 2.40\n",
      "Episode length: 16.40 +/- 11.34\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 16.4      |\n",
      "|    mean_reward          | -99.2     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 35000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0098559 |\n",
      "|    clip_fraction        | 0.144     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -2.78     |\n",
      "|    explained_variance   | 9.42e-06  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 13        |\n",
      "|    n_updates            | 170       |\n",
      "|    policy_gradient_loss | -0.0179   |\n",
      "|    value_loss           | 29.5      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-98.58 +/- 0.93\n",
      "Episode length: 9.80 +/- 6.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | -98.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.1     |\n",
      "|    ep_rew_mean     | -98.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 36864    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-98.66 +/- 2.21\n",
      "Episode length: 12.80 +/- 9.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.8        |\n",
      "|    mean_reward          | -98.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009889983 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.77       |\n",
      "|    explained_variance   | 1.07e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 31.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-97.44 +/- 1.20\n",
      "Episode length: 12.80 +/- 5.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.8     |\n",
      "|    mean_reward     | -97.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.5     |\n",
      "|    ep_rew_mean     | -98.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 19       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 38912    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-102.13 +/- 2.82\n",
      "Episode length: 12.80 +/- 6.52\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12.8       |\n",
      "|    mean_reward          | -102       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 39000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00640318 |\n",
      "|    clip_fraction        | 0.0166     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.79      |\n",
      "|    explained_variance   | 0.103      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 12.3       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00844   |\n",
      "|    value_loss           | 27.4       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-99.87 +/- 2.84\n",
      "Episode length: 7.60 +/- 3.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | -99.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.8     |\n",
      "|    ep_rew_mean     | -99.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=-98.50 +/- 2.31\n",
      "Episode length: 12.60 +/- 7.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 12.6         |\n",
      "|    mean_reward          | -98.5        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068953875 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.78        |\n",
      "|    explained_variance   | 0.0975       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00994     |\n",
      "|    value_loss           | 29.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-98.07 +/- 2.39\n",
      "Episode length: 16.60 +/- 9.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-98.01 +/- 1.17\n",
      "Episode length: 12.20 +/- 8.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.9     |\n",
      "|    ep_rew_mean     | -99.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 21       |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-99.77 +/- 2.01\n",
      "Episode length: 12.20 +/- 5.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 12.2         |\n",
      "|    mean_reward          | -99.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0089211855 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.77        |\n",
      "|    explained_variance   | 0.0872       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 27           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-97.87 +/- 2.27\n",
      "Episode length: 9.20 +/- 4.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | -97.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.8     |\n",
      "|    ep_rew_mean     | -98      |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-98.30 +/- 0.98\n",
      "Episode length: 8.00 +/- 5.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -98.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010686858 |\n",
      "|    clip_fraction        | 0.0897      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.75       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 25.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=47000, episode_reward=-97.17 +/- 0.82\n",
      "Episode length: 13.00 +/- 4.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.8     |\n",
      "|    ep_rew_mean     | -98.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 23       |\n",
      "|    time_elapsed    | 135      |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-98.91 +/- 3.90\n",
      "Episode length: 17.00 +/- 6.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17          |\n",
      "|    mean_reward          | -98.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008718331 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.71       |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 24          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-99.68 +/- 3.20\n",
      "Episode length: 15.20 +/- 7.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.2     |\n",
      "|    mean_reward     | -99.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.9     |\n",
      "|    ep_rew_mean     | -98.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-98.54 +/- 1.34\n",
      "Episode length: 7.00 +/- 4.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | -98.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008751147 |\n",
      "|    clip_fraction        | 0.0608      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.68       |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10          |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 21.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-96.53 +/- 1.90\n",
      "Episode length: 11.80 +/- 5.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | -96.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.4     |\n",
      "|    ep_rew_mean     | -98.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-97.72 +/- 2.94\n",
      "Episode length: 12.40 +/- 7.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 12.4         |\n",
      "|    mean_reward          | -97.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093032885 |\n",
      "|    clip_fraction        | 0.0807       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.66        |\n",
      "|    explained_variance   | 0.194        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.47         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.0157      |\n",
      "|    value_loss           | 19.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-97.17 +/- 1.43\n",
      "Episode length: 12.00 +/- 3.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.8     |\n",
      "|    ep_rew_mean     | -97.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 151      |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=-97.23 +/- 3.23\n",
      "Episode length: 13.40 +/- 10.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | -97.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009648127 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.53        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 21.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-96.63 +/- 2.51\n",
      "Episode length: 17.40 +/- 12.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.4     |\n",
      "|    mean_reward     | -96.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.8     |\n",
      "|    ep_rew_mean     | -98.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 55296    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=-99.30 +/- 3.39\n",
      "Episode length: 13.00 +/- 9.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13          |\n",
      "|    mean_reward          | -99.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010765033 |\n",
      "|    clip_fraction        | 0.0871      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.68        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 20.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=-97.41 +/- 1.20\n",
      "Episode length: 7.80 +/- 3.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | -97.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.7     |\n",
      "|    ep_rew_mean     | -98      |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 161      |\n",
      "|    total_timesteps | 57344    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=58000, episode_reward=-98.78 +/- 1.72\n",
      "Episode length: 12.80 +/- 10.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.8        |\n",
      "|    mean_reward          | -98.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010013619 |\n",
      "|    clip_fraction        | 0.095       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.55       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.43        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 23.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=-96.92 +/- 1.08\n",
      "Episode length: 13.20 +/- 6.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.7     |\n",
      "|    ep_rew_mean     | -97.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 29       |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 59392    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-98.79 +/- 1.58\n",
      "Episode length: 11.20 +/- 6.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.2        |\n",
      "|    mean_reward          | -98.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012080494 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 26.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=-95.90 +/- 0.58\n",
      "Episode length: 11.80 +/- 2.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.9     |\n",
      "|    ep_rew_mean     | -98.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=-96.84 +/- 0.94\n",
      "Episode length: 11.20 +/- 3.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.2        |\n",
      "|    mean_reward          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010162009 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.57       |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 24.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=-98.25 +/- 0.70\n",
      "Episode length: 6.60 +/- 2.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.6      |\n",
      "|    mean_reward     | -98.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.3     |\n",
      "|    ep_rew_mean     | -97.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 31       |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 63488    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-97.26 +/- 2.00\n",
      "Episode length: 11.60 +/- 8.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | -97.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007568907 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.42        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 21.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=-106.71 +/- 9.76\n",
      "Episode length: 22.40 +/- 21.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.4     |\n",
      "|    mean_reward     | -107     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.5     |\n",
      "|    ep_rew_mean     | -97.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 356      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=-96.82 +/- 1.64\n",
      "Episode length: 9.20 +/- 4.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010615297 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.34        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 21.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=-96.62 +/- 2.68\n",
      "Episode length: 12.80 +/- 5.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.8     |\n",
      "|    mean_reward     | -96.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.9     |\n",
      "|    ep_rew_mean     | -97.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 357      |\n",
      "|    iterations      | 33       |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=-99.05 +/- 0.82\n",
      "Episode length: 5.40 +/- 4.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5.4         |\n",
      "|    mean_reward          | -99.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009510402 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.55        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=69000, episode_reward=-97.37 +/- 2.98\n",
      "Episode length: 13.00 +/- 7.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -97.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.6     |\n",
      "|    ep_rew_mean     | -97.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-97.62 +/- 0.88\n",
      "Episode length: 7.20 +/- 2.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.2        |\n",
      "|    mean_reward          | -97.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00929936 |\n",
      "|    clip_fraction        | 0.0686     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.53      |\n",
      "|    explained_variance   | 0.206      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 9.19       |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    value_loss           | 21.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=-98.04 +/- 2.03\n",
      "Episode length: 10.40 +/- 6.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.4     |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.5     |\n",
      "|    ep_rew_mean     | -97.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 359      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-97.50 +/- 1.65\n",
      "Episode length: 14.00 +/- 5.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011544461 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.52       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.4        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 23.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-96.89 +/- 2.20\n",
      "Episode length: 14.60 +/- 5.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.6     |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.3     |\n",
      "|    ep_rew_mean     | -97.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 361      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=-97.46 +/- 1.72\n",
      "Episode length: 9.20 +/- 6.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010504031 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.54       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 27.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-97.80 +/- 1.24\n",
      "Episode length: 7.20 +/- 3.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | -97.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.6     |\n",
      "|    ep_rew_mean     | -97.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 75776    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-97.79 +/- 1.29\n",
      "Episode length: 11.40 +/- 7.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 11.4         |\n",
      "|    mean_reward          | -97.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0102012865 |\n",
      "|    clip_fraction        | 0.0894       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.182        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0161      |\n",
      "|    value_loss           | 26.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-96.03 +/- 2.30\n",
      "Episode length: 13.40 +/- 8.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.4     |\n",
      "|    mean_reward     | -96      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.5     |\n",
      "|    ep_rew_mean     | -98      |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 77824    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=-98.94 +/- 1.15\n",
      "Episode length: 5.80 +/- 6.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.8          |\n",
      "|    mean_reward          | -98.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0103886565 |\n",
      "|    clip_fraction        | 0.0972       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.51        |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.0144      |\n",
      "|    value_loss           | 25.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-98.04 +/- 1.27\n",
      "Episode length: 6.20 +/- 3.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 10.9     |\n",
      "|    ep_rew_mean     | -97.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 39       |\n",
      "|    time_elapsed    | 219      |\n",
      "|    total_timesteps | 79872    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=80000, episode_reward=-97.11 +/- 1.37\n",
      "Episode length: 15.00 +/- 10.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 15           |\n",
      "|    mean_reward          | -97.1        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077182166 |\n",
      "|    clip_fraction        | 0.0687       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.5         |\n",
      "|    explained_variance   | 0.246        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.53         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0154      |\n",
      "|    value_loss           | 24.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=-96.33 +/- 0.63\n",
      "Episode length: 10.00 +/- 1.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13       |\n",
      "|    ep_rew_mean     | -97.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=-94.69 +/- 2.64\n",
      "Episode length: 20.20 +/- 5.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.2        |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011819011 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.53       |\n",
      "|    explained_variance   | 0.0866      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 28.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=83000, episode_reward=-97.72 +/- 1.46\n",
      "Episode length: 8.80 +/- 6.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -97.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.7     |\n",
      "|    ep_rew_mean     | -97.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 41       |\n",
      "|    time_elapsed    | 231      |\n",
      "|    total_timesteps | 83968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-98.28 +/- 2.22\n",
      "Episode length: 10.60 +/- 5.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.6        |\n",
      "|    mean_reward          | -98.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010323445 |\n",
      "|    clip_fraction        | 0.0908      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 29.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-97.73 +/- 2.19\n",
      "Episode length: 8.80 +/- 7.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -97.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-96.87 +/- 3.72\n",
      "Episode length: 11.60 +/- 13.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13       |\n",
      "|    ep_rew_mean     | -97.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-95.91 +/- 3.04\n",
      "Episode length: 13.60 +/- 9.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | -95.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 87000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009530934 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 27.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=-97.54 +/- 1.59\n",
      "Episode length: 25.20 +/- 20.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.2     |\n",
      "|    mean_reward     | -97.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.9     |\n",
      "|    ep_rew_mean     | -97.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 242      |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=-97.55 +/- 0.89\n",
      "Episode length: 9.00 +/- 4.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 89000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008460404 |\n",
      "|    clip_fraction        | 0.0746      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 30.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-96.24 +/- 1.37\n",
      "Episode length: 15.80 +/- 12.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13       |\n",
      "|    ep_rew_mean     | -97.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-97.11 +/- 1.02\n",
      "Episode length: 10.80 +/- 8.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.8        |\n",
      "|    mean_reward          | -97.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 91000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011805685 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.51       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 29.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=92000, episode_reward=-98.57 +/- 1.98\n",
      "Episode length: 14.40 +/- 9.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.4     |\n",
      "|    mean_reward     | -98.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 92000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.3     |\n",
      "|    ep_rew_mean     | -97.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-97.00 +/- 1.16\n",
      "Episode length: 12.40 +/- 6.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | -97         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 93000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010401316 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.5        |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.9        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 31.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=-98.62 +/- 3.44\n",
      "Episode length: 11.60 +/- 5.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | -98.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.2     |\n",
      "|    ep_rew_mean     | -96.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 46       |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=-97.61 +/- 1.25\n",
      "Episode length: 14.00 +/- 13.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | -97.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 95000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011039477 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.47       |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.1        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 28.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-98.06 +/- 0.83\n",
      "Episode length: 8.60 +/- 3.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.6     |\n",
      "|    ep_rew_mean     | -97.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 96256    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-97.82 +/- 2.21\n",
      "Episode length: 11.00 +/- 7.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11          |\n",
      "|    mean_reward          | -97.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 97000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011407917 |\n",
      "|    clip_fraction        | 0.097       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.49       |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 33.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-98.08 +/- 3.05\n",
      "Episode length: 10.40 +/- 4.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.4     |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.2     |\n",
      "|    ep_rew_mean     | -97.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 48       |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 98304    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-99.60 +/- 5.11\n",
      "Episode length: 14.00 +/- 9.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | -99.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 99000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011975723 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.48       |\n",
      "|    explained_variance   | -0.0274     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20          |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 37.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-96.08 +/- 2.30\n",
      "Episode length: 11.40 +/- 5.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | -96.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 11.4     |\n",
      "|    ep_rew_mean     | -97.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 273      |\n",
      "|    total_timesteps | 100352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-98.26 +/- 1.62\n",
      "Episode length: 18.00 +/- 14.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | -98.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 101000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010715848 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.1        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 40.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=-96.04 +/- 3.69\n",
      "Episode length: 17.40 +/- 20.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.4     |\n",
      "|    mean_reward     | -96      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 102000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 12.5     |\n",
      "|    ep_rew_mean     | -97.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 368      |\n",
      "|    iterations      | 50       |\n",
      "|    time_elapsed    | 278      |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=103000, episode_reward=-97.03 +/- 1.54\n",
      "Episode length: 8.00 +/- 3.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -97         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 103000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013362846 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.1        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 36.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-97.88 +/- 1.81\n",
      "Episode length: 10.60 +/- 6.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -97.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 104000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.8     |\n",
      "|    ep_rew_mean     | -97.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 369      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 282      |\n",
      "|    total_timesteps | 104448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-97.56 +/- 0.53\n",
      "Episode length: 10.40 +/- 4.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.4        |\n",
      "|    mean_reward          | -97.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 105000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010681535 |\n",
      "|    clip_fraction        | 0.0917      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.4        |\n",
      "|    explained_variance   | 0.0904      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 40.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-96.45 +/- 3.54\n",
      "Episode length: 10.00 +/- 8.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -96.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 106000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.1     |\n",
      "|    ep_rew_mean     | -97.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 370      |\n",
      "|    iterations      | 52       |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 106496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-96.65 +/- 0.98\n",
      "Episode length: 10.20 +/- 3.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | -96.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 107000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011659071 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 0.0295      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 43          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-98.30 +/- 1.59\n",
      "Episode length: 12.00 +/- 12.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | -98.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 108000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.8     |\n",
      "|    ep_rew_mean     | -97.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 370      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-95.45 +/- 2.81\n",
      "Episode length: 11.80 +/- 5.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 109000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009027824 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.42       |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 39.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-97.32 +/- 2.04\n",
      "Episode length: 7.60 +/- 4.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | -97.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.1     |\n",
      "|    ep_rew_mean     | -97.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 371      |\n",
      "|    iterations      | 54       |\n",
      "|    time_elapsed    | 297      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-95.83 +/- 1.17\n",
      "Episode length: 16.60 +/- 8.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.6        |\n",
      "|    mean_reward          | -95.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 111000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011487499 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 0.0843      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.3        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-95.69 +/- 3.77\n",
      "Episode length: 18.20 +/- 9.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 112000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.1     |\n",
      "|    ep_rew_mean     | -97.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 372      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=-96.35 +/- 2.04\n",
      "Episode length: 18.00 +/- 6.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 113000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011962527 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 0.0557      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 42.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=114000, episode_reward=-93.58 +/- 1.95\n",
      "Episode length: 17.80 +/- 6.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | -93.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 114000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.2     |\n",
      "|    ep_rew_mean     | -96.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 372      |\n",
      "|    iterations      | 56       |\n",
      "|    time_elapsed    | 307      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-98.08 +/- 2.43\n",
      "Episode length: 12.00 +/- 7.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | -98.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 115000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012664106 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.3        |\n",
      "|    explained_variance   | 0.0387      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-95.85 +/- 3.10\n",
      "Episode length: 12.40 +/- 7.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | -95.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 116000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18       |\n",
      "|    ep_rew_mean     | -97.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 372      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-96.24 +/- 1.71\n",
      "Episode length: 10.40 +/- 4.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 10.4         |\n",
      "|    mean_reward          | -96.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 117000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0119750975 |\n",
      "|    clip_fraction        | 0.0885       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.3         |\n",
      "|    explained_variance   | 0.0126       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27           |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.0161      |\n",
      "|    value_loss           | 51           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-99.62 +/- 9.10\n",
      "Episode length: 28.60 +/- 21.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.6     |\n",
      "|    mean_reward     | -99.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 118000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.8     |\n",
      "|    ep_rew_mean     | -96.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 372      |\n",
      "|    iterations      | 58       |\n",
      "|    time_elapsed    | 319      |\n",
      "|    total_timesteps | 118784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-95.33 +/- 1.69\n",
      "Episode length: 17.60 +/- 10.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17.6       |\n",
      "|    mean_reward          | -95.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 119000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01064853 |\n",
      "|    clip_fraction        | 0.0958     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.3       |\n",
      "|    explained_variance   | 0.119      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 27.5       |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    value_loss           | 62.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-98.67 +/- 1.10\n",
      "Episode length: 7.80 +/- 5.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | -98.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 13.5     |\n",
      "|    ep_rew_mean     | -96.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 369      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 120832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=-92.99 +/- 4.98\n",
      "Episode length: 22.80 +/- 16.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -93         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 121000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012567103 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 0.00196     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.9        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 56.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=122000, episode_reward=-98.08 +/- 1.09\n",
      "Episode length: 6.20 +/- 2.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.2      |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 122000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -96.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 60       |\n",
      "|    time_elapsed    | 334      |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=-96.60 +/- 4.38\n",
      "Episode length: 26.80 +/- 26.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.8        |\n",
      "|    mean_reward          | -96.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 123000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010107981 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.21       |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 50          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-94.99 +/- 3.46\n",
      "Episode length: 16.20 +/- 8.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 124000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.1     |\n",
      "|    ep_rew_mean     | -95.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 124928   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=125000, episode_reward=-96.00 +/- 4.72\n",
      "Episode length: 20.40 +/- 15.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -96         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 125000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009905076 |\n",
      "|    clip_fraction        | 0.0812      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.5        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 63.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-95.09 +/- 3.63\n",
      "Episode length: 13.20 +/- 9.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -95.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 126000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.4     |\n",
      "|    ep_rew_mean     | -96      |\n",
      "| time/              |          |\n",
      "|    fps             | 368      |\n",
      "|    iterations      | 62       |\n",
      "|    time_elapsed    | 344      |\n",
      "|    total_timesteps | 126976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=-93.97 +/- 5.44\n",
      "Episode length: 27.20 +/- 19.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.2        |\n",
      "|    mean_reward          | -94         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 127000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013047917 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 57.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-94.75 +/- 4.38\n",
      "Episode length: 14.20 +/- 12.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-96.62 +/- 1.28\n",
      "Episode length: 11.80 +/- 7.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | -96.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 129000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16       |\n",
      "|    ep_rew_mean     | -96.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 351      |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-95.02 +/- 3.81\n",
      "Episode length: 42.80 +/- 38.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 42.8        |\n",
      "|    mean_reward          | -95         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008558705 |\n",
      "|    clip_fraction        | 0.0858      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 58.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-96.22 +/- 3.85\n",
      "Episode length: 18.00 +/- 9.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 131000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.6     |\n",
      "|    ep_rew_mean     | -95.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 64       |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-96.20 +/- 2.66\n",
      "Episode length: 13.60 +/- 7.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13.6       |\n",
      "|    mean_reward          | -96.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 132000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00904535 |\n",
      "|    clip_fraction        | 0.0746     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.14      |\n",
      "|    explained_variance   | 0.113      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 34.3       |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    value_loss           | 69.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-95.62 +/- 3.27\n",
      "Episode length: 31.20 +/- 27.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 31.2     |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 133000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.1     |\n",
      "|    ep_rew_mean     | -95.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=-96.84 +/- 2.90\n",
      "Episode length: 11.60 +/- 7.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 134000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008941878 |\n",
      "|    clip_fraction        | 0.0725      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | 0.0231      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=-96.97 +/- 3.63\n",
      "Episode length: 11.20 +/- 8.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | -97      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -96.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 66       |\n",
      "|    time_elapsed    | 367      |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-93.01 +/- 2.12\n",
      "Episode length: 28.00 +/- 15.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 28          |\n",
      "|    mean_reward          | -93         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 136000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010969826 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.0452      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.4        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 54.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=137000, episode_reward=-94.51 +/- 3.55\n",
      "Episode length: 19.20 +/- 10.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.2     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 137000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -96      |\n",
      "| time/              |          |\n",
      "|    fps             | 368      |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 372      |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=-97.66 +/- 1.61\n",
      "Episode length: 13.00 +/- 7.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13          |\n",
      "|    mean_reward          | -97.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 138000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012047344 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.02       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.2        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 64.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-97.64 +/- 1.85\n",
      "Episode length: 7.40 +/- 5.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | -97.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 139000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14       |\n",
      "|    ep_rew_mean     | -96.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 369      |\n",
      "|    iterations      | 68       |\n",
      "|    time_elapsed    | 377      |\n",
      "|    total_timesteps | 139264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-95.85 +/- 1.62\n",
      "Episode length: 11.00 +/- 4.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11          |\n",
      "|    mean_reward          | -95.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010598153 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2          |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 62.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=-93.83 +/- 8.64\n",
      "Episode length: 56.60 +/- 45.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 56.6     |\n",
      "|    mean_reward     | -93.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 141000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -95.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 384      |\n",
      "|    total_timesteps | 141312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-98.18 +/- 1.52\n",
      "Episode length: 7.80 +/- 1.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.8         |\n",
      "|    mean_reward          | -98.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 142000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012113023 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.96       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.4        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 73.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-97.15 +/- 2.76\n",
      "Episode length: 13.20 +/- 9.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -97.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 143000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.7     |\n",
      "|    ep_rew_mean     | -95.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 70       |\n",
      "|    time_elapsed    | 389      |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=-93.82 +/- 4.80\n",
      "Episode length: 18.60 +/- 16.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.6        |\n",
      "|    mean_reward          | -93.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 144000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015422071 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | 0.0134      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 66.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=-98.57 +/- 2.25\n",
      "Episode length: 7.00 +/- 2.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -98.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18       |\n",
      "|    ep_rew_mean     | -95.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 368      |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 395      |\n",
      "|    total_timesteps | 145408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-94.20 +/- 3.39\n",
      "Episode length: 25.20 +/- 21.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.2        |\n",
      "|    mean_reward          | -94.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 146000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009716922 |\n",
      "|    clip_fraction        | 0.0743      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2          |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.1        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 74.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=-96.90 +/- 1.73\n",
      "Episode length: 10.60 +/- 6.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 147000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -95.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 72       |\n",
      "|    time_elapsed    | 400      |\n",
      "|    total_timesteps | 147456   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=148000, episode_reward=-94.33 +/- 6.78\n",
      "Episode length: 19.40 +/- 22.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19.4        |\n",
      "|    mean_reward          | -94.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 148000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009580988 |\n",
      "|    clip_fraction        | 0.0807      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.222       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28          |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 74.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-95.08 +/- 4.14\n",
      "Episode length: 17.00 +/- 16.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17       |\n",
      "|    mean_reward     | -95.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 149000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.5     |\n",
      "|    ep_rew_mean     | -95.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 366      |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 407      |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-94.53 +/- 3.55\n",
      "Episode length: 14.40 +/- 8.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | -94.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009875616 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.97       |\n",
      "|    explained_variance   | 0.0916      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41          |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 83.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=-95.41 +/- 2.80\n",
      "Episode length: 14.60 +/- 5.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.6     |\n",
      "|    mean_reward     | -95.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 151000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -95.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 74       |\n",
      "|    time_elapsed    | 416      |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-97.50 +/- 0.98\n",
      "Episode length: 21.60 +/- 25.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.6        |\n",
      "|    mean_reward          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 152000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011632619 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26          |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 76.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-96.93 +/- 4.21\n",
      "Episode length: 14.20 +/- 8.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 153000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.2     |\n",
      "|    ep_rew_mean     | -95.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 422      |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=-95.08 +/- 6.60\n",
      "Episode length: 18.80 +/- 27.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.8         |\n",
      "|    mean_reward          | -95.1        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 154000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0101257805 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | 0.257        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 32.6         |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.0183      |\n",
      "|    value_loss           | 88           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=-95.58 +/- 2.38\n",
      "Episode length: 11.80 +/- 5.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -95.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 76       |\n",
      "|    time_elapsed    | 427      |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=-91.71 +/- 4.31\n",
      "Episode length: 52.80 +/- 56.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 52.8        |\n",
      "|    mean_reward          | -91.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 156000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010057756 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 67          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=157000, episode_reward=-94.46 +/- 5.89\n",
      "Episode length: 16.80 +/- 16.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.8     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 157000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -95.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 432      |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-97.16 +/- 4.11\n",
      "Episode length: 10.00 +/- 15.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10          |\n",
      "|    mean_reward          | -97.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 158000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009280479 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 67.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=159000, episode_reward=-95.61 +/- 3.77\n",
      "Episode length: 11.20 +/- 7.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 159000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | -95      |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 78       |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-92.84 +/- 3.11\n",
      "Episode length: 18.20 +/- 5.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | -92.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009722708 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.91       |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 70.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=-96.44 +/- 2.93\n",
      "Episode length: 11.60 +/- 5.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | -96.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 161000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.5     |\n",
      "|    ep_rew_mean     | -95.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 442      |\n",
      "|    total_timesteps | 161792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=-95.75 +/- 3.30\n",
      "Episode length: 12.40 +/- 9.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | -95.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 162000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013617388 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.0191      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 67.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-94.32 +/- 3.96\n",
      "Episode length: 16.80 +/- 9.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.8     |\n",
      "|    mean_reward     | -94.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 163000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | -95.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 80       |\n",
      "|    time_elapsed    | 448      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=-91.52 +/- 6.76\n",
      "Episode length: 29.00 +/- 21.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29          |\n",
      "|    mean_reward          | -91.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 164000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009480514 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.0437      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.5        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 62.6        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=165000, episode_reward=-93.54 +/- 4.58\n",
      "Episode length: 20.40 +/- 13.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -93.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -95.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 453      |\n",
      "|    total_timesteps | 165888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=-96.14 +/- 3.06\n",
      "Episode length: 12.20 +/- 6.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.2        |\n",
      "|    mean_reward          | -96.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 166000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009941148 |\n",
      "|    clip_fraction        | 0.0968      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.0978      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.4        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 75          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-97.17 +/- 1.20\n",
      "Episode length: 7.20 +/- 3.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 167000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.6     |\n",
      "|    ep_rew_mean     | -95.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 82       |\n",
      "|    time_elapsed    | 461      |\n",
      "|    total_timesteps | 167936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=-92.22 +/- 5.23\n",
      "Episode length: 49.40 +/- 44.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 49.4        |\n",
      "|    mean_reward          | -92.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 168000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010918258 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.5        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 78          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-95.42 +/- 3.87\n",
      "Episode length: 17.80 +/- 13.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | -95.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 169000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.3     |\n",
      "|    ep_rew_mean     | -95.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 468      |\n",
      "|    total_timesteps | 169984   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=170000, episode_reward=-92.48 +/- 4.54\n",
      "Episode length: 32.00 +/- 31.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 32          |\n",
      "|    mean_reward          | -92.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 170000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008192968 |\n",
      "|    clip_fraction        | 0.0947      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.3        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 83.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=-96.71 +/- 2.23\n",
      "Episode length: 11.40 +/- 9.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | -96.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 171000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-95.34 +/- 4.82\n",
      "Episode length: 14.00 +/- 13.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | -95.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 172000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -95.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 474      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=-95.58 +/- 2.98\n",
      "Episode length: 17.80 +/- 15.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17.8       |\n",
      "|    mean_reward          | -95.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 173000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01084421 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.74      |\n",
      "|    explained_variance   | 0.286      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 33.5       |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    value_loss           | 71.3       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-98.31 +/- 1.26\n",
      "Episode length: 5.60 +/- 2.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.6      |\n",
      "|    mean_reward     | -98.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 174000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.7     |\n",
      "|    ep_rew_mean     | -95.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 85       |\n",
      "|    time_elapsed    | 479      |\n",
      "|    total_timesteps | 174080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=-96.15 +/- 2.71\n",
      "Episode length: 13.60 +/- 8.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | -96.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 175000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009625031 |\n",
      "|    clip_fraction        | 0.0933      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.77       |\n",
      "|    explained_variance   | 0.0965      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.6        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 76.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=-91.36 +/- 6.74\n",
      "Episode length: 22.80 +/- 17.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -91.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 176000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.9     |\n",
      "|    ep_rew_mean     | -95      |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 484      |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-92.75 +/- 4.94\n",
      "Episode length: 23.80 +/- 20.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.8        |\n",
      "|    mean_reward          | -92.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 177000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013942586 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 70.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-94.98 +/- 3.64\n",
      "Episode length: 16.00 +/- 7.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 178000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.2     |\n",
      "|    ep_rew_mean     | -95      |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 87       |\n",
      "|    time_elapsed    | 489      |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=-95.87 +/- 2.95\n",
      "Episode length: 11.00 +/- 7.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11          |\n",
      "|    mean_reward          | -95.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 179000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009722922 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.69       |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.2        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 76.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-95.95 +/- 1.61\n",
      "Episode length: 12.60 +/- 4.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.6     |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17       |\n",
      "|    ep_rew_mean     | -95.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 494      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=-94.82 +/- 2.26\n",
      "Episode length: 17.80 +/- 8.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.8        |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 181000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008806521 |\n",
      "|    clip_fraction        | 0.0916      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.7        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 75.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=182000, episode_reward=-97.31 +/- 1.49\n",
      "Episode length: 8.60 +/- 3.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | -97.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 182000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | -94.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 89       |\n",
      "|    time_elapsed    | 499      |\n",
      "|    total_timesteps | 182272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=-96.44 +/- 2.03\n",
      "Episode length: 9.20 +/- 4.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.2        |\n",
      "|    mean_reward          | -96.4      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 183000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00958117 |\n",
      "|    clip_fraction        | 0.0842     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.63      |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 49.6       |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    value_loss           | 79.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-93.66 +/- 4.51\n",
      "Episode length: 24.80 +/- 21.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.8     |\n",
      "|    mean_reward     | -93.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 184000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 14.2     |\n",
      "|    ep_rew_mean     | -95.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 505      |\n",
      "|    total_timesteps | 184320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-96.55 +/- 1.69\n",
      "Episode length: 10.40 +/- 4.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.4        |\n",
      "|    mean_reward          | -96.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 185000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010621453 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.8        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 82.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-94.42 +/- 7.34\n",
      "Episode length: 22.60 +/- 22.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 186000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | -94.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 91       |\n",
      "|    time_elapsed    | 510      |\n",
      "|    total_timesteps | 186368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-97.06 +/- 1.95\n",
      "Episode length: 11.80 +/- 7.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | -97.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 187000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011147773 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.68       |\n",
      "|    explained_variance   | 0.0391      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 80.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=-97.08 +/- 4.40\n",
      "Episode length: 11.00 +/- 11.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11       |\n",
      "|    mean_reward     | -97.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 188000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.8     |\n",
      "|    ep_rew_mean     | -94.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 515      |\n",
      "|    total_timesteps | 188416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=-95.32 +/- 4.67\n",
      "Episode length: 16.00 +/- 11.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | -95.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 189000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009930258 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.5        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 72.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-92.77 +/- 9.69\n",
      "Episode length: 18.20 +/- 22.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 93       |\n",
      "|    time_elapsed    | 521      |\n",
      "|    total_timesteps | 190464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=-90.64 +/- 12.72\n",
      "Episode length: 45.80 +/- 50.94\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 45.8       |\n",
      "|    mean_reward          | -90.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 191000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01042172 |\n",
      "|    clip_fraction        | 0.0919     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.61      |\n",
      "|    explained_variance   | 0.15       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 33.7       |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    value_loss           | 78.5       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=192000, episode_reward=-96.10 +/- 2.49\n",
      "Episode length: 11.80 +/- 6.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | -96.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.7     |\n",
      "|    ep_rew_mean     | -95.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 529      |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=193000, episode_reward=-92.89 +/- 8.55\n",
      "Episode length: 26.80 +/- 25.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.8        |\n",
      "|    mean_reward          | -92.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 193000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010447769 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.8        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 88.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=-95.89 +/- 2.22\n",
      "Episode length: 11.80 +/- 6.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 194000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.5     |\n",
      "|    ep_rew_mean     | -94.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 95       |\n",
      "|    time_elapsed    | 535      |\n",
      "|    total_timesteps | 194560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-95.12 +/- 2.92\n",
      "Episode length: 15.60 +/- 10.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.6        |\n",
      "|    mean_reward          | -95.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 195000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012302562 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.9        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 84.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=-95.03 +/- 6.04\n",
      "Episode length: 20.40 +/- 20.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 196000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 540      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=-95.56 +/- 2.16\n",
      "Episode length: 14.20 +/- 9.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.2        |\n",
      "|    mean_reward          | -95.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 197000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010825999 |\n",
      "|    clip_fraction        | 0.0849      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.1        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 82.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=-88.54 +/- 7.65\n",
      "Episode length: 35.80 +/- 28.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 35.8     |\n",
      "|    mean_reward     | -88.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 198000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18       |\n",
      "|    ep_rew_mean     | -94.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 97       |\n",
      "|    time_elapsed    | 545      |\n",
      "|    total_timesteps | 198656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-95.47 +/- 3.48\n",
      "Episode length: 18.40 +/- 12.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | -95.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 199000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009381024 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 81.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-90.78 +/- 10.97\n",
      "Episode length: 40.00 +/- 60.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 40       |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.7     |\n",
      "|    ep_rew_mean     | -95.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 550      |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=-88.87 +/- 10.54\n",
      "Episode length: 48.20 +/- 58.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 48.2        |\n",
      "|    mean_reward          | -88.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 201000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010855699 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 74.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=-92.58 +/- 5.94\n",
      "Episode length: 19.40 +/- 12.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.4     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 202000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | -94.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 99       |\n",
      "|    time_elapsed    | 556      |\n",
      "|    total_timesteps | 202752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=-94.73 +/- 6.99\n",
      "Episode length: 16.80 +/- 17.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.8        |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 203000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008527635 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 76.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=204000, episode_reward=-94.46 +/- 4.76\n",
      "Episode length: 15.40 +/- 13.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.4     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 204000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.4     |\n",
      "|    ep_rew_mean     | -95.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 561      |\n",
      "|    total_timesteps | 204800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=-93.61 +/- 4.74\n",
      "Episode length: 25.80 +/- 24.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.8        |\n",
      "|    mean_reward          | -93.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 205000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010163769 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.2        |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 78.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=-92.11 +/- 4.13\n",
      "Episode length: 20.00 +/- 10.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | -92.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 206000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 364      |\n",
      "|    iterations      | 101      |\n",
      "|    time_elapsed    | 566      |\n",
      "|    total_timesteps | 206848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=-97.42 +/- 2.15\n",
      "Episode length: 10.60 +/- 7.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 10.6         |\n",
      "|    mean_reward          | -97.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 207000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0116393035 |\n",
      "|    clip_fraction        | 0.0947       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.315        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.5         |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.0167      |\n",
      "|    value_loss           | 71.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=-97.50 +/- 1.94\n",
      "Episode length: 9.80 +/- 4.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | -97.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 208000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.7     |\n",
      "|    ep_rew_mean     | -94.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 571      |\n",
      "|    total_timesteps | 208896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=-96.61 +/- 2.81\n",
      "Episode length: 9.60 +/- 6.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | -96.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 209000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013454921 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.7        |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 80.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=-99.01 +/- 2.11\n",
      "Episode length: 7.20 +/- 7.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.2      |\n",
      "|    mean_reward     | -99      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.8     |\n",
      "|    ep_rew_mean     | -95.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 365      |\n",
      "|    iterations      | 103      |\n",
      "|    time_elapsed    | 577      |\n",
      "|    total_timesteps | 210944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=-95.28 +/- 4.18\n",
      "Episode length: 12.40 +/- 10.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | -95.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 211000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009917878 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.295       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.1        |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 69.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=-91.76 +/- 8.87\n",
      "Episode length: 26.80 +/- 31.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.8     |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 212000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 585      |\n",
      "|    total_timesteps | 212992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=-93.34 +/- 7.53\n",
      "Episode length: 20.80 +/- 25.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.8        |\n",
      "|    mean_reward          | -93.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 213000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009469348 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.00925    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 61.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=-89.06 +/- 6.90\n",
      "Episode length: 30.00 +/- 22.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -89.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 214000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=-89.23 +/- 2.64\n",
      "Episode length: 33.40 +/- 20.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.4     |\n",
      "|    mean_reward     | -89.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 215000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 105      |\n",
      "|    time_elapsed    | 593      |\n",
      "|    total_timesteps | 215040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=-98.15 +/- 0.49\n",
      "Episode length: 6.00 +/- 1.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | -98.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 216000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012587482 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.5        |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 65.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=-88.72 +/- 9.84\n",
      "Episode length: 50.40 +/- 59.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 50.4     |\n",
      "|    mean_reward     | -88.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 217000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -94.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 599      |\n",
      "|    total_timesteps | 217088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=-97.13 +/- 2.61\n",
      "Episode length: 8.20 +/- 6.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | -97.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 218000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009483744 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.1        |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 82.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=-94.10 +/- 9.46\n",
      "Episode length: 24.00 +/- 26.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24       |\n",
      "|    mean_reward     | -94.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 219000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.3     |\n",
      "|    ep_rew_mean     | -95.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 107      |\n",
      "|    time_elapsed    | 604      |\n",
      "|    total_timesteps | 219136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=-95.26 +/- 2.66\n",
      "Episode length: 13.80 +/- 10.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | -95.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012724377 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 75.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=-92.54 +/- 7.19\n",
      "Episode length: 22.20 +/- 21.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.2     |\n",
      "|    mean_reward     | -92.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 221000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -95.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 609      |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=-93.31 +/- 3.40\n",
      "Episode length: 23.40 +/- 14.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.4        |\n",
      "|    mean_reward          | -93.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 222000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015196122 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -0.0128     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=-94.26 +/- 4.22\n",
      "Episode length: 16.20 +/- 9.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | -94.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 223000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 109      |\n",
      "|    time_elapsed    | 615      |\n",
      "|    total_timesteps | 223232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=-95.02 +/- 6.27\n",
      "Episode length: 19.60 +/- 28.24\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 19.6         |\n",
      "|    mean_reward          | -95          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 224000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0123238545 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.51        |\n",
      "|    explained_variance   | -0.00274     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.2         |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.0187      |\n",
      "|    value_loss           | 70.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=-90.92 +/- 6.67\n",
      "Episode length: 33.40 +/- 20.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.4     |\n",
      "|    mean_reward     | -90.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 225000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 621      |\n",
      "|    total_timesteps | 225280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=-96.70 +/- 1.08\n",
      "Episode length: 10.20 +/- 2.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | -96.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 226000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008918275 |\n",
      "|    clip_fraction        | 0.0905      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.0725      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.1        |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 72.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=227000, episode_reward=-95.66 +/- 2.95\n",
      "Episode length: 14.60 +/- 8.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.6     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 227000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | -94.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 111      |\n",
      "|    time_elapsed    | 626      |\n",
      "|    total_timesteps | 227328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=-90.52 +/- 4.67\n",
      "Episode length: 46.40 +/- 39.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 46.4        |\n",
      "|    mean_reward          | -90.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 228000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011769613 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.9        |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 93.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=-95.68 +/- 4.45\n",
      "Episode length: 13.40 +/- 9.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.4     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 229000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.5     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 361      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 633      |\n",
      "|    total_timesteps | 229376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=-98.33 +/- 3.10\n",
      "Episode length: 6.20 +/- 5.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | -98.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 230000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009789636 |\n",
      "|    clip_fraction        | 0.0892      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.9        |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 95.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=-93.52 +/- 7.51\n",
      "Episode length: 22.00 +/- 28.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22       |\n",
      "|    mean_reward     | -93.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 231000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.2     |\n",
      "|    ep_rew_mean     | -94.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 361      |\n",
      "|    iterations      | 113      |\n",
      "|    time_elapsed    | 640      |\n",
      "|    total_timesteps | 231424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=-96.45 +/- 1.52\n",
      "Episode length: 12.60 +/- 7.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.6        |\n",
      "|    mean_reward          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 232000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010791302 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.6        |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 78.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=-92.03 +/- 4.20\n",
      "Episode length: 28.60 +/- 11.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.6     |\n",
      "|    mean_reward     | -92      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 233000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.7     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 361      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 645      |\n",
      "|    total_timesteps | 233472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=-92.18 +/- 6.53\n",
      "Episode length: 44.20 +/- 56.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 44.2        |\n",
      "|    mean_reward          | -92.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 234000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009772408 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.1        |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 72.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=-93.77 +/- 3.32\n",
      "Episode length: 18.80 +/- 7.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | -93.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.8     |\n",
      "|    ep_rew_mean     | -94.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 115      |\n",
      "|    time_elapsed    | 650      |\n",
      "|    total_timesteps | 235520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=-94.27 +/- 3.39\n",
      "Episode length: 17.00 +/- 6.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17          |\n",
      "|    mean_reward          | -94.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 236000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011655301 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.226       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 1150        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 83.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=-98.61 +/- 1.46\n",
      "Episode length: 5.00 +/- 4.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | -98.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 237000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.7     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 362      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 655      |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=238000, episode_reward=-96.78 +/- 2.20\n",
      "Episode length: 9.60 +/- 6.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 238000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012392931 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.0851      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 78.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=-91.48 +/- 8.43\n",
      "Episode length: 28.40 +/- 33.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.4     |\n",
      "|    mean_reward     | -91.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 239000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.7     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 117      |\n",
      "|    time_elapsed    | 659      |\n",
      "|    total_timesteps | 239616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-96.85 +/- 3.17\n",
      "Episode length: 9.60 +/- 8.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.6          |\n",
      "|    mean_reward          | -96.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 240000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0138937775 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.1          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28           |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -0.0225      |\n",
      "|    value_loss           | 86.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=-97.84 +/- 0.82\n",
      "Episode length: 9.40 +/- 8.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | -97.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 241000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 15.3     |\n",
      "|    ep_rew_mean     | -96      |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 664      |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=-95.09 +/- 6.18\n",
      "Episode length: 18.60 +/- 14.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.6        |\n",
      "|    mean_reward          | -95.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 242000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013371927 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.8        |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 84.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=-97.22 +/- 1.59\n",
      "Episode length: 9.00 +/- 3.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 243000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -94.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 119      |\n",
      "|    time_elapsed    | 670      |\n",
      "|    total_timesteps | 243712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=-96.27 +/- 4.38\n",
      "Episode length: 13.40 +/- 16.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | -96.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 244000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011817515 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.7        |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 75.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-92.70 +/- 6.47\n",
      "Episode length: 30.60 +/- 16.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.6     |\n",
      "|    mean_reward     | -92.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 245000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.9     |\n",
      "|    ep_rew_mean     | -95.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 363      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 677      |\n",
      "|    total_timesteps | 245760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-86.69 +/- 2.92\n",
      "Episode length: 45.00 +/- 10.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 45          |\n",
      "|    mean_reward          | -86.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 246000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011091232 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.0389      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.6        |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 85          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=247000, episode_reward=-95.60 +/- 3.43\n",
      "Episode length: 13.60 +/- 9.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.6     |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 247000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.3     |\n",
      "|    ep_rew_mean     | -95.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 361      |\n",
      "|    iterations      | 121      |\n",
      "|    time_elapsed    | 686      |\n",
      "|    total_timesteps | 247808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=-96.87 +/- 3.24\n",
      "Episode length: 15.00 +/- 10.37\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | -96.9      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 248000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01573208 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.5       |\n",
      "|    explained_variance   | 0.1        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 28         |\n",
      "|    n_updates            | 1210       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    value_loss           | 79.9       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=249000, episode_reward=-94.83 +/- 8.41\n",
      "Episode length: 16.60 +/- 26.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 249000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -94.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 359      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 694      |\n",
      "|    total_timesteps | 249856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=-96.65 +/- 3.64\n",
      "Episode length: 9.80 +/- 9.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | -96.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 250000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012974471 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.2        |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 68.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=-92.76 +/- 4.93\n",
      "Episode length: 49.60 +/- 46.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 49.6     |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 251000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.2     |\n",
      "|    ep_rew_mean     | -95.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 360      |\n",
      "|    iterations      | 123      |\n",
      "|    time_elapsed    | 699      |\n",
      "|    total_timesteps | 251904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=-93.64 +/- 4.18\n",
      "Episode length: 20.40 +/- 8.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -93.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 252000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013479182 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.2        |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 70          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=-96.45 +/- 3.30\n",
      "Episode length: 12.20 +/- 7.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | -96.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 253000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 360      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 704      |\n",
      "|    total_timesteps | 253952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=-93.93 +/- 2.01\n",
      "Episode length: 16.40 +/- 4.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | -93.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 254000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010227531 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.6        |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 74.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=-95.88 +/- 3.42\n",
      "Episode length: 10.60 +/- 6.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 255000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-95.76 +/- 4.09\n",
      "Episode length: 16.00 +/- 12.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | -95.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17       |\n",
      "|    ep_rew_mean     | -94.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 360      |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 711      |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=-94.01 +/- 6.26\n",
      "Episode length: 17.40 +/- 18.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.4        |\n",
      "|    mean_reward          | -94         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 257000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012483726 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.037       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 81.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=-92.42 +/- 3.16\n",
      "Episode length: 23.40 +/- 19.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.4     |\n",
      "|    mean_reward     | -92.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 258000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 360      |\n",
      "|    iterations      | 126      |\n",
      "|    time_elapsed    | 716      |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=-95.20 +/- 5.83\n",
      "Episode length: 16.20 +/- 22.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | -95.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 259000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010037037 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0927      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.1        |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 76.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-92.59 +/- 5.88\n",
      "Episode length: 22.80 +/- 20.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.8     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -94.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 360      |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 722      |\n",
      "|    total_timesteps | 260096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=-87.05 +/- 14.39\n",
      "Episode length: 46.20 +/- 50.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 46.2        |\n",
      "|    mean_reward          | -87         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 261000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012534758 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.3        |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 68.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=-94.94 +/- 5.74\n",
      "Episode length: 20.80 +/- 28.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -94.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 262000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -94.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 128      |\n",
      "|    time_elapsed    | 730      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=-94.54 +/- 2.80\n",
      "Episode length: 18.00 +/- 9.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18           |\n",
      "|    mean_reward          | -94.5        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 263000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0111910105 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 46.6         |\n",
      "|    n_updates            | 1280         |\n",
      "|    policy_gradient_loss | -0.0158      |\n",
      "|    value_loss           | 71.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=-94.94 +/- 5.18\n",
      "Episode length: 16.20 +/- 14.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | -94.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 264000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.3     |\n",
      "|    ep_rew_mean     | -94.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 357      |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 738      |\n",
      "|    total_timesteps | 264192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=-95.35 +/- 1.67\n",
      "Episode length: 12.60 +/- 4.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.6        |\n",
      "|    mean_reward          | -95.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 265000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010811856 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.7        |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 88          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=-93.09 +/- 3.14\n",
      "Episode length: 17.80 +/- 6.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | -93.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 266000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 130      |\n",
      "|    time_elapsed    | 743      |\n",
      "|    total_timesteps | 266240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=-85.45 +/- 7.81\n",
      "Episode length: 44.80 +/- 28.56\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 44.8       |\n",
      "|    mean_reward          | -85.4      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 267000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01196729 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0.169      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 43.9       |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    value_loss           | 83.5       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=268000, episode_reward=-92.98 +/- 7.39\n",
      "Episode length: 35.00 +/- 28.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 35       |\n",
      "|    mean_reward     | -93      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 268000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.1     |\n",
      "|    ep_rew_mean     | -94.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 749      |\n",
      "|    total_timesteps | 268288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=-94.81 +/- 3.18\n",
      "Episode length: 17.80 +/- 5.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.8        |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 269000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009135414 |\n",
      "|    clip_fraction        | 0.0955      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.00378     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.9        |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 95.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=-97.18 +/- 3.12\n",
      "Episode length: 8.60 +/- 8.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | -94.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 754      |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=-97.58 +/- 2.98\n",
      "Episode length: 6.80 +/- 6.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | -97.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 271000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011469243 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.7        |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 80.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=272000, episode_reward=-96.01 +/- 2.22\n",
      "Episode length: 12.40 +/- 5.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | -96      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 272000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.9     |\n",
      "|    ep_rew_mean     | -94.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 358      |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 759      |\n",
      "|    total_timesteps | 272384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=-94.61 +/- 2.57\n",
      "Episode length: 18.60 +/- 10.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.6        |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 273000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008219688 |\n",
      "|    clip_fraction        | 0.0947      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.0991      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.7        |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 80.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=-93.32 +/- 3.90\n",
      "Episode length: 25.00 +/- 14.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -93.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 274000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.2     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 357      |\n",
      "|    iterations      | 134      |\n",
      "|    time_elapsed    | 768      |\n",
      "|    total_timesteps | 274432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=-95.27 +/- 3.12\n",
      "Episode length: 12.40 +/- 7.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | -95.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 275000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013507543 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 72.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=-92.91 +/- 7.01\n",
      "Episode length: 23.40 +/- 24.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.4     |\n",
      "|    mean_reward     | -92.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 276000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 776      |\n",
      "|    total_timesteps | 276480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=-94.21 +/- 10.69\n",
      "Episode length: 19.00 +/- 34.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19          |\n",
      "|    mean_reward          | -94.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 277000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012851503 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.0689      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 81.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=-90.28 +/- 12.07\n",
      "Episode length: 53.00 +/- 41.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 53       |\n",
      "|    mean_reward     | -90.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 278000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 136      |\n",
      "|    time_elapsed    | 785      |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=-96.79 +/- 4.54\n",
      "Episode length: 11.60 +/- 12.61\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.6       |\n",
      "|    mean_reward          | -96.8      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 279000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01126349 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.35      |\n",
      "|    explained_variance   | 0.0886     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 34.3       |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    value_loss           | 92         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=-96.24 +/- 3.11\n",
      "Episode length: 10.20 +/- 7.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | -95      |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 790      |\n",
      "|    total_timesteps | 280576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=-92.37 +/- 4.83\n",
      "Episode length: 18.80 +/- 13.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.8        |\n",
      "|    mean_reward          | -92.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 281000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010243949 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.0728      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.4        |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 83.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=-92.83 +/- 3.71\n",
      "Episode length: 28.60 +/- 17.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.6     |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 282000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 138      |\n",
      "|    time_elapsed    | 796      |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=283000, episode_reward=-91.53 +/- 8.01\n",
      "Episode length: 22.60 +/- 22.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -91.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 283000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014346642 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.9        |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 69.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=-91.22 +/- 3.35\n",
      "Episode length: 23.20 +/- 9.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.2     |\n",
      "|    mean_reward     | -91.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 284000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 355      |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 801      |\n",
      "|    total_timesteps | 284672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=-92.27 +/- 3.21\n",
      "Episode length: 22.00 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22          |\n",
      "|    mean_reward          | -92.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 285000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011068469 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30          |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 76.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=-95.37 +/- 5.00\n",
      "Episode length: 13.00 +/- 15.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -95.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 286000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.4     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 354      |\n",
      "|    iterations      | 140      |\n",
      "|    time_elapsed    | 808      |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=-90.27 +/- 6.11\n",
      "Episode length: 29.20 +/- 18.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.2        |\n",
      "|    mean_reward          | -90.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 287000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012372186 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34          |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 82.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=-96.35 +/- 4.59\n",
      "Episode length: 12.80 +/- 12.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.8     |\n",
      "|    mean_reward     | -96.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -95.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 816      |\n",
      "|    total_timesteps | 288768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=-95.11 +/- 5.84\n",
      "Episode length: 20.60 +/- 12.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -95.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 289000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012350451 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.0983      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 82.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=-95.52 +/- 4.13\n",
      "Episode length: 11.00 +/- 8.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11       |\n",
      "|    mean_reward     | -95.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 142      |\n",
      "|    time_elapsed    | 825      |\n",
      "|    total_timesteps | 290816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=-96.12 +/- 4.40\n",
      "Episode length: 17.60 +/- 14.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.6        |\n",
      "|    mean_reward          | -96.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 291000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011606697 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.00958     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.1        |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 86.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=-93.61 +/- 3.75\n",
      "Episode length: 16.80 +/- 9.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.8     |\n",
      "|    mean_reward     | -93.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 292000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.1     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 830      |\n",
      "|    total_timesteps | 292864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=-92.33 +/- 6.16\n",
      "Episode length: 20.00 +/- 17.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20          |\n",
      "|    mean_reward          | -92.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 293000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013907386 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.5        |\n",
      "|    n_updates            | 1430        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 66.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=294000, episode_reward=-97.03 +/- 2.56\n",
      "Episode length: 9.60 +/- 7.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | -97      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 294000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.2     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 144      |\n",
      "|    time_elapsed    | 836      |\n",
      "|    total_timesteps | 294912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=-97.58 +/- 3.02\n",
      "Episode length: 17.40 +/- 12.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.4        |\n",
      "|    mean_reward          | -97.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 295000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010821075 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | -0.0231     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.8        |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=-97.28 +/- 2.94\n",
      "Episode length: 9.20 +/- 7.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | -97.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 296000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 841      |\n",
      "|    total_timesteps | 296960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=-94.68 +/- 2.79\n",
      "Episode length: 19.00 +/- 12.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19          |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 297000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016055427 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 84.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=-89.48 +/- 5.44\n",
      "Episode length: 32.20 +/- 18.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 32.2     |\n",
      "|    mean_reward     | -89.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 298000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=-89.50 +/- 10.74\n",
      "Episode length: 30.00 +/- 32.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30       |\n",
      "|    mean_reward     | -89.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 299000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 146      |\n",
      "|    time_elapsed    | 847      |\n",
      "|    total_timesteps | 299008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=-92.57 +/- 4.45\n",
      "Episode length: 26.40 +/- 18.35\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 26.4         |\n",
      "|    mean_reward          | -92.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 300000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118401125 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 29.2         |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.0168      |\n",
      "|    value_loss           | 78.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=301000, episode_reward=-95.31 +/- 5.16\n",
      "Episode length: 16.40 +/- 17.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.4     |\n",
      "|    mean_reward     | -95.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 301000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.3     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 852      |\n",
      "|    total_timesteps | 301056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=302000, episode_reward=-94.85 +/- 5.83\n",
      "Episode length: 13.00 +/- 12.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13         |\n",
      "|    mean_reward          | -94.8      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 302000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00975686 |\n",
      "|    clip_fraction        | 0.0966     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.3       |\n",
      "|    explained_variance   | 0.0954     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 44.6       |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    value_loss           | 104        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=303000, episode_reward=-94.36 +/- 7.21\n",
      "Episode length: 22.20 +/- 20.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.2     |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 303000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.4     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 148      |\n",
      "|    time_elapsed    | 858      |\n",
      "|    total_timesteps | 303104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=304000, episode_reward=-99.66 +/- 1.38\n",
      "Episode length: 3.40 +/- 1.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 3.4        |\n",
      "|    mean_reward          | -99.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 304000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00887148 |\n",
      "|    clip_fraction        | 0.104      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.117      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 50.1       |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 104        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=-94.81 +/- 5.86\n",
      "Episode length: 15.40 +/- 12.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.4     |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 305000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.3     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 865      |\n",
      "|    total_timesteps | 305152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=306000, episode_reward=-94.37 +/- 4.52\n",
      "Episode length: 18.60 +/- 10.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.6        |\n",
      "|    mean_reward          | -94.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 306000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008347581 |\n",
      "|    clip_fraction        | 0.0922      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.5        |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=307000, episode_reward=-96.02 +/- 3.72\n",
      "Episode length: 9.20 +/- 8.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | -96      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 307000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 150      |\n",
      "|    time_elapsed    | 874      |\n",
      "|    total_timesteps | 307200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=308000, episode_reward=-92.01 +/- 3.24\n",
      "Episode length: 24.80 +/- 11.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.8        |\n",
      "|    mean_reward          | -92         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 308000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011533769 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.6        |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 75          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=309000, episode_reward=-92.80 +/- 7.08\n",
      "Episode length: 18.20 +/- 17.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 309000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.2     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 880      |\n",
      "|    total_timesteps | 309248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=-95.99 +/- 3.55\n",
      "Episode length: 12.60 +/- 10.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 12.6         |\n",
      "|    mean_reward          | -96          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 310000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125144925 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.00257      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.4         |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.0178      |\n",
      "|    value_loss           | 91.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=311000, episode_reward=-94.53 +/- 3.98\n",
      "Episode length: 14.20 +/- 8.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 311000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.9     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 152      |\n",
      "|    time_elapsed    | 885      |\n",
      "|    total_timesteps | 311296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=312000, episode_reward=-94.46 +/- 3.15\n",
      "Episode length: 16.20 +/- 11.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | -94.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 312000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011078153 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.9        |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 85.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=313000, episode_reward=-92.60 +/- 7.25\n",
      "Episode length: 18.80 +/- 17.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 313000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.4     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 891      |\n",
      "|    total_timesteps | 313344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=314000, episode_reward=-95.27 +/- 2.70\n",
      "Episode length: 15.20 +/- 11.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.2       |\n",
      "|    mean_reward          | -95.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 314000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01145934 |\n",
      "|    clip_fraction        | 0.104      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | 0.0258     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 39.2       |\n",
      "|    n_updates            | 1530       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    value_loss           | 79         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=-93.48 +/- 3.14\n",
      "Episode length: 20.20 +/- 7.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -93.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 315000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 154      |\n",
      "|    time_elapsed    | 896      |\n",
      "|    total_timesteps | 315392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=316000, episode_reward=-86.13 +/- 11.80\n",
      "Episode length: 38.40 +/- 31.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 38.4        |\n",
      "|    mean_reward          | -86.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 316000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014380075 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.3        |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 88.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=317000, episode_reward=-97.91 +/- 1.72\n",
      "Episode length: 8.80 +/- 4.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -97.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 317000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.8     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 901      |\n",
      "|    total_timesteps | 317440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=318000, episode_reward=-91.14 +/- 7.26\n",
      "Episode length: 27.60 +/- 25.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.6        |\n",
      "|    mean_reward          | -91.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 318000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010487717 |\n",
      "|    clip_fraction        | 0.0987      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.0457      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.1        |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 93          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=319000, episode_reward=-94.99 +/- 8.55\n",
      "Episode length: 20.00 +/- 21.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 319000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.3     |\n",
      "|    ep_rew_mean     | -94.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 156      |\n",
      "|    time_elapsed    | 907      |\n",
      "|    total_timesteps | 319488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=-95.96 +/- 2.41\n",
      "Episode length: 10.80 +/- 7.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.8        |\n",
      "|    mean_reward          | -96         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 320000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013228111 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.0524      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.3        |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 87.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=321000, episode_reward=-96.19 +/- 4.19\n",
      "Episode length: 10.60 +/- 11.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 321000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 913      |\n",
      "|    total_timesteps | 321536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=322000, episode_reward=-92.53 +/- 3.49\n",
      "Episode length: 22.80 +/- 8.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -92.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 322000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011606265 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 1570        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 77.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=323000, episode_reward=-87.48 +/- 9.26\n",
      "Episode length: 33.00 +/- 24.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | -87.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 323000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 158      |\n",
      "|    time_elapsed    | 920      |\n",
      "|    total_timesteps | 323584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=324000, episode_reward=-94.76 +/- 2.65\n",
      "Episode length: 14.80 +/- 8.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.8        |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 324000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011424342 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 68.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=-94.53 +/- 5.30\n",
      "Episode length: 13.80 +/- 12.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 325000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 926      |\n",
      "|    total_timesteps | 325632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=326000, episode_reward=-97.46 +/- 1.53\n",
      "Episode length: 6.80 +/- 4.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.8         |\n",
      "|    mean_reward          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 326000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011649584 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 88.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=327000, episode_reward=-91.76 +/- 3.85\n",
      "Episode length: 21.80 +/- 10.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.8     |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 327000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 160      |\n",
      "|    time_elapsed    | 931      |\n",
      "|    total_timesteps | 327680   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=328000, episode_reward=-94.74 +/- 6.56\n",
      "Episode length: 17.80 +/- 15.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.8        |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 328000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009741891 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | -0.024      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 75.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=329000, episode_reward=-86.69 +/- 4.88\n",
      "Episode length: 38.80 +/- 20.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 38.8     |\n",
      "|    mean_reward     | -86.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 329000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.7     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 936      |\n",
      "|    total_timesteps | 329728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=-96.16 +/- 3.11\n",
      "Episode length: 9.60 +/- 6.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | -96.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 330000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011751684 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.3        |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 78.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=331000, episode_reward=-91.84 +/- 4.10\n",
      "Episode length: 32.60 +/- 16.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 32.6     |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 331000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 162      |\n",
      "|    time_elapsed    | 942      |\n",
      "|    total_timesteps | 331776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=332000, episode_reward=-91.85 +/- 7.81\n",
      "Episode length: 23.00 +/- 22.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23          |\n",
      "|    mean_reward          | -91.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 332000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011838851 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.2        |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 94.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=333000, episode_reward=-92.22 +/- 6.21\n",
      "Episode length: 20.60 +/- 15.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 333000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21       |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 947      |\n",
      "|    total_timesteps | 333824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=334000, episode_reward=-93.56 +/- 5.32\n",
      "Episode length: 24.20 +/- 6.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | -93.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 334000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013079082 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.3        |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 78          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=-94.65 +/- 2.83\n",
      "Episode length: 15.60 +/- 10.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | -94.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 335000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 164      |\n",
      "|    time_elapsed    | 952      |\n",
      "|    total_timesteps | 335872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=336000, episode_reward=-89.97 +/- 11.84\n",
      "Episode length: 44.40 +/- 38.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 44.4         |\n",
      "|    mean_reward          | -90          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 336000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091982195 |\n",
      "|    clip_fraction        | 0.0924       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0.03         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.4         |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=337000, episode_reward=-94.05 +/- 4.64\n",
      "Episode length: 16.00 +/- 12.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | -94      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 337000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 352      |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 959      |\n",
      "|    total_timesteps | 337920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=338000, episode_reward=-97.55 +/- 2.20\n",
      "Episode length: 6.80 +/- 5.88\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 6.8        |\n",
      "|    mean_reward          | -97.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 338000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01251039 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.34      |\n",
      "|    explained_variance   | 0.131      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 27.9       |\n",
      "|    n_updates            | 1650       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 84.5       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=339000, episode_reward=-95.72 +/- 3.55\n",
      "Episode length: 11.00 +/- 7.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11       |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 339000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 166      |\n",
      "|    time_elapsed    | 967      |\n",
      "|    total_timesteps | 339968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=-97.46 +/- 1.48\n",
      "Episode length: 13.00 +/- 11.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13          |\n",
      "|    mean_reward          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 340000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012121886 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.0464      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.7        |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 88.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=341000, episode_reward=-94.36 +/- 3.88\n",
      "Episode length: 23.60 +/- 16.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.6     |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 341000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=342000, episode_reward=-94.52 +/- 5.37\n",
      "Episode length: 20.80 +/- 21.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 342000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 975      |\n",
      "|    total_timesteps | 342016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=343000, episode_reward=-90.74 +/- 8.09\n",
      "Episode length: 31.40 +/- 28.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 31.4        |\n",
      "|    mean_reward          | -90.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 343000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009810762 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.00788     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 1670        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 79.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=344000, episode_reward=-93.17 +/- 3.71\n",
      "Episode length: 18.80 +/- 10.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | -93.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 344000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 980      |\n",
      "|    total_timesteps | 344064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=-92.04 +/- 7.75\n",
      "Episode length: 24.20 +/- 25.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | -92         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 345000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010439461 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.0332      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.2        |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 84.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=346000, episode_reward=-91.39 +/- 7.86\n",
      "Episode length: 24.60 +/- 20.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.6     |\n",
      "|    mean_reward     | -91.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 346000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 169      |\n",
      "|    time_elapsed    | 985      |\n",
      "|    total_timesteps | 346112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=347000, episode_reward=-97.58 +/- 2.35\n",
      "Episode length: 11.00 +/- 7.77\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11         |\n",
      "|    mean_reward          | -97.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 347000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01135608 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.18      |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23.7       |\n",
      "|    n_updates            | 1690       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 67.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=348000, episode_reward=-99.04 +/- 0.74\n",
      "Episode length: 3.40 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 3.4      |\n",
      "|    mean_reward     | -99      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 348000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | -94.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 990      |\n",
      "|    total_timesteps | 348160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=349000, episode_reward=-85.91 +/- 8.54\n",
      "Episode length: 45.80 +/- 36.86\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 45.8       |\n",
      "|    mean_reward          | -85.9      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 349000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00977557 |\n",
      "|    clip_fraction        | 0.0947     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.25      |\n",
      "|    explained_variance   | 0.153      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 27.2       |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    value_loss           | 84.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=-98.32 +/- 1.29\n",
      "Episode length: 8.40 +/- 8.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | -98.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 171      |\n",
      "|    time_elapsed    | 995      |\n",
      "|    total_timesteps | 350208   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=351000, episode_reward=-90.41 +/- 6.68\n",
      "Episode length: 28.80 +/- 22.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 28.8         |\n",
      "|    mean_reward          | -90.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 351000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114489775 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.25        |\n",
      "|    explained_variance   | 0.13         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.6         |\n",
      "|    n_updates            | 1710         |\n",
      "|    policy_gradient_loss | -0.0182      |\n",
      "|    value_loss           | 83.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=352000, episode_reward=-92.89 +/- 3.20\n",
      "Episode length: 20.00 +/- 4.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | -92.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 352000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 1001     |\n",
      "|    total_timesteps | 352256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=353000, episode_reward=-93.81 +/- 7.32\n",
      "Episode length: 17.20 +/- 19.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.2        |\n",
      "|    mean_reward          | -93.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 353000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011466298 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.6        |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 88.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=354000, episode_reward=-90.37 +/- 5.33\n",
      "Episode length: 27.40 +/- 18.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27.4     |\n",
      "|    mean_reward     | -90.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 354000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 173      |\n",
      "|    time_elapsed    | 1007     |\n",
      "|    total_timesteps | 354304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=-93.39 +/- 5.10\n",
      "Episode length: 16.80 +/- 14.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.8        |\n",
      "|    mean_reward          | -93.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 355000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009264538 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.2        |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=356000, episode_reward=-97.49 +/- 1.94\n",
      "Episode length: 7.40 +/- 4.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | -97.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 356000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 1012     |\n",
      "|    total_timesteps | 356352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=357000, episode_reward=-90.54 +/- 4.82\n",
      "Episode length: 27.80 +/- 9.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 27.8         |\n",
      "|    mean_reward          | -90.5        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 357000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0116985105 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.171        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.3         |\n",
      "|    n_updates            | 1740         |\n",
      "|    policy_gradient_loss | -0.0174      |\n",
      "|    value_loss           | 73.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=358000, episode_reward=-97.71 +/- 2.56\n",
      "Episode length: 12.00 +/- 4.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | -97.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 358000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.3     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 351      |\n",
      "|    iterations      | 175      |\n",
      "|    time_elapsed    | 1020     |\n",
      "|    total_timesteps | 358400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=359000, episode_reward=-89.99 +/- 11.47\n",
      "Episode length: 32.60 +/- 35.89\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 32.6       |\n",
      "|    mean_reward          | -90        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 359000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01083415 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.0739     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 61.1       |\n",
      "|    n_updates            | 1750       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    value_loss           | 87.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=-95.25 +/- 3.38\n",
      "Episode length: 12.80 +/- 6.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.8     |\n",
      "|    mean_reward     | -95.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.4     |\n",
      "|    ep_rew_mean     | -94.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 1028     |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=361000, episode_reward=-93.73 +/- 8.43\n",
      "Episode length: 18.60 +/- 22.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18.6       |\n",
      "|    mean_reward          | -93.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 361000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01340818 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.32      |\n",
      "|    explained_variance   | -0.0989    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 40.4       |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    value_loss           | 87.1       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=362000, episode_reward=-90.38 +/- 7.00\n",
      "Episode length: 37.00 +/- 26.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 37       |\n",
      "|    mean_reward     | -90.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 362000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.5     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 177      |\n",
      "|    time_elapsed    | 1035     |\n",
      "|    total_timesteps | 362496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=363000, episode_reward=-92.68 +/- 8.45\n",
      "Episode length: 23.40 +/- 22.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.4        |\n",
      "|    mean_reward          | -92.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 363000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012634305 |\n",
      "|    clip_fraction        | 0.0968      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41          |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 75.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=364000, episode_reward=-96.50 +/- 3.68\n",
      "Episode length: 12.40 +/- 8.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | -96.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 364000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.3     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 1040     |\n",
      "|    total_timesteps | 364544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=-93.00 +/- 7.52\n",
      "Episode length: 19.80 +/- 22.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19.8        |\n",
      "|    mean_reward          | -93         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 365000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011162661 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47          |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 85.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=366000, episode_reward=-90.90 +/- 7.69\n",
      "Episode length: 34.20 +/- 36.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 34.2     |\n",
      "|    mean_reward     | -90.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 366000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 179      |\n",
      "|    time_elapsed    | 1045     |\n",
      "|    total_timesteps | 366592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=367000, episode_reward=-90.43 +/- 5.86\n",
      "Episode length: 26.40 +/- 15.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 26.4       |\n",
      "|    mean_reward          | -90.4      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 367000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00960959 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.13      |\n",
      "|    explained_variance   | 0.202      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 25.7       |\n",
      "|    n_updates            | 1790       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    value_loss           | 71.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=368000, episode_reward=-93.91 +/- 4.91\n",
      "Episode length: 19.00 +/- 16.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19       |\n",
      "|    mean_reward     | -93.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 368000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.2     |\n",
      "|    ep_rew_mean     | -94.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 1051     |\n",
      "|    total_timesteps | 368640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=369000, episode_reward=-93.42 +/- 5.23\n",
      "Episode length: 20.60 +/- 19.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.6        |\n",
      "|    mean_reward          | -93.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 369000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009775125 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.0771      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.4        |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 85.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=-91.93 +/- 7.98\n",
      "Episode length: 22.20 +/- 24.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.2     |\n",
      "|    mean_reward     | -91.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.2     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 181      |\n",
      "|    time_elapsed    | 1057     |\n",
      "|    total_timesteps | 370688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=371000, episode_reward=-86.59 +/- 14.90\n",
      "Episode length: 41.60 +/- 54.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 41.6       |\n",
      "|    mean_reward          | -86.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 371000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01501679 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.2       |\n",
      "|    explained_variance   | 0.0167     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 28.8       |\n",
      "|    n_updates            | 1810       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 75.4       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=372000, episode_reward=-95.74 +/- 3.03\n",
      "Episode length: 10.60 +/- 7.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 372000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.3     |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 350      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 1064     |\n",
      "|    total_timesteps | 372736   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=373000, episode_reward=-95.02 +/- 2.98\n",
      "Episode length: 13.00 +/- 8.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13          |\n",
      "|    mean_reward          | -95         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 373000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012943923 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 71.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=374000, episode_reward=-88.81 +/- 4.52\n",
      "Episode length: 30.60 +/- 12.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.6     |\n",
      "|    mean_reward     | -88.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 374000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.1     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 183      |\n",
      "|    time_elapsed    | 1072     |\n",
      "|    total_timesteps | 374784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=-94.02 +/- 5.46\n",
      "Episode length: 17.00 +/- 15.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17          |\n",
      "|    mean_reward          | -94         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 375000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011573162 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | -0.0353     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 1830        |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 79.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=376000, episode_reward=-88.10 +/- 5.98\n",
      "Episode length: 33.00 +/- 19.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | -88.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 376000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.7     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 1080     |\n",
      "|    total_timesteps | 376832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=377000, episode_reward=-92.01 +/- 5.09\n",
      "Episode length: 20.40 +/- 12.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -92         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 377000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010964416 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.0862      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 86          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=378000, episode_reward=-95.18 +/- 3.81\n",
      "Episode length: 15.00 +/- 12.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | -95.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 378000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.1     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 185      |\n",
      "|    time_elapsed    | 1086     |\n",
      "|    total_timesteps | 378880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=379000, episode_reward=-96.74 +/- 2.42\n",
      "Episode length: 10.40 +/- 5.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.4        |\n",
      "|    mean_reward          | -96.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 379000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012004554 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.0706      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.2        |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 88.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=-92.16 +/- 5.24\n",
      "Episode length: 22.20 +/- 15.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.2     |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 1091     |\n",
      "|    total_timesteps | 380928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=381000, episode_reward=-92.80 +/- 6.41\n",
      "Episode length: 20.40 +/- 15.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -92.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 381000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010163153 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.6        |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 80.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=382000, episode_reward=-93.93 +/- 6.32\n",
      "Episode length: 23.80 +/- 31.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.8     |\n",
      "|    mean_reward     | -93.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 382000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 187      |\n",
      "|    time_elapsed    | 1096     |\n",
      "|    total_timesteps | 382976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=383000, episode_reward=-93.33 +/- 5.62\n",
      "Episode length: 21.40 +/- 19.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.4        |\n",
      "|    mean_reward          | -93.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 383000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013915698 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.0435      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.5        |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 84.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=384000, episode_reward=-97.06 +/- 2.54\n",
      "Episode length: 10.80 +/- 10.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.8     |\n",
      "|    mean_reward     | -97.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 384000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=-92.31 +/- 10.84\n",
      "Episode length: 28.40 +/- 40.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.4     |\n",
      "|    mean_reward     | -92.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 385000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.2     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 1102     |\n",
      "|    total_timesteps | 385024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=386000, episode_reward=-93.51 +/- 4.40\n",
      "Episode length: 16.40 +/- 10.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | -93.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 386000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012265559 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.0349      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=387000, episode_reward=-94.68 +/- 5.17\n",
      "Episode length: 13.80 +/- 9.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | -94.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 387000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.5     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 189      |\n",
      "|    time_elapsed    | 1109     |\n",
      "|    total_timesteps | 387072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=388000, episode_reward=-88.24 +/- 14.59\n",
      "Episode length: 30.20 +/- 38.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | -88.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 388000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011878727 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 72.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=389000, episode_reward=-92.63 +/- 3.82\n",
      "Episode length: 20.40 +/- 9.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 389000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.8     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 1117     |\n",
      "|    total_timesteps | 389120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=-85.64 +/- 6.92\n",
      "Episode length: 37.80 +/- 19.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 37.8        |\n",
      "|    mean_reward          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 390000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008970594 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 96.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=391000, episode_reward=-94.74 +/- 3.89\n",
      "Episode length: 16.60 +/- 9.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | -94.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 391000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.3     |\n",
      "|    ep_rew_mean     | -91.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 191      |\n",
      "|    time_elapsed    | 1126     |\n",
      "|    total_timesteps | 391168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=392000, episode_reward=-96.04 +/- 3.81\n",
      "Episode length: 12.20 +/- 8.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.2        |\n",
      "|    mean_reward          | -96         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 392000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008370647 |\n",
      "|    clip_fraction        | 0.093       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.8        |\n",
      "|    n_updates            | 1910        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 96.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=393000, episode_reward=-90.99 +/- 9.82\n",
      "Episode length: 28.00 +/- 31.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28       |\n",
      "|    mean_reward     | -91      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 393000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 1131     |\n",
      "|    total_timesteps | 393216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=394000, episode_reward=-93.32 +/- 3.38\n",
      "Episode length: 15.00 +/- 6.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | -93.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 394000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012021883 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.0327     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.3        |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 86.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=-96.93 +/- 2.61\n",
      "Episode length: 11.40 +/- 6.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 395000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.4     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 193      |\n",
      "|    time_elapsed    | 1136     |\n",
      "|    total_timesteps | 395264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=396000, episode_reward=-94.01 +/- 5.65\n",
      "Episode length: 23.20 +/- 26.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.2        |\n",
      "|    mean_reward          | -94         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 396000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010999848 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.0761      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 84.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=397000, episode_reward=-92.20 +/- 2.21\n",
      "Episode length: 19.60 +/- 5.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.6     |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 397000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.8     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 1141     |\n",
      "|    total_timesteps | 397312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=398000, episode_reward=-96.95 +/- 4.94\n",
      "Episode length: 15.20 +/- 10.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.2        |\n",
      "|    mean_reward          | -97         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 398000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011559309 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 86.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=399000, episode_reward=-92.21 +/- 9.90\n",
      "Episode length: 24.40 +/- 29.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.4     |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 399000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.1     |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 195      |\n",
      "|    time_elapsed    | 1146     |\n",
      "|    total_timesteps | 399360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=-89.85 +/- 8.92\n",
      "Episode length: 36.20 +/- 33.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 36.2        |\n",
      "|    mean_reward          | -89.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 400000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014573149 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.0451      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 71.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=401000, episode_reward=-88.09 +/- 10.79\n",
      "Episode length: 36.60 +/- 33.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 36.6     |\n",
      "|    mean_reward     | -88.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 401000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 1151     |\n",
      "|    total_timesteps | 401408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=402000, episode_reward=-91.89 +/- 9.34\n",
      "Episode length: 22.80 +/- 28.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -91.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 402000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009819515 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 88.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=403000, episode_reward=-93.08 +/- 5.03\n",
      "Episode length: 20.80 +/- 14.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -93.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 403000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 197      |\n",
      "|    time_elapsed    | 1156     |\n",
      "|    total_timesteps | 403456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=404000, episode_reward=-96.30 +/- 2.74\n",
      "Episode length: 10.20 +/- 6.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | -96.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 404000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012035077 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | -0.209      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.1        |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 93.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=-90.27 +/- 8.98\n",
      "Episode length: 25.80 +/- 25.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.8     |\n",
      "|    mean_reward     | -90.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 405000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.1     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 1162     |\n",
      "|    total_timesteps | 405504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=406000, episode_reward=-93.30 +/- 3.63\n",
      "Episode length: 14.40 +/- 8.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | -93.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 406000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011609378 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.8        |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 86.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=407000, episode_reward=-92.98 +/- 7.41\n",
      "Episode length: 20.40 +/- 23.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -93      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 407000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.1     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 349      |\n",
      "|    iterations      | 199      |\n",
      "|    time_elapsed    | 1167     |\n",
      "|    total_timesteps | 407552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=408000, episode_reward=-94.24 +/- 3.43\n",
      "Episode length: 16.60 +/- 10.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.6        |\n",
      "|    mean_reward          | -94.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 408000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015583298 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.00442     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.9        |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 94.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=409000, episode_reward=-90.47 +/- 5.10\n",
      "Episode length: 29.20 +/- 23.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 29.2     |\n",
      "|    mean_reward     | -90.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 409000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 1174     |\n",
      "|    total_timesteps | 409600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=-93.73 +/- 7.79\n",
      "Episode length: 20.80 +/- 24.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.8        |\n",
      "|    mean_reward          | -93.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 410000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012141436 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.0807      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 80.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=411000, episode_reward=-95.07 +/- 3.03\n",
      "Episode length: 13.40 +/- 10.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.4     |\n",
      "|    mean_reward     | -95.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 411000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.3     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 348      |\n",
      "|    iterations      | 201      |\n",
      "|    time_elapsed    | 1182     |\n",
      "|    total_timesteps | 411648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=412000, episode_reward=-90.22 +/- 8.84\n",
      "Episode length: 38.80 +/- 47.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 38.8        |\n",
      "|    mean_reward          | -90.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 412000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009973135 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 86.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=413000, episode_reward=-87.29 +/- 10.44\n",
      "Episode length: 37.60 +/- 34.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 37.6     |\n",
      "|    mean_reward     | -87.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 413000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18       |\n",
      "|    ep_rew_mean     | -94.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 1190     |\n",
      "|    total_timesteps | 413696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=414000, episode_reward=-96.14 +/- 4.52\n",
      "Episode length: 12.80 +/- 10.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.8        |\n",
      "|    mean_reward          | -96.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 414000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010656992 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 80          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=-91.59 +/- 7.55\n",
      "Episode length: 27.20 +/- 26.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27.2     |\n",
      "|    mean_reward     | -91.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 415000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.5     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 203      |\n",
      "|    time_elapsed    | 1196     |\n",
      "|    total_timesteps | 415744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=416000, episode_reward=-96.41 +/- 2.19\n",
      "Episode length: 9.00 +/- 5.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 416000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010234664 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31          |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 87.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=417000, episode_reward=-92.11 +/- 7.62\n",
      "Episode length: 26.00 +/- 23.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26       |\n",
      "|    mean_reward     | -92.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 417000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 1201     |\n",
      "|    total_timesteps | 417792   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=418000, episode_reward=-87.85 +/- 9.89\n",
      "Episode length: 42.00 +/- 35.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 42          |\n",
      "|    mean_reward          | -87.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 418000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012227735 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0309      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 91.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=419000, episode_reward=-85.32 +/- 4.98\n",
      "Episode length: 38.40 +/- 11.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 38.4     |\n",
      "|    mean_reward     | -85.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 419000   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.7     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 205      |\n",
      "|    time_elapsed    | 1207     |\n",
      "|    total_timesteps | 419840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=-82.70 +/- 13.65\n",
      "Episode length: 55.40 +/- 48.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 55.4        |\n",
      "|    mean_reward          | -82.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 420000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012278816 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.3        |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 85          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=421000, episode_reward=-93.60 +/- 2.73\n",
      "Episode length: 16.20 +/- 8.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | -93.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 421000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.4     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 1214     |\n",
      "|    total_timesteps | 421888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=422000, episode_reward=-96.15 +/- 1.25\n",
      "Episode length: 10.20 +/- 2.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.2        |\n",
      "|    mean_reward          | -96.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 422000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009513142 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | -0.209      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.7        |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 82.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=423000, episode_reward=-96.35 +/- 3.03\n",
      "Episode length: 8.80 +/- 5.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 423000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 347      |\n",
      "|    iterations      | 207      |\n",
      "|    time_elapsed    | 1221     |\n",
      "|    total_timesteps | 423936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=424000, episode_reward=-94.59 +/- 2.55\n",
      "Episode length: 12.80 +/- 5.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.8        |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 424000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011324627 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0535      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.9        |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 83.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=-95.92 +/- 3.85\n",
      "Episode length: 14.60 +/- 16.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.6     |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 425000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 1230     |\n",
      "|    total_timesteps | 425984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=426000, episode_reward=-94.56 +/- 4.10\n",
      "Episode length: 13.20 +/- 9.24\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 13.2       |\n",
      "|    mean_reward          | -94.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 426000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01148336 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.00699    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 39.2       |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    value_loss           | 92.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=427000, episode_reward=-99.10 +/- 3.67\n",
      "Episode length: 10.00 +/- 7.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -99.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 427000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=428000, episode_reward=-87.33 +/- 12.88\n",
      "Episode length: 39.40 +/- 40.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 39.4     |\n",
      "|    mean_reward     | -87.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 428000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 1236     |\n",
      "|    total_timesteps | 428032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=429000, episode_reward=-92.11 +/- 4.07\n",
      "Episode length: 22.80 +/- 11.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -92.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 429000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010544754 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 81.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=430000, episode_reward=-91.04 +/- 4.00\n",
      "Episode length: 31.40 +/- 14.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 31.4     |\n",
      "|    mean_reward     | -91      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.5     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 210      |\n",
      "|    time_elapsed    | 1242     |\n",
      "|    total_timesteps | 430080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=431000, episode_reward=-96.49 +/- 3.55\n",
      "Episode length: 11.80 +/- 8.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.8        |\n",
      "|    mean_reward          | -96.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 431000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009475373 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 82.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=432000, episode_reward=-90.54 +/- 3.94\n",
      "Episode length: 22.60 +/- 9.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -90.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 432000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.7     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 1247     |\n",
      "|    total_timesteps | 432128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=433000, episode_reward=-95.03 +/- 2.76\n",
      "Episode length: 11.40 +/- 5.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.4        |\n",
      "|    mean_reward          | -95         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 433000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015024643 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.0132     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.2        |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=434000, episode_reward=-97.09 +/- 3.49\n",
      "Episode length: 9.40 +/- 9.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | -97.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 434000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 212      |\n",
      "|    time_elapsed    | 1253     |\n",
      "|    total_timesteps | 434176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=-91.70 +/- 7.60\n",
      "Episode length: 21.60 +/- 16.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.6        |\n",
      "|    mean_reward          | -91.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 435000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009794796 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.000502    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 85.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=436000, episode_reward=-93.09 +/- 3.73\n",
      "Episode length: 15.60 +/- 8.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | -93.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 436000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 1261     |\n",
      "|    total_timesteps | 436224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=437000, episode_reward=-96.85 +/- 2.85\n",
      "Episode length: 9.00 +/- 6.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -96.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 437000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010029951 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.5        |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 75.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=438000, episode_reward=-91.33 +/- 5.88\n",
      "Episode length: 34.60 +/- 26.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 34.6     |\n",
      "|    mean_reward     | -91.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 438000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 214      |\n",
      "|    time_elapsed    | 1269     |\n",
      "|    total_timesteps | 438272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=439000, episode_reward=-94.80 +/- 3.82\n",
      "Episode length: 13.00 +/- 8.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13          |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 439000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012968155 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48          |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 79          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=-83.37 +/- 6.44\n",
      "Episode length: 49.60 +/- 30.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 49.6     |\n",
      "|    mean_reward     | -83.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.8     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 1274     |\n",
      "|    total_timesteps | 440320   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=441000, episode_reward=-93.03 +/- 4.32\n",
      "Episode length: 29.00 +/- 21.88\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 29         |\n",
      "|    mean_reward          | -93        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 441000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00993385 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.0727     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 32.9       |\n",
      "|    n_updates            | 2150       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    value_loss           | 84.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=442000, episode_reward=-90.79 +/- 3.22\n",
      "Episode length: 27.60 +/- 13.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27.6     |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 442000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 216      |\n",
      "|    time_elapsed    | 1279     |\n",
      "|    total_timesteps | 442368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=443000, episode_reward=-93.74 +/- 6.97\n",
      "Episode length: 15.80 +/- 16.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | -93.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 443000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009264584 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.5        |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 99.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=444000, episode_reward=-97.17 +/- 2.02\n",
      "Episode length: 7.00 +/- 3.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 444000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.9     |\n",
      "|    ep_rew_mean     | -92.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 1284     |\n",
      "|    total_timesteps | 444416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=-89.88 +/- 8.15\n",
      "Episode length: 25.60 +/- 18.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.6        |\n",
      "|    mean_reward          | -89.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 445000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013041239 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 67.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=446000, episode_reward=-96.20 +/- 2.77\n",
      "Episode length: 9.80 +/- 6.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 446000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.9     |\n",
      "|    ep_rew_mean     | -92.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 346      |\n",
      "|    iterations      | 218      |\n",
      "|    time_elapsed    | 1290     |\n",
      "|    total_timesteps | 446464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=447000, episode_reward=-81.27 +/- 16.19\n",
      "Episode length: 59.80 +/- 46.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 59.8        |\n",
      "|    mean_reward          | -81.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 447000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009321404 |\n",
      "|    clip_fraction        | 0.0827      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.5        |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 85.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=448000, episode_reward=-92.37 +/- 6.02\n",
      "Episode length: 24.40 +/- 15.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.4     |\n",
      "|    mean_reward     | -92.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 448000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 1296     |\n",
      "|    total_timesteps | 448512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=449000, episode_reward=-94.64 +/- 3.74\n",
      "Episode length: 15.40 +/- 7.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.4        |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 449000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009235121 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.2        |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 94.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=-92.63 +/- 7.38\n",
      "Episode length: 23.20 +/- 23.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.2     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 1304     |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=451000, episode_reward=-93.99 +/- 3.24\n",
      "Episode length: 15.20 +/- 7.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.2        |\n",
      "|    mean_reward          | -94         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 451000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010624738 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 94.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=452000, episode_reward=-88.77 +/- 8.78\n",
      "Episode length: 49.20 +/- 49.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 49.2     |\n",
      "|    mean_reward     | -88.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 452000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.3     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 1312     |\n",
      "|    total_timesteps | 452608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=453000, episode_reward=-88.41 +/- 10.99\n",
      "Episode length: 34.00 +/- 34.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 34          |\n",
      "|    mean_reward          | -88.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 453000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009343761 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.7        |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 80          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=454000, episode_reward=-95.97 +/- 2.72\n",
      "Episode length: 9.80 +/- 5.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | -96      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 454000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.8     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 222      |\n",
      "|    time_elapsed    | 1318     |\n",
      "|    total_timesteps | 454656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=-92.84 +/- 11.13\n",
      "Episode length: 21.60 +/- 28.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.6        |\n",
      "|    mean_reward          | -92.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 455000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010731913 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | 0.0332      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.3        |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 76          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=456000, episode_reward=-93.86 +/- 2.26\n",
      "Episode length: 17.40 +/- 12.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.4     |\n",
      "|    mean_reward     | -93.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 456000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.5     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 1323     |\n",
      "|    total_timesteps | 456704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=457000, episode_reward=-91.12 +/- 8.33\n",
      "Episode length: 24.60 +/- 21.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.6        |\n",
      "|    mean_reward          | -91.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 457000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010463136 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.6        |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 88.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=458000, episode_reward=-95.88 +/- 3.93\n",
      "Episode length: 11.00 +/- 9.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11       |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 458000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 224      |\n",
      "|    time_elapsed    | 1329     |\n",
      "|    total_timesteps | 458752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=459000, episode_reward=-92.65 +/- 3.76\n",
      "Episode length: 22.00 +/- 17.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22          |\n",
      "|    mean_reward          | -92.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 459000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011687843 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34          |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 88.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=-96.75 +/- 2.97\n",
      "Episode length: 11.40 +/- 9.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | -96.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 345      |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 1334     |\n",
      "|    total_timesteps | 460800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=461000, episode_reward=-92.90 +/- 5.31\n",
      "Episode length: 28.20 +/- 16.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 28.2        |\n",
      "|    mean_reward          | -92.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 461000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010898267 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 90.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=462000, episode_reward=-91.16 +/- 9.11\n",
      "Episode length: 32.00 +/- 33.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 32       |\n",
      "|    mean_reward     | -91.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 462000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.8     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 226      |\n",
      "|    time_elapsed    | 1343     |\n",
      "|    total_timesteps | 462848   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=463000, episode_reward=-94.08 +/- 4.12\n",
      "Episode length: 18.60 +/- 10.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.6        |\n",
      "|    mean_reward          | -94.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 463000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010049658 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.5        |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 90.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=464000, episode_reward=-93.83 +/- 4.56\n",
      "Episode length: 14.80 +/- 10.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.8     |\n",
      "|    mean_reward     | -93.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 464000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24       |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 1350     |\n",
      "|    total_timesteps | 464896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=-95.38 +/- 5.38\n",
      "Episode length: 13.20 +/- 13.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.2        |\n",
      "|    mean_reward          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 465000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009369253 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 2270        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 79.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=466000, episode_reward=-92.58 +/- 5.36\n",
      "Episode length: 26.60 +/- 21.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.6     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 466000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.7     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 228      |\n",
      "|    time_elapsed    | 1355     |\n",
      "|    total_timesteps | 466944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=467000, episode_reward=-92.20 +/- 5.68\n",
      "Episode length: 21.60 +/- 11.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.6        |\n",
      "|    mean_reward          | -92.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 467000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009268506 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.089       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.9        |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 90.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=468000, episode_reward=-90.24 +/- 13.05\n",
      "Episode length: 30.20 +/- 43.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.2     |\n",
      "|    mean_reward     | -90.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 468000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.1     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 1361     |\n",
      "|    total_timesteps | 468992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=469000, episode_reward=-93.63 +/- 4.27\n",
      "Episode length: 15.60 +/- 10.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.6        |\n",
      "|    mean_reward          | -93.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 469000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011695384 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.00589     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.2        |\n",
      "|    n_updates            | 2290        |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 96.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=-94.83 +/- 2.88\n",
      "Episode length: 15.20 +/- 9.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.2     |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=471000, episode_reward=-90.80 +/- 8.42\n",
      "Episode length: 30.60 +/- 33.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.6     |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 471000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.2     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 230      |\n",
      "|    time_elapsed    | 1366     |\n",
      "|    total_timesteps | 471040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=472000, episode_reward=-88.97 +/- 8.51\n",
      "Episode length: 34.20 +/- 32.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 34.2        |\n",
      "|    mean_reward          | -89         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 472000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012302661 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | -0.223      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.7        |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 92.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=473000, episode_reward=-96.06 +/- 4.03\n",
      "Episode length: 14.40 +/- 16.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.4     |\n",
      "|    mean_reward     | -96.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 473000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.4     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 344      |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 1373     |\n",
      "|    total_timesteps | 473088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=474000, episode_reward=-96.87 +/- 3.45\n",
      "Episode length: 13.80 +/- 13.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | -96.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 474000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011523493 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.8        |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 81.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=475000, episode_reward=-92.18 +/- 3.12\n",
      "Episode length: 23.60 +/- 13.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.6     |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 475000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.7     |\n",
      "|    ep_rew_mean     | -91      |\n",
      "| time/              |          |\n",
      "|    fps             | 343      |\n",
      "|    iterations      | 232      |\n",
      "|    time_elapsed    | 1381     |\n",
      "|    total_timesteps | 475136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=476000, episode_reward=-94.66 +/- 3.51\n",
      "Episode length: 12.60 +/- 8.06\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12.6       |\n",
      "|    mean_reward          | -94.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 476000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01229482 |\n",
      "|    clip_fraction        | 0.0998     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | 0.275      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 39.5       |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    value_loss           | 97.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=477000, episode_reward=-92.72 +/- 5.07\n",
      "Episode length: 25.60 +/- 16.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.6     |\n",
      "|    mean_reward     | -92.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 477000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.3     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 343      |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 1389     |\n",
      "|    total_timesteps | 477184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=478000, episode_reward=-91.21 +/- 3.63\n",
      "Episode length: 23.20 +/- 11.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.2        |\n",
      "|    mean_reward          | -91.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 478000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010182785 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.4        |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 99.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=479000, episode_reward=-95.27 +/- 4.21\n",
      "Episode length: 13.20 +/- 8.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -95.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 479000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.1     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 234      |\n",
      "|    time_elapsed    | 1397     |\n",
      "|    total_timesteps | 479232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=-90.83 +/- 6.39\n",
      "Episode length: 25.60 +/- 16.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.6        |\n",
      "|    mean_reward          | -90.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 480000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011580198 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.0908      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.8        |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 87.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=481000, episode_reward=-96.90 +/- 2.68\n",
      "Episode length: 8.20 +/- 6.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 481000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.3     |\n",
      "|    ep_rew_mean     | -94.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 1403     |\n",
      "|    total_timesteps | 481280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=482000, episode_reward=-93.92 +/- 6.28\n",
      "Episode length: 16.20 +/- 14.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | -93.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 482000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016408753 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.044       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.6        |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 81.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=483000, episode_reward=-92.98 +/- 8.91\n",
      "Episode length: 20.00 +/- 21.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | -93      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 483000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 236      |\n",
      "|    time_elapsed    | 1409     |\n",
      "|    total_timesteps | 483328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=484000, episode_reward=-96.38 +/- 1.75\n",
      "Episode length: 10.80 +/- 6.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.8        |\n",
      "|    mean_reward          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 484000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011233379 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.0263      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.4        |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 78.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=-96.88 +/- 3.63\n",
      "Episode length: 12.40 +/- 7.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 485000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 343      |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 1414     |\n",
      "|    total_timesteps | 485376   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=486000, episode_reward=-74.04 +/- 23.50\n",
      "Episode length: 84.00 +/- 85.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 84          |\n",
      "|    mean_reward          | -74         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 486000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013115242 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.8        |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 78.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=487000, episode_reward=-85.35 +/- 11.19\n",
      "Episode length: 38.40 +/- 28.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 38.4     |\n",
      "|    mean_reward     | -85.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 487000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 238      |\n",
      "|    time_elapsed    | 1421     |\n",
      "|    total_timesteps | 487424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=488000, episode_reward=-90.82 +/- 12.91\n",
      "Episode length: 26.80 +/- 39.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.8        |\n",
      "|    mean_reward          | -90.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 488000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010899541 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.066       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.9        |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 76.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=489000, episode_reward=-97.70 +/- 3.72\n",
      "Episode length: 10.20 +/- 7.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | -97.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 489000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.2     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 1429     |\n",
      "|    total_timesteps | 489472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=-88.79 +/- 10.93\n",
      "Episode length: 37.80 +/- 32.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 37.8        |\n",
      "|    mean_reward          | -88.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 490000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010643067 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | -0.00705    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 2390        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 72.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=491000, episode_reward=-92.94 +/- 5.04\n",
      "Episode length: 19.40 +/- 9.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.4     |\n",
      "|    mean_reward     | -92.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 491000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.9     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 240      |\n",
      "|    time_elapsed    | 1438     |\n",
      "|    total_timesteps | 491520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=492000, episode_reward=-95.54 +/- 3.95\n",
      "Episode length: 16.20 +/- 13.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.2        |\n",
      "|    mean_reward          | -95.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 492000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010729382 |\n",
      "|    clip_fraction        | 0.0974      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.1        |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 86.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=493000, episode_reward=-94.71 +/- 4.51\n",
      "Episode length: 13.20 +/- 10.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -94.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 493000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21       |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 1444     |\n",
      "|    total_timesteps | 493568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=494000, episode_reward=-94.53 +/- 4.26\n",
      "Episode length: 14.00 +/- 11.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | -94.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 494000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009995152 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.0175      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.8        |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 91.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=-92.46 +/- 11.11\n",
      "Episode length: 20.60 +/- 30.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -92.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 495000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.9     |\n",
      "|    ep_rew_mean     | -91.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 242      |\n",
      "|    time_elapsed    | 1449     |\n",
      "|    total_timesteps | 495616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=496000, episode_reward=-96.38 +/- 2.96\n",
      "Episode length: 12.80 +/- 9.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.8        |\n",
      "|    mean_reward          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 496000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010738201 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.351       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 85.1        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=497000, episode_reward=-95.00 +/- 2.62\n",
      "Episode length: 15.60 +/- 8.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 497000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 1454     |\n",
      "|    total_timesteps | 497664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=498000, episode_reward=-93.93 +/- 1.94\n",
      "Episode length: 16.00 +/- 2.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | -93.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 498000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011271727 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.0328      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 80.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=499000, episode_reward=-90.70 +/- 12.35\n",
      "Episode length: 27.00 +/- 40.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27       |\n",
      "|    mean_reward     | -90.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 499000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.3     |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 244      |\n",
      "|    time_elapsed    | 1460     |\n",
      "|    total_timesteps | 499712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-97.04 +/- 2.29\n",
      "Episode length: 9.80 +/- 3.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | -97         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013006082 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.1        |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 73.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=501000, episode_reward=-96.36 +/- 2.36\n",
      "Episode length: 8.40 +/- 4.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | -96.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 501000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.5     |\n",
      "|    ep_rew_mean     | -92.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 1465     |\n",
      "|    total_timesteps | 501760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=502000, episode_reward=-94.08 +/- 3.47\n",
      "Episode length: 17.20 +/- 11.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.2        |\n",
      "|    mean_reward          | -94.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 502000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011370704 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 79.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=503000, episode_reward=-88.91 +/- 8.29\n",
      "Episode length: 27.00 +/- 20.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27       |\n",
      "|    mean_reward     | -88.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 503000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.6     |\n",
      "|    ep_rew_mean     | -91.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 246      |\n",
      "|    time_elapsed    | 1471     |\n",
      "|    total_timesteps | 503808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=504000, episode_reward=-89.83 +/- 5.10\n",
      "Episode length: 24.40 +/- 12.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.4        |\n",
      "|    mean_reward          | -89.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 504000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011893872 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0938      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.7        |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 74.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=-90.50 +/- 4.61\n",
      "Episode length: 25.00 +/- 12.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25       |\n",
      "|    mean_reward     | -90.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 505000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.1     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 1479     |\n",
      "|    total_timesteps | 505856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=506000, episode_reward=-86.69 +/- 16.37\n",
      "Episode length: 45.80 +/- 60.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 45.8        |\n",
      "|    mean_reward          | -86.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 506000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010065175 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | -0.0432     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.8        |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 80.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=507000, episode_reward=-91.16 +/- 7.01\n",
      "Episode length: 24.80 +/- 20.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.8     |\n",
      "|    mean_reward     | -91.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 507000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.9     |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 248      |\n",
      "|    time_elapsed    | 1486     |\n",
      "|    total_timesteps | 507904   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=508000, episode_reward=-93.97 +/- 2.59\n",
      "Episode length: 18.20 +/- 9.13\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18.2       |\n",
      "|    mean_reward          | -94        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 508000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01375236 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.19      |\n",
      "|    explained_variance   | 0.172      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 42.1       |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    value_loss           | 86.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=509000, episode_reward=-95.20 +/- 1.71\n",
      "Episode length: 11.60 +/- 2.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | -95.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 509000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16       |\n",
      "|    ep_rew_mean     | -94.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 249      |\n",
      "|    time_elapsed    | 1492     |\n",
      "|    total_timesteps | 509952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-97.70 +/- 1.46\n",
      "Episode length: 6.20 +/- 2.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.2         |\n",
      "|    mean_reward          | -97.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 510000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011072752 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.2        |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 78.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=511000, episode_reward=-98.06 +/- 2.34\n",
      "Episode length: 7.40 +/- 7.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.4      |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 511000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=512000, episode_reward=-98.02 +/- 2.35\n",
      "Episode length: 5.40 +/- 5.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.4      |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 1497     |\n",
      "|    total_timesteps | 512000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=513000, episode_reward=-90.43 +/- 9.74\n",
      "Episode length: 32.60 +/- 33.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 32.6        |\n",
      "|    mean_reward          | -90.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 513000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011356949 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.0737      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.4        |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 61.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=514000, episode_reward=-94.01 +/- 4.67\n",
      "Episode length: 15.80 +/- 11.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | -94      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 514000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.2     |\n",
      "|    ep_rew_mean     | -94.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 251      |\n",
      "|    time_elapsed    | 1502     |\n",
      "|    total_timesteps | 514048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=-87.36 +/- 7.27\n",
      "Episode length: 34.40 +/- 20.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 34.4        |\n",
      "|    mean_reward          | -87.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 515000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012270505 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.00957     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.6        |\n",
      "|    n_updates            | 2510        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 80.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=516000, episode_reward=-96.83 +/- 1.05\n",
      "Episode length: 10.00 +/- 6.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -96.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 516000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.4     |\n",
      "|    ep_rew_mean     | -94.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 1508     |\n",
      "|    total_timesteps | 516096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=517000, episode_reward=-95.91 +/- 8.99\n",
      "Episode length: 43.20 +/- 42.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 43.2        |\n",
      "|    mean_reward          | -95.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 517000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013640085 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.4        |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 79.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=518000, episode_reward=-90.14 +/- 12.69\n",
      "Episode length: 37.20 +/- 51.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 37.2     |\n",
      "|    mean_reward     | -90.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 518000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.5     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 253      |\n",
      "|    time_elapsed    | 1514     |\n",
      "|    total_timesteps | 518144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=519000, episode_reward=-95.20 +/- 5.56\n",
      "Episode length: 13.40 +/- 10.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | -95.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 519000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011618966 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.0765      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.9        |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 87.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=520000, episode_reward=-88.32 +/- 8.87\n",
      "Episode length: 37.00 +/- 27.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 37       |\n",
      "|    mean_reward     | -88.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.1     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 1522     |\n",
      "|    total_timesteps | 520192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=521000, episode_reward=-96.67 +/- 2.50\n",
      "Episode length: 12.20 +/- 4.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 12.2       |\n",
      "|    mean_reward          | -96.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 521000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01132744 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.191      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 31.9       |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 77.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=522000, episode_reward=-89.70 +/- 6.86\n",
      "Episode length: 36.40 +/- 26.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 36.4     |\n",
      "|    mean_reward     | -89.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 522000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.1     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 255      |\n",
      "|    time_elapsed    | 1529     |\n",
      "|    total_timesteps | 522240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=523000, episode_reward=-94.75 +/- 2.10\n",
      "Episode length: 14.00 +/- 4.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 523000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010508067 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.00529     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.8        |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=524000, episode_reward=-92.89 +/- 7.36\n",
      "Episode length: 24.40 +/- 13.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.4     |\n",
      "|    mean_reward     | -92.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 524000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 1533     |\n",
      "|    total_timesteps | 524288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=-96.73 +/- 5.00\n",
      "Episode length: 11.80 +/- 12.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.8       |\n",
      "|    mean_reward          | -96.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 525000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01200523 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.269      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 17.1       |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    value_loss           | 62.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=526000, episode_reward=-91.07 +/- 16.33\n",
      "Episode length: 54.40 +/- 54.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 54.4     |\n",
      "|    mean_reward     | -91.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 526000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 257      |\n",
      "|    time_elapsed    | 1539     |\n",
      "|    total_timesteps | 526336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=527000, episode_reward=-91.16 +/- 2.40\n",
      "Episode length: 21.80 +/- 5.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.8        |\n",
      "|    mean_reward          | -91.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 527000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013200058 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.315       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28          |\n",
      "|    n_updates            | 2570        |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 83.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=528000, episode_reward=-90.61 +/- 6.62\n",
      "Episode length: 23.80 +/- 15.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.8     |\n",
      "|    mean_reward     | -90.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 528000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 1544     |\n",
      "|    total_timesteps | 528384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=529000, episode_reward=-86.89 +/- 12.02\n",
      "Episode length: 41.20 +/- 32.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 41.2         |\n",
      "|    mean_reward          | -86.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 529000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106650395 |\n",
      "|    clip_fraction        | 0.0993       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0.00635      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.8         |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -0.0157      |\n",
      "|    value_loss           | 75.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=-87.65 +/- 14.88\n",
      "Episode length: 46.00 +/- 61.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 46       |\n",
      "|    mean_reward     | -87.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 259      |\n",
      "|    time_elapsed    | 1550     |\n",
      "|    total_timesteps | 530432   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=531000, episode_reward=-88.73 +/- 10.21\n",
      "Episode length: 36.40 +/- 37.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 36.4        |\n",
      "|    mean_reward          | -88.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 531000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009839259 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.8        |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 82.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=532000, episode_reward=-93.02 +/- 4.91\n",
      "Episode length: 19.40 +/- 16.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.4     |\n",
      "|    mean_reward     | -93      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 532000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24       |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 1555     |\n",
      "|    total_timesteps | 532480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=533000, episode_reward=-92.94 +/- 4.47\n",
      "Episode length: 15.80 +/- 9.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | -92.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 533000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010988245 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.0781      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 78.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=534000, episode_reward=-96.92 +/- 4.08\n",
      "Episode length: 11.60 +/- 9.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 534000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.1     |\n",
      "|    ep_rew_mean     | -92.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 261      |\n",
      "|    time_elapsed    | 1560     |\n",
      "|    total_timesteps | 534528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=-93.92 +/- 4.86\n",
      "Episode length: 22.60 +/- 15.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -93.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 535000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011920458 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.7        |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 80.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=536000, episode_reward=-90.71 +/- 5.04\n",
      "Episode length: 26.20 +/- 18.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.2     |\n",
      "|    mean_reward     | -90.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 536000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.3     |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 342      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 1568     |\n",
      "|    total_timesteps | 536576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=537000, episode_reward=-89.29 +/- 9.76\n",
      "Episode length: 32.20 +/- 25.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 32.2        |\n",
      "|    mean_reward          | -89.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 537000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010885798 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.083       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.9        |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 97.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=538000, episode_reward=-92.20 +/- 5.83\n",
      "Episode length: 21.00 +/- 16.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21       |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 538000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 263      |\n",
      "|    time_elapsed    | 1577     |\n",
      "|    total_timesteps | 538624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=539000, episode_reward=-93.48 +/- 9.18\n",
      "Episode length: 21.60 +/- 27.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.6        |\n",
      "|    mean_reward          | -93.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 539000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012149531 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.0348      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.7        |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 87.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=-95.35 +/- 5.89\n",
      "Episode length: 13.20 +/- 12.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -95.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 1583     |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=541000, episode_reward=-95.05 +/- 4.66\n",
      "Episode length: 13.40 +/- 9.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | -95         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 541000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012568279 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.0049      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.3        |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 97.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=542000, episode_reward=-94.60 +/- 10.38\n",
      "Episode length: 64.20 +/- 60.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 64.2     |\n",
      "|    mean_reward     | -94.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 542000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 265      |\n",
      "|    time_elapsed    | 1589     |\n",
      "|    total_timesteps | 542720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=543000, episode_reward=-92.46 +/- 3.69\n",
      "Episode length: 23.20 +/- 14.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.2        |\n",
      "|    mean_reward          | -92.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 543000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013325666 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.7        |\n",
      "|    n_updates            | 2650        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 92          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=544000, episode_reward=-95.63 +/- 2.11\n",
      "Episode length: 14.60 +/- 9.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.6     |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 544000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | -94.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 1594     |\n",
      "|    total_timesteps | 544768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=-96.54 +/- 1.94\n",
      "Episode length: 8.60 +/- 4.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | -96.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 545000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014690725 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.6        |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 91.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=546000, episode_reward=-92.45 +/- 7.48\n",
      "Episode length: 25.40 +/- 26.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.4     |\n",
      "|    mean_reward     | -92.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 546000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.6     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 267      |\n",
      "|    time_elapsed    | 1599     |\n",
      "|    total_timesteps | 546816   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=547000, episode_reward=-90.22 +/- 4.37\n",
      "Episode length: 29.00 +/- 19.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 29           |\n",
      "|    mean_reward          | -90.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 547000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0130264405 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.229        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.7         |\n",
      "|    n_updates            | 2670         |\n",
      "|    policy_gradient_loss | -0.0202      |\n",
      "|    value_loss           | 85.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=548000, episode_reward=-96.06 +/- 12.31\n",
      "Episode length: 23.60 +/- 22.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.6     |\n",
      "|    mean_reward     | -96.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 548000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.6     |\n",
      "|    ep_rew_mean     | -95.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 1606     |\n",
      "|    total_timesteps | 548864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=549000, episode_reward=-88.20 +/- 14.18\n",
      "Episode length: 40.60 +/- 47.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 40.6        |\n",
      "|    mean_reward          | -88.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 549000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016457176 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24          |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 68.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=-86.29 +/- 8.20\n",
      "Episode length: 36.00 +/- 20.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 36       |\n",
      "|    mean_reward     | -86.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.7     |\n",
      "|    ep_rew_mean     | -94.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 269      |\n",
      "|    time_elapsed    | 1614     |\n",
      "|    total_timesteps | 550912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=551000, episode_reward=-95.58 +/- 2.50\n",
      "Episode length: 12.00 +/- 5.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | -95.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 551000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013989957 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.6        |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 77.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=552000, episode_reward=-88.83 +/- 10.23\n",
      "Episode length: 29.00 +/- 33.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 29       |\n",
      "|    mean_reward     | -88.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 552000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 1623     |\n",
      "|    total_timesteps | 552960   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=553000, episode_reward=-94.76 +/- 4.19\n",
      "Episode length: 16.00 +/- 14.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 553000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012337324 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 75.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=554000, episode_reward=-93.74 +/- 4.99\n",
      "Episode length: 17.60 +/- 13.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.6     |\n",
      "|    mean_reward     | -93.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 554000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=-95.89 +/- 4.01\n",
      "Episode length: 20.40 +/- 9.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 555000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.3     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 271      |\n",
      "|    time_elapsed    | 1629     |\n",
      "|    total_timesteps | 555008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=556000, episode_reward=-93.42 +/- 3.48\n",
      "Episode length: 18.80 +/- 12.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.8        |\n",
      "|    mean_reward          | -93.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 556000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013018606 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.6        |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 52.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=557000, episode_reward=-92.85 +/- 7.21\n",
      "Episode length: 31.20 +/- 45.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 31.2     |\n",
      "|    mean_reward     | -92.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 557000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.6     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 1635     |\n",
      "|    total_timesteps | 557056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=558000, episode_reward=-95.41 +/- 3.34\n",
      "Episode length: 18.00 +/- 14.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 558000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011410702 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 67.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=559000, episode_reward=-96.25 +/- 5.11\n",
      "Episode length: 9.00 +/- 10.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -96.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 559000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 273      |\n",
      "|    time_elapsed    | 1640     |\n",
      "|    total_timesteps | 559104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=-91.60 +/- 4.15\n",
      "Episode length: 24.80 +/- 19.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.8        |\n",
      "|    mean_reward          | -91.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011001985 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 68.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=561000, episode_reward=-90.26 +/- 5.80\n",
      "Episode length: 36.20 +/- 38.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 36.2     |\n",
      "|    mean_reward     | -90.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 561000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 1645     |\n",
      "|    total_timesteps | 561152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=562000, episode_reward=-95.99 +/- 3.14\n",
      "Episode length: 11.60 +/- 5.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | -96         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 562000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009282149 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.257       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 73.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=563000, episode_reward=-97.19 +/- 3.25\n",
      "Episode length: 9.60 +/- 11.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 563000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 275      |\n",
      "|    time_elapsed    | 1651     |\n",
      "|    total_timesteps | 563200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=564000, episode_reward=-95.54 +/- 10.07\n",
      "Episode length: 25.60 +/- 7.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 25.6       |\n",
      "|    mean_reward          | -95.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 564000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00851678 |\n",
      "|    clip_fraction        | 0.0935     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.23      |\n",
      "|    explained_variance   | 0.282      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 30.7       |\n",
      "|    n_updates            | 2750       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    value_loss           | 89         |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=565000, episode_reward=-97.52 +/- 3.98\n",
      "Episode length: 8.20 +/- 6.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | -97.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 565000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.7     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 1656     |\n",
      "|    total_timesteps | 565248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=566000, episode_reward=-92.03 +/- 9.18\n",
      "Episode length: 33.60 +/- 22.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 33.6        |\n",
      "|    mean_reward          | -92         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 566000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012419042 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | -0.0335     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.8        |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 79.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=567000, episode_reward=-94.64 +/- 3.53\n",
      "Episode length: 15.20 +/- 7.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.2     |\n",
      "|    mean_reward     | -94.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 567000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 277      |\n",
      "|    time_elapsed    | 1661     |\n",
      "|    total_timesteps | 567296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=568000, episode_reward=-93.61 +/- 3.00\n",
      "Episode length: 16.40 +/- 7.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.4        |\n",
      "|    mean_reward          | -93.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 568000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014911002 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.172       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.9        |\n",
      "|    n_updates            | 2770        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 68.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=569000, episode_reward=-97.97 +/- 2.13\n",
      "Episode length: 13.20 +/- 6.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -98      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 569000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 1669     |\n",
      "|    total_timesteps | 569344   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=-91.19 +/- 6.83\n",
      "Episode length: 29.60 +/- 29.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 29.6         |\n",
      "|    mean_reward          | -91.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 570000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106425155 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.1         |\n",
      "|    n_updates            | 2780         |\n",
      "|    policy_gradient_loss | -0.0166      |\n",
      "|    value_loss           | 64.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=571000, episode_reward=-93.07 +/- 4.13\n",
      "Episode length: 21.00 +/- 11.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21       |\n",
      "|    mean_reward     | -93.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 571000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 279      |\n",
      "|    time_elapsed    | 1677     |\n",
      "|    total_timesteps | 571392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=572000, episode_reward=-94.57 +/- 2.89\n",
      "Episode length: 16.40 +/- 8.11\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.4       |\n",
      "|    mean_reward          | -94.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 572000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01398918 |\n",
      "|    clip_fraction        | 0.0984     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.28      |\n",
      "|    explained_variance   | 0.108      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 33.8       |\n",
      "|    n_updates            | 2790       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    value_loss           | 81.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=573000, episode_reward=-88.28 +/- 8.18\n",
      "Episode length: 33.40 +/- 21.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.4     |\n",
      "|    mean_reward     | -88.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 573000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.9     |\n",
      "|    ep_rew_mean     | -94.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 1683     |\n",
      "|    total_timesteps | 573440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=574000, episode_reward=-89.53 +/- 8.25\n",
      "Episode length: 29.80 +/- 27.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.8        |\n",
      "|    mean_reward          | -89.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 574000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010698718 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.4        |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 80          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=-93.44 +/- 6.34\n",
      "Episode length: 19.00 +/- 21.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19       |\n",
      "|    mean_reward     | -93.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 575000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.5     |\n",
      "|    ep_rew_mean     | -92.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 281      |\n",
      "|    time_elapsed    | 1688     |\n",
      "|    total_timesteps | 575488   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=576000, episode_reward=-84.23 +/- 10.93\n",
      "Episode length: 46.60 +/- 34.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 46.6        |\n",
      "|    mean_reward          | -84.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 576000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011788007 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 74.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=577000, episode_reward=-98.49 +/- 1.40\n",
      "Episode length: 12.60 +/- 8.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.6     |\n",
      "|    mean_reward     | -98.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 577000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 1693     |\n",
      "|    total_timesteps | 577536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=578000, episode_reward=-91.93 +/- 4.73\n",
      "Episode length: 23.40 +/- 12.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.4        |\n",
      "|    mean_reward          | -91.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 578000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009615891 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.0504      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.4        |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 89.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=579000, episode_reward=-95.75 +/- 2.41\n",
      "Episode length: 10.60 +/- 4.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 579000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 283      |\n",
      "|    time_elapsed    | 1699     |\n",
      "|    total_timesteps | 579584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=-93.54 +/- 7.55\n",
      "Episode length: 18.60 +/- 18.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.6        |\n",
      "|    mean_reward          | -93.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014965175 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.0137      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.2        |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 68.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=581000, episode_reward=-92.15 +/- 6.32\n",
      "Episode length: 23.20 +/- 13.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.2     |\n",
      "|    mean_reward     | -92.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 581000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 1704     |\n",
      "|    total_timesteps | 581632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=582000, episode_reward=-96.78 +/- 2.26\n",
      "Episode length: 10.60 +/- 9.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.6        |\n",
      "|    mean_reward          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 582000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011254313 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.4        |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 82.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=583000, episode_reward=-90.57 +/- 7.40\n",
      "Episode length: 21.40 +/- 15.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.4     |\n",
      "|    mean_reward     | -90.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 583000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.2     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 285      |\n",
      "|    time_elapsed    | 1712     |\n",
      "|    total_timesteps | 583680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=584000, episode_reward=-84.69 +/- 15.34\n",
      "Episode length: 45.20 +/- 47.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 45.2        |\n",
      "|    mean_reward          | -84.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 584000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010885866 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.2        |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 84          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=-92.58 +/- 5.43\n",
      "Episode length: 21.40 +/- 13.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.4     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 585000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 1721     |\n",
      "|    total_timesteps | 585728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=586000, episode_reward=-92.46 +/- 9.42\n",
      "Episode length: 24.20 +/- 22.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | -92.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 586000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009479027 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.8        |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 87.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=587000, episode_reward=-91.99 +/- 11.44\n",
      "Episode length: 24.60 +/- 34.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.6     |\n",
      "|    mean_reward     | -92      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 587000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 287      |\n",
      "|    time_elapsed    | 1728     |\n",
      "|    total_timesteps | 587776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=588000, episode_reward=-95.88 +/- 2.52\n",
      "Episode length: 18.20 +/- 18.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | -95.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 588000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012165316 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.3        |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 88.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=589000, episode_reward=-90.63 +/- 11.97\n",
      "Episode length: 30.20 +/- 39.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.2     |\n",
      "|    mean_reward     | -90.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 589000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.8     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 1732     |\n",
      "|    total_timesteps | 589824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=-91.45 +/- 6.44\n",
      "Episode length: 25.40 +/- 20.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.4        |\n",
      "|    mean_reward          | -91.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 590000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013880912 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | -0.14       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.3        |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=591000, episode_reward=-92.98 +/- 5.51\n",
      "Episode length: 18.00 +/- 15.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18       |\n",
      "|    mean_reward     | -93      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 591000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 289      |\n",
      "|    time_elapsed    | 1737     |\n",
      "|    total_timesteps | 591872   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=592000, episode_reward=-97.17 +/- 5.47\n",
      "Episode length: 12.20 +/- 13.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 12.2         |\n",
      "|    mean_reward          | -97.2        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 592000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125822835 |\n",
      "|    clip_fraction        | 0.115        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | -0.0591      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.2         |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -0.0201      |\n",
      "|    value_loss           | 96.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=593000, episode_reward=-91.82 +/- 10.96\n",
      "Episode length: 30.80 +/- 29.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.8     |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 593000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 1743     |\n",
      "|    total_timesteps | 593920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=594000, episode_reward=-96.41 +/- 1.75\n",
      "Episode length: 12.40 +/- 7.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.4        |\n",
      "|    mean_reward          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 594000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012286635 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -0.00174    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.4        |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 86.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=-87.39 +/- 9.35\n",
      "Episode length: 41.60 +/- 39.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 41.6     |\n",
      "|    mean_reward     | -87.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 595000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.8     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 291      |\n",
      "|    time_elapsed    | 1748     |\n",
      "|    total_timesteps | 595968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=596000, episode_reward=-89.81 +/- 11.68\n",
      "Episode length: 30.20 +/- 35.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.2        |\n",
      "|    mean_reward          | -89.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 596000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012394344 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.4        |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 81.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=597000, episode_reward=-95.45 +/- 1.81\n",
      "Episode length: 10.80 +/- 3.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.8     |\n",
      "|    mean_reward     | -95.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 597000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=598000, episode_reward=-92.27 +/- 5.70\n",
      "Episode length: 18.40 +/- 12.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | -92.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 598000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | -92.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 1753     |\n",
      "|    total_timesteps | 598016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=599000, episode_reward=-94.63 +/- 7.35\n",
      "Episode length: 17.00 +/- 24.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17          |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 599000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012239718 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.047       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.3        |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 80.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=-95.67 +/- 5.56\n",
      "Episode length: 10.60 +/- 11.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.4     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 1758     |\n",
      "|    total_timesteps | 600064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=601000, episode_reward=-95.74 +/- 2.47\n",
      "Episode length: 11.20 +/- 6.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.2        |\n",
      "|    mean_reward          | -95.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 601000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014486689 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.0743      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.8        |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 82.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=602000, episode_reward=-91.19 +/- 2.87\n",
      "Episode length: 28.80 +/- 19.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.8     |\n",
      "|    mean_reward     | -91.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 602000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.2     |\n",
      "|    ep_rew_mean     | -92      |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 294      |\n",
      "|    time_elapsed    | 1764     |\n",
      "|    total_timesteps | 602112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=603000, episode_reward=-78.08 +/- 21.99\n",
      "Episode length: 64.40 +/- 67.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 64.4        |\n",
      "|    mean_reward          | -78.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 603000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012908639 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.4        |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 78.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=604000, episode_reward=-97.09 +/- 2.95\n",
      "Episode length: 9.20 +/- 4.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | -97.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 604000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.5     |\n",
      "|    ep_rew_mean     | -91.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 1772     |\n",
      "|    total_timesteps | 604160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=-91.69 +/- 4.42\n",
      "Episode length: 22.60 +/- 14.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.6        |\n",
      "|    mean_reward          | -91.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 605000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011408125 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.1        |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 88          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=606000, episode_reward=-96.61 +/- 2.16\n",
      "Episode length: 15.80 +/- 15.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | -96.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 606000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.3     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 296      |\n",
      "|    time_elapsed    | 1781     |\n",
      "|    total_timesteps | 606208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=607000, episode_reward=-96.61 +/- 4.20\n",
      "Episode length: 9.80 +/- 7.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | -96.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 607000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009380946 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.9        |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=608000, episode_reward=-96.78 +/- 3.10\n",
      "Episode length: 8.40 +/- 6.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | -96.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 608000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 297      |\n",
      "|    time_elapsed    | 1787     |\n",
      "|    total_timesteps | 608256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=609000, episode_reward=-94.97 +/- 3.27\n",
      "Episode length: 24.00 +/- 8.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24          |\n",
      "|    mean_reward          | -95         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 609000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012385393 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45.7        |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=610000, episode_reward=-96.68 +/- 3.05\n",
      "Episode length: 7.80 +/- 6.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | -96.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 298      |\n",
      "|    time_elapsed    | 1792     |\n",
      "|    total_timesteps | 610304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=611000, episode_reward=-93.80 +/- 6.07\n",
      "Episode length: 21.80 +/- 21.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 21.8         |\n",
      "|    mean_reward          | -93.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 611000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114527065 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.0569       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.9         |\n",
      "|    n_updates            | 2980         |\n",
      "|    policy_gradient_loss | -0.0209      |\n",
      "|    value_loss           | 87           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=612000, episode_reward=-94.43 +/- 3.21\n",
      "Episode length: 15.00 +/- 8.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15       |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 612000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.5     |\n",
      "|    ep_rew_mean     | -94.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 299      |\n",
      "|    time_elapsed    | 1797     |\n",
      "|    total_timesteps | 612352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=613000, episode_reward=-94.72 +/- 5.55\n",
      "Episode length: 15.40 +/- 11.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.4        |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 613000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010976665 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.4        |\n",
      "|    n_updates            | 2990        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 95.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=614000, episode_reward=-93.70 +/- 2.93\n",
      "Episode length: 16.60 +/- 7.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | -93.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 614000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 1803     |\n",
      "|    total_timesteps | 614400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=-88.90 +/- 5.49\n",
      "Episode length: 29.20 +/- 13.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.2        |\n",
      "|    mean_reward          | -88.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 615000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009695742 |\n",
      "|    clip_fraction        | 0.0915      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.9        |\n",
      "|    n_updates            | 3000        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 94.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=616000, episode_reward=-94.40 +/- 7.96\n",
      "Episode length: 16.80 +/- 23.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.8     |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 616000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.8     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 301      |\n",
      "|    time_elapsed    | 1809     |\n",
      "|    total_timesteps | 616448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=617000, episode_reward=-97.70 +/- 1.17\n",
      "Episode length: 8.00 +/- 6.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -97.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 617000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011708967 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.3        |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 88.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=618000, episode_reward=-93.24 +/- 3.66\n",
      "Episode length: 21.00 +/- 13.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21       |\n",
      "|    mean_reward     | -93.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 618000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.3     |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 302      |\n",
      "|    time_elapsed    | 1814     |\n",
      "|    total_timesteps | 618496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=619000, episode_reward=-94.76 +/- 4.24\n",
      "Episode length: 16.00 +/- 10.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16          |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 619000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015857708 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.9        |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 79.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-89.75 +/- 9.34\n",
      "Episode length: 28.20 +/- 29.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.2     |\n",
      "|    mean_reward     | -89.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 1820     |\n",
      "|    total_timesteps | 620544   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=621000, episode_reward=-95.01 +/- 4.68\n",
      "Episode length: 15.40 +/- 10.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.4        |\n",
      "|    mean_reward          | -95         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 621000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011106411 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.9        |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 90.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=622000, episode_reward=-94.99 +/- 1.68\n",
      "Episode length: 15.60 +/- 11.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 622000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 304      |\n",
      "|    time_elapsed    | 1827     |\n",
      "|    total_timesteps | 622592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=623000, episode_reward=-93.48 +/- 6.12\n",
      "Episode length: 19.00 +/- 21.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19          |\n",
      "|    mean_reward          | -93.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 623000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013124639 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.0358     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.6        |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 93.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=624000, episode_reward=-96.25 +/- 1.98\n",
      "Episode length: 15.80 +/- 8.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 624000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 1835     |\n",
      "|    total_timesteps | 624640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=-92.33 +/- 6.11\n",
      "Episode length: 24.40 +/- 22.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 24.4       |\n",
      "|    mean_reward          | -92.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 625000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01284641 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.18      |\n",
      "|    explained_variance   | -0.176     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 30.4       |\n",
      "|    n_updates            | 3050       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    value_loss           | 81.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=626000, episode_reward=-92.30 +/- 8.87\n",
      "Episode length: 21.60 +/- 26.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.6     |\n",
      "|    mean_reward     | -92.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 626000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 306      |\n",
      "|    time_elapsed    | 1840     |\n",
      "|    total_timesteps | 626688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=627000, episode_reward=-87.85 +/- 6.04\n",
      "Episode length: 36.20 +/- 30.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 36.2        |\n",
      "|    mean_reward          | -87.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 627000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011596512 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.8        |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 88.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=628000, episode_reward=-95.57 +/- 2.74\n",
      "Episode length: 16.00 +/- 11.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 628000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 1845     |\n",
      "|    total_timesteps | 628736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=629000, episode_reward=-91.42 +/- 7.98\n",
      "Episode length: 24.60 +/- 23.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.6        |\n",
      "|    mean_reward          | -91.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 629000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009524256 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.0967      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.5        |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=-95.11 +/- 3.20\n",
      "Episode length: 12.80 +/- 7.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.8     |\n",
      "|    mean_reward     | -95.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 1851     |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=631000, episode_reward=-97.75 +/- 2.28\n",
      "Episode length: 6.60 +/- 5.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.6         |\n",
      "|    mean_reward          | -97.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 631000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011022569 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.8        |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 72.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=632000, episode_reward=-84.26 +/- 12.29\n",
      "Episode length: 51.80 +/- 45.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 51.8     |\n",
      "|    mean_reward     | -84.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 632000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.2     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 309      |\n",
      "|    time_elapsed    | 1856     |\n",
      "|    total_timesteps | 632832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=633000, episode_reward=-94.77 +/- 5.20\n",
      "Episode length: 17.20 +/- 16.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.2        |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 633000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009669326 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.3        |\n",
      "|    n_updates            | 3090        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 87          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=634000, episode_reward=-97.52 +/- 1.52\n",
      "Episode length: 17.60 +/- 21.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.6     |\n",
      "|    mean_reward     | -97.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 634000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.4     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 310      |\n",
      "|    time_elapsed    | 1863     |\n",
      "|    total_timesteps | 634880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=-93.68 +/- 3.61\n",
      "Episode length: 18.20 +/- 12.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | -93.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 635000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012460319 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.3        |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 79.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=636000, episode_reward=-96.04 +/- 5.42\n",
      "Episode length: 12.40 +/- 11.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | -96      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 636000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 1872     |\n",
      "|    total_timesteps | 636928   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=637000, episode_reward=-89.97 +/- 4.26\n",
      "Episode length: 26.80 +/- 11.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 26.8       |\n",
      "|    mean_reward          | -90        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 637000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01046853 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.12      |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 34.6       |\n",
      "|    n_updates            | 3110       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    value_loss           | 82.3       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=638000, episode_reward=-93.56 +/- 8.30\n",
      "Episode length: 24.00 +/- 32.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24       |\n",
      "|    mean_reward     | -93.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 638000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.8     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 312      |\n",
      "|    time_elapsed    | 1880     |\n",
      "|    total_timesteps | 638976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=639000, episode_reward=-91.93 +/- 5.96\n",
      "Episode length: 23.20 +/- 14.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.2        |\n",
      "|    mean_reward          | -91.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 639000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012932427 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.2        |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 81.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=-90.79 +/- 2.53\n",
      "Episode length: 23.00 +/- 9.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23       |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=641000, episode_reward=-91.84 +/- 6.71\n",
      "Episode length: 20.60 +/- 18.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 641000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.8     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 1885     |\n",
      "|    total_timesteps | 641024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=642000, episode_reward=-94.65 +/- 7.50\n",
      "Episode length: 14.00 +/- 18.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14          |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 642000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011543743 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.0776      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.1        |\n",
      "|    n_updates            | 3130        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=643000, episode_reward=-88.94 +/- 11.64\n",
      "Episode length: 44.80 +/- 49.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 44.8     |\n",
      "|    mean_reward     | -88.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 643000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 314      |\n",
      "|    time_elapsed    | 1890     |\n",
      "|    total_timesteps | 643072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=644000, episode_reward=-95.34 +/- 4.60\n",
      "Episode length: 15.60 +/- 10.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.6        |\n",
      "|    mean_reward          | -95.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 644000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014002865 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.0581      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 3140        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 70.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=-83.74 +/- 5.52\n",
      "Episode length: 48.00 +/- 15.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 48       |\n",
      "|    mean_reward     | -83.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 645000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 1896     |\n",
      "|    total_timesteps | 645120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=646000, episode_reward=-92.46 +/- 5.01\n",
      "Episode length: 17.00 +/- 11.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17          |\n",
      "|    mean_reward          | -92.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 646000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016771229 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.7        |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 71          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=647000, episode_reward=-94.10 +/- 3.51\n",
      "Episode length: 14.20 +/- 7.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | -94.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 647000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 316      |\n",
      "|    time_elapsed    | 1901     |\n",
      "|    total_timesteps | 647168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=648000, episode_reward=-87.83 +/- 15.17\n",
      "Episode length: 37.00 +/- 52.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 37          |\n",
      "|    mean_reward          | -87.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 648000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012386637 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | -0.0529     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 88          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=649000, episode_reward=-92.66 +/- 4.56\n",
      "Episode length: 17.60 +/- 10.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.6     |\n",
      "|    mean_reward     | -92.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 649000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.1     |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 317      |\n",
      "|    time_elapsed    | 1906     |\n",
      "|    total_timesteps | 649216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=-91.39 +/- 13.72\n",
      "Episode length: 25.00 +/- 35.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25          |\n",
      "|    mean_reward          | -91.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 650000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011484947 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.00242     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 80.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=651000, episode_reward=-92.62 +/- 6.32\n",
      "Episode length: 20.60 +/- 18.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 651000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.7     |\n",
      "|    ep_rew_mean     | -91.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 318      |\n",
      "|    time_elapsed    | 1911     |\n",
      "|    total_timesteps | 651264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=652000, episode_reward=-95.52 +/- 2.53\n",
      "Episode length: 11.80 +/- 6.08\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.8       |\n",
      "|    mean_reward          | -95.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 652000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01100355 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.17       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 26.5       |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    value_loss           | 70.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=653000, episode_reward=-96.59 +/- 2.62\n",
      "Episode length: 11.60 +/- 9.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | -96.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 653000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.6     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 319      |\n",
      "|    time_elapsed    | 1918     |\n",
      "|    total_timesteps | 653312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=654000, episode_reward=-89.04 +/- 11.09\n",
      "Episode length: 44.80 +/- 40.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 44.8         |\n",
      "|    mean_reward          | -89          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 654000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117984805 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.191        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.5         |\n",
      "|    n_updates            | 3190         |\n",
      "|    policy_gradient_loss | -0.0205      |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=655000, episode_reward=-94.36 +/- 3.32\n",
      "Episode length: 14.20 +/- 7.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 655000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18       |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 320      |\n",
      "|    time_elapsed    | 1926     |\n",
      "|    total_timesteps | 655360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=656000, episode_reward=-96.79 +/- 1.91\n",
      "Episode length: 15.00 +/- 12.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 656000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010197612 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.7        |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=657000, episode_reward=-94.36 +/- 2.50\n",
      "Episode length: 17.20 +/- 8.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.2     |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 657000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.5     |\n",
      "|    ep_rew_mean     | -92      |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 1935     |\n",
      "|    total_timesteps | 657408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=658000, episode_reward=-88.47 +/- 8.76\n",
      "Episode length: 35.80 +/- 27.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 35.8       |\n",
      "|    mean_reward          | -88.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 658000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01083287 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.14      |\n",
      "|    explained_variance   | 0.23       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 28.3       |\n",
      "|    n_updates            | 3210       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    value_loss           | 76.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=659000, episode_reward=-93.71 +/- 3.47\n",
      "Episode length: 17.20 +/- 8.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.2     |\n",
      "|    mean_reward     | -93.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 659000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 322      |\n",
      "|    time_elapsed    | 1941     |\n",
      "|    total_timesteps | 659456   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=-93.74 +/- 4.35\n",
      "Episode length: 21.20 +/- 14.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.2        |\n",
      "|    mean_reward          | -93.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 660000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011687566 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0613      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.9        |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 85.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=661000, episode_reward=-92.04 +/- 5.62\n",
      "Episode length: 23.00 +/- 13.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23       |\n",
      "|    mean_reward     | -92      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 661000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.6     |\n",
      "|    ep_rew_mean     | -92      |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 1947     |\n",
      "|    total_timesteps | 661504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=662000, episode_reward=-97.31 +/- 2.70\n",
      "Episode length: 7.60 +/- 6.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7.6         |\n",
      "|    mean_reward          | -97.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 662000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009547237 |\n",
      "|    clip_fraction        | 0.0975      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.2        |\n",
      "|    n_updates            | 3230        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 84.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=663000, episode_reward=-97.43 +/- 2.29\n",
      "Episode length: 10.80 +/- 8.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.8     |\n",
      "|    mean_reward     | -97.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 663000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 324      |\n",
      "|    time_elapsed    | 1953     |\n",
      "|    total_timesteps | 663552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=664000, episode_reward=-93.78 +/- 5.75\n",
      "Episode length: 17.40 +/- 15.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.4        |\n",
      "|    mean_reward          | -93.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 664000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010700606 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.4        |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=-92.77 +/- 5.32\n",
      "Episode length: 17.80 +/- 13.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 665000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.3     |\n",
      "|    ep_rew_mean     | -91.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 325      |\n",
      "|    time_elapsed    | 1959     |\n",
      "|    total_timesteps | 665600   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=666000, episode_reward=-85.53 +/- 9.19\n",
      "Episode length: 41.80 +/- 31.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 41.8       |\n",
      "|    mean_reward          | -85.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 666000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01009965 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.996     |\n",
      "|    explained_variance   | 0.226      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 47         |\n",
      "|    n_updates            | 3250       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    value_loss           | 90.4       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=667000, episode_reward=-95.63 +/- 5.92\n",
      "Episode length: 12.00 +/- 15.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12       |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 667000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23       |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 326      |\n",
      "|    time_elapsed    | 1968     |\n",
      "|    total_timesteps | 667648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=668000, episode_reward=-89.50 +/- 7.11\n",
      "Episode length: 31.80 +/- 17.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 31.8        |\n",
      "|    mean_reward          | -89.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 668000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009278823 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 46.7        |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=669000, episode_reward=-96.78 +/- 2.28\n",
      "Episode length: 8.80 +/- 4.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -96.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 669000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 327      |\n",
      "|    time_elapsed    | 1976     |\n",
      "|    total_timesteps | 669696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=-83.16 +/- 15.07\n",
      "Episode length: 53.60 +/- 38.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 53.6        |\n",
      "|    mean_reward          | -83.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011744027 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.6        |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 93.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=671000, episode_reward=-91.20 +/- 6.81\n",
      "Episode length: 24.20 +/- 23.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.2     |\n",
      "|    mean_reward     | -91.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 671000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.1     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 328      |\n",
      "|    time_elapsed    | 1984     |\n",
      "|    total_timesteps | 671744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=672000, episode_reward=-94.56 +/- 6.47\n",
      "Episode length: 14.20 +/- 15.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 14.2         |\n",
      "|    mean_reward          | -94.6        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 672000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0133091975 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0909       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.8         |\n",
      "|    n_updates            | 3280         |\n",
      "|    policy_gradient_loss | -0.0199      |\n",
      "|    value_loss           | 77.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=673000, episode_reward=-94.26 +/- 2.73\n",
      "Episode length: 15.20 +/- 7.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.2     |\n",
      "|    mean_reward     | -94.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 673000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 329      |\n",
      "|    time_elapsed    | 1989     |\n",
      "|    total_timesteps | 673792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=674000, episode_reward=-95.19 +/- 2.93\n",
      "Episode length: 14.80 +/- 7.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.8        |\n",
      "|    mean_reward          | -95.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 674000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014826274 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 82.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=-90.83 +/- 5.77\n",
      "Episode length: 28.80 +/- 20.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.8     |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 675000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 330      |\n",
      "|    time_elapsed    | 1994     |\n",
      "|    total_timesteps | 675840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=676000, episode_reward=-94.42 +/- 4.68\n",
      "Episode length: 13.60 +/- 11.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | -94.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 676000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010195849 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.981      |\n",
      "|    explained_variance   | 0.0424      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31          |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 85          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=677000, episode_reward=-94.94 +/- 4.85\n",
      "Episode length: 18.80 +/- 13.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | -94.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 677000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | -91.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 331      |\n",
      "|    time_elapsed    | 1999     |\n",
      "|    total_timesteps | 677888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=678000, episode_reward=-91.60 +/- 6.16\n",
      "Episode length: 21.20 +/- 15.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.2        |\n",
      "|    mean_reward          | -91.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 678000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012890012 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.98       |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 79.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=679000, episode_reward=-94.45 +/- 4.35\n",
      "Episode length: 15.60 +/- 14.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 679000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.7     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 332      |\n",
      "|    time_elapsed    | 2003     |\n",
      "|    total_timesteps | 679936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=-92.18 +/- 6.92\n",
      "Episode length: 17.60 +/- 14.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17.6        |\n",
      "|    mean_reward          | -92.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 680000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012904171 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.063       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.2        |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 75.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=681000, episode_reward=-92.76 +/- 6.07\n",
      "Episode length: 21.40 +/- 20.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.4     |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 681000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.9     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 2008     |\n",
      "|    total_timesteps | 681984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=682000, episode_reward=-89.63 +/- 8.85\n",
      "Episode length: 26.60 +/- 24.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.6        |\n",
      "|    mean_reward          | -89.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 682000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011477816 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.07        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 67.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=683000, episode_reward=-95.61 +/- 2.58\n",
      "Episode length: 10.00 +/- 5.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 683000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=684000, episode_reward=-91.23 +/- 6.81\n",
      "Episode length: 23.20 +/- 19.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.2     |\n",
      "|    mean_reward     | -91.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 684000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29       |\n",
      "|    ep_rew_mean     | -91      |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 2013     |\n",
      "|    total_timesteps | 684032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=-82.71 +/- 15.01\n",
      "Episode length: 53.60 +/- 51.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 53.6        |\n",
      "|    mean_reward          | -82.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 685000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009481104 |\n",
      "|    clip_fraction        | 0.0902      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.835      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.2        |\n",
      "|    n_updates            | 3340        |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 71.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=686000, episode_reward=-92.18 +/- 5.38\n",
      "Episode length: 24.80 +/- 14.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.8     |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 686000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.1     |\n",
      "|    ep_rew_mean     | -92.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 335      |\n",
      "|    time_elapsed    | 2018     |\n",
      "|    total_timesteps | 686080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=687000, episode_reward=-89.31 +/- 5.77\n",
      "Episode length: 31.20 +/- 16.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 31.2        |\n",
      "|    mean_reward          | -89.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 687000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008591026 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.2        |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 90.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=688000, episode_reward=-88.38 +/- 11.96\n",
      "Episode length: 37.60 +/- 39.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 37.6     |\n",
      "|    mean_reward     | -88.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 688000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 2025     |\n",
      "|    total_timesteps | 688128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=689000, episode_reward=-95.63 +/- 5.47\n",
      "Episode length: 14.60 +/- 19.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.6        |\n",
      "|    mean_reward          | -95.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 689000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011345185 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.0819      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.9        |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 93.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=-92.08 +/- 3.54\n",
      "Episode length: 19.40 +/- 8.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.4     |\n",
      "|    mean_reward     | -92.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.1     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 337      |\n",
      "|    time_elapsed    | 2034     |\n",
      "|    total_timesteps | 690176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=691000, episode_reward=-95.80 +/- 3.27\n",
      "Episode length: 15.40 +/- 9.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.4        |\n",
      "|    mean_reward          | -95.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 691000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010039508 |\n",
      "|    clip_fraction        | 0.0967      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | -0.00179    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.9        |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 91          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=692000, episode_reward=-92.82 +/- 6.28\n",
      "Episode length: 19.00 +/- 16.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19       |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 692000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 2041     |\n",
      "|    total_timesteps | 692224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=693000, episode_reward=-94.77 +/- 2.20\n",
      "Episode length: 13.60 +/- 5.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 693000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015522614 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.0952     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.7        |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 76.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=694000, episode_reward=-97.62 +/- 2.89\n",
      "Episode length: 7.80 +/- 5.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | -97.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 694000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 339      |\n",
      "|    time_elapsed    | 2048     |\n",
      "|    total_timesteps | 694272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=-85.52 +/- 13.25\n",
      "Episode length: 44.00 +/- 43.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 44         |\n",
      "|    mean_reward          | -85.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 695000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01074148 |\n",
      "|    clip_fraction        | 0.0894     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.03      |\n",
      "|    explained_variance   | 0.0717     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.3       |\n",
      "|    n_updates            | 3390       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    value_loss           | 79.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=696000, episode_reward=-94.71 +/- 5.78\n",
      "Episode length: 18.40 +/- 18.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | -94.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 696000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 2053     |\n",
      "|    total_timesteps | 696320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=697000, episode_reward=-94.27 +/- 3.59\n",
      "Episode length: 14.80 +/- 8.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.8        |\n",
      "|    mean_reward          | -94.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 697000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011662861 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.1        |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 76.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=698000, episode_reward=-94.99 +/- 3.07\n",
      "Episode length: 12.60 +/- 6.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.6     |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 698000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.7     |\n",
      "|    ep_rew_mean     | -91.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 341      |\n",
      "|    time_elapsed    | 2059     |\n",
      "|    total_timesteps | 698368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=699000, episode_reward=-87.77 +/- 5.22\n",
      "Episode length: 39.20 +/- 20.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 39.2        |\n",
      "|    mean_reward          | -87.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 699000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012088082 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.92       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.9        |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 64.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=700000, episode_reward=-96.34 +/- 3.36\n",
      "Episode length: 9.00 +/- 6.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 2064     |\n",
      "|    total_timesteps | 700416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=701000, episode_reward=-95.18 +/- 3.54\n",
      "Episode length: 11.60 +/- 8.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | -95.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 701000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011177353 |\n",
      "|    clip_fraction        | 0.0923      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.0853     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.5        |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 91.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=702000, episode_reward=-86.63 +/- 11.17\n",
      "Episode length: 33.40 +/- 29.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.4     |\n",
      "|    mean_reward     | -86.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 702000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.8     |\n",
      "|    ep_rew_mean     | -91.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 343      |\n",
      "|    time_elapsed    | 2070     |\n",
      "|    total_timesteps | 702464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=703000, episode_reward=-93.84 +/- 5.30\n",
      "Episode length: 15.80 +/- 13.44\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.8       |\n",
      "|    mean_reward          | -93.8      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 703000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01310751 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.956     |\n",
      "|    explained_variance   | 0.212      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 17.7       |\n",
      "|    n_updates            | 3430       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    value_loss           | 71         |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=704000, episode_reward=-91.79 +/- 6.08\n",
      "Episode length: 27.00 +/- 16.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27       |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 704000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 2077     |\n",
      "|    total_timesteps | 704512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=-93.01 +/- 9.01\n",
      "Episode length: 27.20 +/- 31.57\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 27.2      |\n",
      "|    mean_reward          | -93       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 705000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0114364 |\n",
      "|    clip_fraction        | 0.109     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.07     |\n",
      "|    explained_variance   | -0.0701   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 41.2      |\n",
      "|    n_updates            | 3440      |\n",
      "|    policy_gradient_loss | -0.0185   |\n",
      "|    value_loss           | 87.9      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=706000, episode_reward=-90.75 +/- 5.49\n",
      "Episode length: 27.00 +/- 19.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27       |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 706000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25       |\n",
      "|    ep_rew_mean     | -91.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 345      |\n",
      "|    time_elapsed    | 2085     |\n",
      "|    total_timesteps | 706560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=707000, episode_reward=-97.50 +/- 2.23\n",
      "Episode length: 6.60 +/- 4.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6.6         |\n",
      "|    mean_reward          | -97.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 707000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011077287 |\n",
      "|    clip_fraction        | 0.0967      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 72.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=708000, episode_reward=-84.67 +/- 16.26\n",
      "Episode length: 53.00 +/- 52.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 53       |\n",
      "|    mean_reward     | -84.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 708000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.6     |\n",
      "|    ep_rew_mean     | -91.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 2092     |\n",
      "|    total_timesteps | 708608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=709000, episode_reward=-81.70 +/- 22.89\n",
      "Episode length: 67.00 +/- 92.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 67          |\n",
      "|    mean_reward          | -81.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 709000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009601955 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.1        |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 67.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=-89.26 +/- 11.88\n",
      "Episode length: 33.20 +/- 44.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.2     |\n",
      "|    mean_reward     | -89.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.9     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 347      |\n",
      "|    time_elapsed    | 2098     |\n",
      "|    total_timesteps | 710656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=711000, episode_reward=-90.00 +/- 11.26\n",
      "Episode length: 38.20 +/- 30.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 38.2       |\n",
      "|    mean_reward          | -90        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 711000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01153624 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.302      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 39.7       |\n",
      "|    n_updates            | 3470       |\n",
      "|    policy_gradient_loss | -0.021     |\n",
      "|    value_loss           | 88.5       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=712000, episode_reward=-90.18 +/- 6.71\n",
      "Episode length: 26.80 +/- 14.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.8     |\n",
      "|    mean_reward     | -90.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 712000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.2     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 2103     |\n",
      "|    total_timesteps | 712704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=713000, episode_reward=-87.72 +/- 6.28\n",
      "Episode length: 32.60 +/- 17.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 32.6        |\n",
      "|    mean_reward          | -87.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 713000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009903096 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.8        |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 78.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=714000, episode_reward=-94.27 +/- 5.84\n",
      "Episode length: 17.20 +/- 13.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.2     |\n",
      "|    mean_reward     | -94.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 714000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.4     |\n",
      "|    ep_rew_mean     | -91.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 349      |\n",
      "|    time_elapsed    | 2108     |\n",
      "|    total_timesteps | 714752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=-94.01 +/- 7.15\n",
      "Episode length: 20.80 +/- 27.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.8        |\n",
      "|    mean_reward          | -94         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 715000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009951822 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.928      |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.6        |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 68.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=716000, episode_reward=-92.38 +/- 6.57\n",
      "Episode length: 21.20 +/- 18.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.2     |\n",
      "|    mean_reward     | -92.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 716000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.5     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 2113     |\n",
      "|    total_timesteps | 716800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=717000, episode_reward=-96.46 +/- 2.66\n",
      "Episode length: 8.20 +/- 5.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.2         |\n",
      "|    mean_reward          | -96.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 717000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011100148 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | -0.154      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.5        |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 97.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=718000, episode_reward=-94.77 +/- 2.84\n",
      "Episode length: 13.20 +/- 6.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 718000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.5     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 351      |\n",
      "|    time_elapsed    | 2118     |\n",
      "|    total_timesteps | 718848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=719000, episode_reward=-87.90 +/- 7.63\n",
      "Episode length: 35.80 +/- 22.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 35.8        |\n",
      "|    mean_reward          | -87.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 719000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012428338 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 67.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=-95.99 +/- 4.51\n",
      "Episode length: 12.20 +/- 7.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | -96      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 2123     |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=721000, episode_reward=-94.10 +/- 6.25\n",
      "Episode length: 18.40 +/- 18.58\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 18.4       |\n",
      "|    mean_reward          | -94.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 721000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01216063 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | -0.0035    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 52.5       |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 96.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=722000, episode_reward=-89.26 +/- 8.21\n",
      "Episode length: 27.60 +/- 21.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27.6     |\n",
      "|    mean_reward     | -89.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 722000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.2     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 353      |\n",
      "|    time_elapsed    | 2128     |\n",
      "|    total_timesteps | 722944   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=723000, episode_reward=-94.57 +/- 4.48\n",
      "Episode length: 11.20 +/- 8.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.2        |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 723000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010692794 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.0044      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.5        |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 83.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=724000, episode_reward=-92.55 +/- 7.25\n",
      "Episode length: 19.00 +/- 17.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19       |\n",
      "|    mean_reward     | -92.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 724000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.6     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 2133     |\n",
      "|    total_timesteps | 724992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=-90.99 +/- 13.36\n",
      "Episode length: 32.80 +/- 51.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 32.8        |\n",
      "|    mean_reward          | -91         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 725000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011465857 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | -0.014      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 72.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=726000, episode_reward=-97.10 +/- 3.06\n",
      "Episode length: 8.60 +/- 6.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | -97.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 726000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=727000, episode_reward=-93.69 +/- 4.83\n",
      "Episode length: 17.80 +/- 14.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17.8     |\n",
      "|    mean_reward     | -93.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 727000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 355      |\n",
      "|    time_elapsed    | 2141     |\n",
      "|    total_timesteps | 727040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=728000, episode_reward=-93.20 +/- 10.36\n",
      "Episode length: 21.60 +/- 34.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.6        |\n",
      "|    mean_reward          | -93.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 728000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012727534 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.00449     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 66.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=729000, episode_reward=-97.51 +/- 1.28\n",
      "Episode length: 8.80 +/- 4.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -97.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 729000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.5     |\n",
      "|    ep_rew_mean     | -92      |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 2148     |\n",
      "|    total_timesteps | 729088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=-97.88 +/- 3.00\n",
      "Episode length: 13.60 +/- 12.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.6        |\n",
      "|    mean_reward          | -97.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 730000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015190682 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.0106     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 72.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=731000, episode_reward=-87.59 +/- 11.96\n",
      "Episode length: 32.40 +/- 30.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 32.4     |\n",
      "|    mean_reward     | -87.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 731000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 357      |\n",
      "|    time_elapsed    | 2155     |\n",
      "|    total_timesteps | 731136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=732000, episode_reward=-94.42 +/- 3.44\n",
      "Episode length: 13.40 +/- 7.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | -94.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 732000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012356129 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.1        |\n",
      "|    n_updates            | 3570        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 72          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=733000, episode_reward=-84.91 +/- 15.89\n",
      "Episode length: 48.20 +/- 54.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 48.2     |\n",
      "|    mean_reward     | -84.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 733000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.2     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 2161     |\n",
      "|    total_timesteps | 733184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=734000, episode_reward=-94.63 +/- 5.14\n",
      "Episode length: 15.00 +/- 15.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 734000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010811364 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.5        |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 75.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=735000, episode_reward=-90.44 +/- 3.30\n",
      "Episode length: 26.20 +/- 11.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.2     |\n",
      "|    mean_reward     | -90.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 735000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.7     |\n",
      "|    ep_rew_mean     | -92.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 359      |\n",
      "|    time_elapsed    | 2167     |\n",
      "|    total_timesteps | 735232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=736000, episode_reward=-85.94 +/- 7.12\n",
      "Episode length: 36.60 +/- 20.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 36.6        |\n",
      "|    mean_reward          | -85.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 736000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010692421 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.988      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.1        |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 77.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=737000, episode_reward=-90.56 +/- 8.27\n",
      "Episode length: 25.20 +/- 21.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.2     |\n",
      "|    mean_reward     | -90.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 737000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 2172     |\n",
      "|    total_timesteps | 737280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=738000, episode_reward=-90.08 +/- 6.67\n",
      "Episode length: 30.80 +/- 19.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.8        |\n",
      "|    mean_reward          | -90.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 738000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011512199 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | -0.308      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 90.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=739000, episode_reward=-94.90 +/- 5.27\n",
      "Episode length: 16.80 +/- 12.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.8     |\n",
      "|    mean_reward     | -94.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 739000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.9     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 361      |\n",
      "|    time_elapsed    | 2177     |\n",
      "|    total_timesteps | 739328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=-90.88 +/- 5.93\n",
      "Episode length: 22.20 +/- 12.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.2        |\n",
      "|    mean_reward          | -90.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 740000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010598896 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -0.0213     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.7        |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 74.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=741000, episode_reward=-82.81 +/- 13.12\n",
      "Episode length: 57.60 +/- 43.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 57.6     |\n",
      "|    mean_reward     | -82.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 741000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.4     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 2183     |\n",
      "|    total_timesteps | 741376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=742000, episode_reward=-95.57 +/- 3.74\n",
      "Episode length: 21.40 +/- 17.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.4        |\n",
      "|    mean_reward          | -95.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 742000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013546014 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 79.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=743000, episode_reward=-94.12 +/- 5.08\n",
      "Episode length: 16.00 +/- 15.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16       |\n",
      "|    mean_reward     | -94.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 743000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.1     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 363      |\n",
      "|    time_elapsed    | 2190     |\n",
      "|    total_timesteps | 743424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=744000, episode_reward=-94.96 +/- 3.40\n",
      "Episode length: 18.40 +/- 12.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 18.4         |\n",
      "|    mean_reward          | -95          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 744000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131785385 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.153        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.4         |\n",
      "|    n_updates            | 3630         |\n",
      "|    policy_gradient_loss | -0.0211      |\n",
      "|    value_loss           | 83.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=-95.12 +/- 5.78\n",
      "Episode length: 19.00 +/- 22.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19       |\n",
      "|    mean_reward     | -95.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 745000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | -92.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 2197     |\n",
      "|    total_timesteps | 745472   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=746000, episode_reward=-98.26 +/- 1.05\n",
      "Episode length: 5.40 +/- 3.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 5.4          |\n",
      "|    mean_reward          | -98.3        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 746000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0119067095 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.0824       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.5         |\n",
      "|    n_updates            | 3640         |\n",
      "|    policy_gradient_loss | -0.02        |\n",
      "|    value_loss           | 83           |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=747000, episode_reward=-92.10 +/- 9.32\n",
      "Episode length: 22.60 +/- 27.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -92.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 747000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 365      |\n",
      "|    time_elapsed    | 2205     |\n",
      "|    total_timesteps | 747520   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=748000, episode_reward=-85.82 +/- 8.53\n",
      "Episode length: 43.80 +/- 31.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 43.8        |\n",
      "|    mean_reward          | -85.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 748000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010437582 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.024       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.1        |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    value_loss           | 90.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=749000, episode_reward=-92.60 +/- 5.45\n",
      "Episode length: 20.20 +/- 17.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 749000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.7     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 2210     |\n",
      "|    total_timesteps | 749568   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=-90.46 +/- 7.09\n",
      "Episode length: 30.40 +/- 26.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.4        |\n",
      "|    mean_reward          | -90.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 750000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012522874 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.8        |\n",
      "|    n_updates            | 3660        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 86.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=751000, episode_reward=-96.14 +/- 4.18\n",
      "Episode length: 11.00 +/- 11.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11       |\n",
      "|    mean_reward     | -96.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 751000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.2     |\n",
      "|    ep_rew_mean     | -94.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 367      |\n",
      "|    time_elapsed    | 2216     |\n",
      "|    total_timesteps | 751616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=752000, episode_reward=-95.11 +/- 0.38\n",
      "Episode length: 14.80 +/- 5.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.8        |\n",
      "|    mean_reward          | -95.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 752000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012386142 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | -0.0746     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.4        |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 87.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=753000, episode_reward=-99.21 +/- 0.74\n",
      "Episode length: 2.80 +/- 1.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 2.8      |\n",
      "|    mean_reward     | -99.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 753000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.9     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 2221     |\n",
      "|    total_timesteps | 753664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=754000, episode_reward=-88.33 +/- 5.84\n",
      "Episode length: 36.20 +/- 15.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 36.2        |\n",
      "|    mean_reward          | -88.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 754000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009883979 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.0164      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.5        |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 82.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=-94.83 +/- 4.69\n",
      "Episode length: 14.80 +/- 12.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.8     |\n",
      "|    mean_reward     | -94.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 755000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.4     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 369      |\n",
      "|    time_elapsed    | 2226     |\n",
      "|    total_timesteps | 755712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=756000, episode_reward=-91.05 +/- 5.39\n",
      "Episode length: 24.80 +/- 12.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.8        |\n",
      "|    mean_reward          | -91         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 756000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013980502 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.00766     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 3690        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 77.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=757000, episode_reward=-95.52 +/- 6.24\n",
      "Episode length: 13.20 +/- 13.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.2     |\n",
      "|    mean_reward     | -95.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 757000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 2233     |\n",
      "|    total_timesteps | 757760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=758000, episode_reward=-88.46 +/- 12.88\n",
      "Episode length: 28.80 +/- 31.33\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 28.8       |\n",
      "|    mean_reward          | -88.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 758000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01703895 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 0.16       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20.4       |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    value_loss           | 64.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=759000, episode_reward=-86.34 +/- 12.74\n",
      "Episode length: 46.00 +/- 47.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 46       |\n",
      "|    mean_reward     | -86.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 759000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.3     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 371      |\n",
      "|    time_elapsed    | 2242     |\n",
      "|    total_timesteps | 759808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-91.89 +/- 4.57\n",
      "Episode length: 27.00 +/- 19.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27          |\n",
      "|    mean_reward          | -91.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 760000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011805244 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.0948      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 3710        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 74.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=761000, episode_reward=-89.79 +/- 11.63\n",
      "Episode length: 37.00 +/- 33.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 37       |\n",
      "|    mean_reward     | -89.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 761000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.5     |\n",
      "|    ep_rew_mean     | -91.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 2249     |\n",
      "|    total_timesteps | 761856   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=762000, episode_reward=-98.28 +/- 2.38\n",
      "Episode length: 9.00 +/- 13.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -98.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 762000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014484959 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.995      |\n",
      "|    explained_variance   | 0.00426     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.4        |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 69          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=763000, episode_reward=-86.93 +/- 9.24\n",
      "Episode length: 33.20 +/- 24.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.2     |\n",
      "|    mean_reward     | -86.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 763000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.5     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 373      |\n",
      "|    time_elapsed    | 2254     |\n",
      "|    total_timesteps | 763904   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=764000, episode_reward=-92.45 +/- 6.98\n",
      "Episode length: 32.80 +/- 37.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 32.8       |\n",
      "|    mean_reward          | -92.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 764000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01122779 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.16      |\n",
      "|    explained_variance   | 0.0455     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 36.4       |\n",
      "|    n_updates            | 3730       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 87.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=-93.68 +/- 5.90\n",
      "Episode length: 20.00 +/- 23.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | -93.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 765000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.1     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 2260     |\n",
      "|    total_timesteps | 765952   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=766000, episode_reward=-97.45 +/- 1.60\n",
      "Episode length: 9.60 +/- 1.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | -97.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 766000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012199896 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0757      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.3        |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 74.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=767000, episode_reward=-85.64 +/- 12.65\n",
      "Episode length: 49.20 +/- 47.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 49.2     |\n",
      "|    mean_reward     | -85.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 767000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=768000, episode_reward=-90.99 +/- 9.95\n",
      "Episode length: 23.00 +/- 26.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23       |\n",
      "|    mean_reward     | -91      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.5     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 375      |\n",
      "|    time_elapsed    | 2266     |\n",
      "|    total_timesteps | 768000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=769000, episode_reward=-95.22 +/- 2.73\n",
      "Episode length: 13.80 +/- 6.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | -95.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 769000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013771778 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.076       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.7        |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 92.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=-89.52 +/- 9.73\n",
      "Episode length: 39.80 +/- 38.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 39.8     |\n",
      "|    mean_reward     | -89.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.1     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 376      |\n",
      "|    time_elapsed    | 2272     |\n",
      "|    total_timesteps | 770048   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=771000, episode_reward=-89.19 +/- 14.46\n",
      "Episode length: 55.00 +/- 82.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 55          |\n",
      "|    mean_reward          | -89.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 771000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011655992 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23          |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 67.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=772000, episode_reward=-96.06 +/- 4.48\n",
      "Episode length: 15.40 +/- 20.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.4     |\n",
      "|    mean_reward     | -96.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 772000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.4     |\n",
      "|    ep_rew_mean     | -91.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 377      |\n",
      "|    time_elapsed    | 2280     |\n",
      "|    total_timesteps | 772096   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=773000, episode_reward=-91.66 +/- 6.14\n",
      "Episode length: 24.00 +/- 18.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24          |\n",
      "|    mean_reward          | -91.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 773000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010965481 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40          |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 89.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=774000, episode_reward=-94.22 +/- 3.49\n",
      "Episode length: 15.60 +/- 7.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | -94.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 774000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.2     |\n",
      "|    ep_rew_mean     | -91.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 378      |\n",
      "|    time_elapsed    | 2287     |\n",
      "|    total_timesteps | 774144   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=-94.72 +/- 4.44\n",
      "Episode length: 14.40 +/- 12.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.4        |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 775000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010309422 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 83.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=776000, episode_reward=-96.34 +/- 2.23\n",
      "Episode length: 10.80 +/- 6.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.8     |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 776000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 379      |\n",
      "|    time_elapsed    | 2292     |\n",
      "|    total_timesteps | 776192   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=777000, episode_reward=-93.42 +/- 2.07\n",
      "Episode length: 15.60 +/- 4.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.6        |\n",
      "|    mean_reward          | -93.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 777000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010895299 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.0862      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.9        |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 91.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=778000, episode_reward=-92.56 +/- 9.22\n",
      "Episode length: 23.20 +/- 28.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23.2     |\n",
      "|    mean_reward     | -92.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 778000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 380      |\n",
      "|    time_elapsed    | 2297     |\n",
      "|    total_timesteps | 778240   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=779000, episode_reward=-93.85 +/- 7.14\n",
      "Episode length: 16.80 +/- 19.81\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 16.8       |\n",
      "|    mean_reward          | -93.9      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 779000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01417264 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.19      |\n",
      "|    explained_variance   | 0.111      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 33.5       |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    value_loss           | 89.8       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=780000, episode_reward=-96.90 +/- 3.19\n",
      "Episode length: 7.80 +/- 7.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 2301     |\n",
      "|    total_timesteps | 780288   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=781000, episode_reward=-85.57 +/- 8.58\n",
      "Episode length: 41.80 +/- 28.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 41.8        |\n",
      "|    mean_reward          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 781000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011895868 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.0895      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.5        |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 81.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=782000, episode_reward=-95.04 +/- 4.57\n",
      "Episode length: 16.60 +/- 16.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 782000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.6     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 382      |\n",
      "|    time_elapsed    | 2306     |\n",
      "|    total_timesteps | 782336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=783000, episode_reward=-95.69 +/- 4.86\n",
      "Episode length: 15.20 +/- 14.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.2        |\n",
      "|    mean_reward          | -95.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 783000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009695206 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.107      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.8        |\n",
      "|    n_updates            | 3820        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=784000, episode_reward=-86.66 +/- 5.01\n",
      "Episode length: 33.60 +/- 13.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.6     |\n",
      "|    mean_reward     | -86.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 784000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 383      |\n",
      "|    time_elapsed    | 2311     |\n",
      "|    total_timesteps | 784384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=-92.38 +/- 7.76\n",
      "Episode length: 26.20 +/- 32.81\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 26.2      |\n",
      "|    mean_reward          | -92.4     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 785000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0142086 |\n",
      "|    clip_fraction        | 0.132     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.09     |\n",
      "|    explained_variance   | -0.0752   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 23        |\n",
      "|    n_updates            | 3830      |\n",
      "|    policy_gradient_loss | -0.0215   |\n",
      "|    value_loss           | 67.1      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=786000, episode_reward=-93.44 +/- 4.49\n",
      "Episode length: 20.20 +/- 14.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -93.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 786000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.7     |\n",
      "|    ep_rew_mean     | -91.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 384      |\n",
      "|    time_elapsed    | 2316     |\n",
      "|    total_timesteps | 786432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=787000, episode_reward=-92.69 +/- 5.28\n",
      "Episode length: 19.20 +/- 15.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19.2        |\n",
      "|    mean_reward          | -92.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 787000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010961257 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.926      |\n",
      "|    explained_variance   | -0.111      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.2        |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 70.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=788000, episode_reward=-97.02 +/- 1.13\n",
      "Episode length: 9.20 +/- 5.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | -97      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 788000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.5     |\n",
      "|    ep_rew_mean     | -92.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 385      |\n",
      "|    time_elapsed    | 2321     |\n",
      "|    total_timesteps | 788480   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=789000, episode_reward=-89.22 +/- 12.58\n",
      "Episode length: 39.60 +/- 47.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 39.6       |\n",
      "|    mean_reward          | -89.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 789000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01095652 |\n",
      "|    clip_fraction        | 0.105      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | 0.175      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 38         |\n",
      "|    n_updates            | 3850       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 86.1       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=-95.81 +/- 4.21\n",
      "Episode length: 10.20 +/- 8.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | -95.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.5     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 386      |\n",
      "|    time_elapsed    | 2326     |\n",
      "|    total_timesteps | 790528   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=791000, episode_reward=-83.96 +/- 11.65\n",
      "Episode length: 56.60 +/- 49.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 56.6        |\n",
      "|    mean_reward          | -84         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 791000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012485549 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.0812      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.7        |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    value_loss           | 94.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=792000, episode_reward=-94.20 +/- 2.43\n",
      "Episode length: 19.40 +/- 11.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.4     |\n",
      "|    mean_reward     | -94.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 792000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 387      |\n",
      "|    time_elapsed    | 2331     |\n",
      "|    total_timesteps | 792576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=793000, episode_reward=-90.95 +/- 3.17\n",
      "Episode length: 26.40 +/- 6.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.4        |\n",
      "|    mean_reward          | -91         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 793000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010561714 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 75.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=794000, episode_reward=-91.27 +/- 6.06\n",
      "Episode length: 29.40 +/- 23.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 29.4     |\n",
      "|    mean_reward     | -91.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 794000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.1     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 388      |\n",
      "|    time_elapsed    | 2336     |\n",
      "|    total_timesteps | 794624   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=-94.61 +/- 7.01\n",
      "Episode length: 13.80 +/- 16.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 795000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012306346 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -0.117      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.3        |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 96          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=796000, episode_reward=-91.81 +/- 6.58\n",
      "Episode length: 19.40 +/- 15.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.4     |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 796000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 389      |\n",
      "|    time_elapsed    | 2342     |\n",
      "|    total_timesteps | 796672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=797000, episode_reward=-89.56 +/- 5.68\n",
      "Episode length: 24.20 +/- 13.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | -89.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 797000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010840686 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=798000, episode_reward=-91.32 +/- 10.23\n",
      "Episode length: 26.40 +/- 30.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.4     |\n",
      "|    mean_reward     | -91.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 798000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 390      |\n",
      "|    time_elapsed    | 2351     |\n",
      "|    total_timesteps | 798720   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=799000, episode_reward=-95.12 +/- 3.69\n",
      "Episode length: 14.80 +/- 11.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.8        |\n",
      "|    mean_reward          | -95.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 799000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013217995 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 83.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=-89.59 +/- 6.98\n",
      "Episode length: 25.80 +/- 16.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.8     |\n",
      "|    mean_reward     | -89.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.5     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 391      |\n",
      "|    time_elapsed    | 2359     |\n",
      "|    total_timesteps | 800768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=801000, episode_reward=-91.35 +/- 12.69\n",
      "Episode length: 26.40 +/- 38.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.4        |\n",
      "|    mean_reward          | -91.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 801000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008630623 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.9        |\n",
      "|    n_updates            | 3910        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 89          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=802000, episode_reward=-93.84 +/- 3.81\n",
      "Episode length: 15.40 +/- 9.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.4     |\n",
      "|    mean_reward     | -93.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 802000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26       |\n",
      "|    ep_rew_mean     | -91.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 392      |\n",
      "|    time_elapsed    | 2366     |\n",
      "|    total_timesteps | 802816   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=803000, episode_reward=-89.43 +/- 10.34\n",
      "Episode length: 35.60 +/- 32.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 35.6        |\n",
      "|    mean_reward          | -89.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 803000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011847909 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.3        |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 74.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=804000, episode_reward=-94.06 +/- 3.23\n",
      "Episode length: 20.20 +/- 11.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.2     |\n",
      "|    mean_reward     | -94.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 804000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.7     |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 393      |\n",
      "|    time_elapsed    | 2372     |\n",
      "|    total_timesteps | 804864   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=-96.71 +/- 1.97\n",
      "Episode length: 8.80 +/- 4.17\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 8.8        |\n",
      "|    mean_reward          | -96.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 805000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01008304 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.91      |\n",
      "|    explained_variance   | 0.148      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 29.1       |\n",
      "|    n_updates            | 3930       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    value_loss           | 76.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=806000, episode_reward=-92.33 +/- 7.65\n",
      "Episode length: 22.00 +/- 21.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22       |\n",
      "|    mean_reward     | -92.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 806000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 394      |\n",
      "|    time_elapsed    | 2376     |\n",
      "|    total_timesteps | 806912   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=807000, episode_reward=-94.28 +/- 4.01\n",
      "Episode length: 18.80 +/- 10.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.8        |\n",
      "|    mean_reward          | -94.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 807000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010462046 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 3940        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 94.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=808000, episode_reward=-89.14 +/- 8.82\n",
      "Episode length: 32.40 +/- 28.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 32.4     |\n",
      "|    mean_reward     | -89.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 808000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.1     |\n",
      "|    ep_rew_mean     | -92.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 395      |\n",
      "|    time_elapsed    | 2382     |\n",
      "|    total_timesteps | 808960   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=809000, episode_reward=-86.43 +/- 9.49\n",
      "Episode length: 44.60 +/- 32.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 44.6        |\n",
      "|    mean_reward          | -86.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 809000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013106343 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 3950        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 71.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=-91.84 +/- 5.97\n",
      "Episode length: 28.80 +/- 23.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.8     |\n",
      "|    mean_reward     | -91.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=811000, episode_reward=-94.52 +/- 2.73\n",
      "Episode length: 16.60 +/- 10.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 811000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.5     |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 2387     |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=812000, episode_reward=-96.25 +/- 2.71\n",
      "Episode length: 9.20 +/- 5.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | -96.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 812000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013225848 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.1        |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 74.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=813000, episode_reward=-90.82 +/- 6.32\n",
      "Episode length: 25.40 +/- 15.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 25.4     |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 813000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.5     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 397      |\n",
      "|    time_elapsed    | 2392     |\n",
      "|    total_timesteps | 813056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=814000, episode_reward=-95.75 +/- 2.71\n",
      "Episode length: 15.00 +/- 7.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | -95.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 814000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015948651 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.00462     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=815000, episode_reward=-98.28 +/- 0.93\n",
      "Episode length: 4.60 +/- 1.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 4.6      |\n",
      "|    mean_reward     | -98.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 815000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.3     |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 398      |\n",
      "|    time_elapsed    | 2397     |\n",
      "|    total_timesteps | 815104   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=816000, episode_reward=-91.68 +/- 5.37\n",
      "Episode length: 24.20 +/- 13.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | -91.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 816000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015253551 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.0289     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 3980        |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    value_loss           | 79.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=817000, episode_reward=-95.41 +/- 5.79\n",
      "Episode length: 11.20 +/- 12.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | -95.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 817000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.6     |\n",
      "|    ep_rew_mean     | -90.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 399      |\n",
      "|    time_elapsed    | 2402     |\n",
      "|    total_timesteps | 817152   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=818000, episode_reward=-90.49 +/- 6.68\n",
      "Episode length: 26.80 +/- 19.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.8        |\n",
      "|    mean_reward          | -90.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 818000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013052059 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 69.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=819000, episode_reward=-96.98 +/- 1.15\n",
      "Episode length: 8.00 +/- 2.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -97      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 819000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 2409     |\n",
      "|    total_timesteps | 819200   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=-96.08 +/- 5.39\n",
      "Episode length: 14.60 +/- 5.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.6        |\n",
      "|    mean_reward          | -96.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012056326 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.5        |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 96.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=821000, episode_reward=-97.12 +/- 3.15\n",
      "Episode length: 11.20 +/- 4.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | -97.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 821000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.9     |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 401      |\n",
      "|    time_elapsed    | 2416     |\n",
      "|    total_timesteps | 821248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=822000, episode_reward=-95.00 +/- 4.37\n",
      "Episode length: 15.20 +/- 13.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.2        |\n",
      "|    mean_reward          | -95         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 822000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012146712 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.969      |\n",
      "|    explained_variance   | 0.054       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.3        |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 82.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=823000, episode_reward=-95.91 +/- 1.93\n",
      "Episode length: 15.20 +/- 9.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.2     |\n",
      "|    mean_reward     | -95.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 823000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 402      |\n",
      "|    time_elapsed    | 2422     |\n",
      "|    total_timesteps | 823296   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=824000, episode_reward=-94.85 +/- 4.12\n",
      "Episode length: 15.00 +/- 11.97\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15         |\n",
      "|    mean_reward          | -94.9      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 824000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01191659 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.15      |\n",
      "|    explained_variance   | 0.0686     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 39.2       |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    value_loss           | 86.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=-94.94 +/- 4.38\n",
      "Episode length: 12.40 +/- 9.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.4     |\n",
      "|    mean_reward     | -94.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 825000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 403      |\n",
      "|    time_elapsed    | 2427     |\n",
      "|    total_timesteps | 825344   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=826000, episode_reward=-94.84 +/- 6.47\n",
      "Episode length: 14.60 +/- 13.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.6        |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 826000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016007835 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.9        |\n",
      "|    n_updates            | 4030        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 66.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=827000, episode_reward=-92.12 +/- 6.19\n",
      "Episode length: 22.40 +/- 17.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.4     |\n",
      "|    mean_reward     | -92.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 827000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28.8     |\n",
      "|    ep_rew_mean     | -91.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 404      |\n",
      "|    time_elapsed    | 2432     |\n",
      "|    total_timesteps | 827392   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=828000, episode_reward=-95.70 +/- 4.21\n",
      "Episode length: 13.00 +/- 8.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13          |\n",
      "|    mean_reward          | -95.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 828000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009248881 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 4040        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 73.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=829000, episode_reward=-92.11 +/- 6.46\n",
      "Episode length: 19.20 +/- 13.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.2     |\n",
      "|    mean_reward     | -92.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 829000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 405      |\n",
      "|    time_elapsed    | 2437     |\n",
      "|    total_timesteps | 829440   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=-93.57 +/- 4.42\n",
      "Episode length: 25.80 +/- 22.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 25.8        |\n",
      "|    mean_reward          | -93.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 830000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012028055 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.7        |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 87.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=831000, episode_reward=-91.70 +/- 5.14\n",
      "Episode length: 21.80 +/- 11.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.8     |\n",
      "|    mean_reward     | -91.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 831000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.5     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 406      |\n",
      "|    time_elapsed    | 2442     |\n",
      "|    total_timesteps | 831488   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=832000, episode_reward=-91.45 +/- 6.50\n",
      "Episode length: 26.20 +/- 21.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.2        |\n",
      "|    mean_reward          | -91.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 832000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010620076 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.8        |\n",
      "|    n_updates            | 4060        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 87.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=833000, episode_reward=-96.72 +/- 3.26\n",
      "Episode length: 10.00 +/- 6.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -96.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 833000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 407      |\n",
      "|    time_elapsed    | 2448     |\n",
      "|    total_timesteps | 833536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=834000, episode_reward=-90.15 +/- 7.43\n",
      "Episode length: 30.80 +/- 26.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 30.8        |\n",
      "|    mean_reward          | -90.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 834000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012334282 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | -0.114      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.7        |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 67.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=-94.97 +/- 7.16\n",
      "Episode length: 20.40 +/- 20.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.4     |\n",
      "|    mean_reward     | -95      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 835000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.5     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 408      |\n",
      "|    time_elapsed    | 2454     |\n",
      "|    total_timesteps | 835584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=836000, episode_reward=-95.40 +/- 4.18\n",
      "Episode length: 12.80 +/- 12.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.8        |\n",
      "|    mean_reward          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 836000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013315767 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | -0.0727     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 4080        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 76          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=837000, episode_reward=-96.70 +/- 1.94\n",
      "Episode length: 14.20 +/- 14.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | -96.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 837000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.8     |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 409      |\n",
      "|    time_elapsed    | 2462     |\n",
      "|    total_timesteps | 837632   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=838000, episode_reward=-95.02 +/- 3.43\n",
      "Episode length: 13.60 +/- 9.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 13.6         |\n",
      "|    mean_reward          | -95          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 838000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0152210705 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.0749       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.8         |\n",
      "|    n_updates            | 4090         |\n",
      "|    policy_gradient_loss | -0.0194      |\n",
      "|    value_loss           | 67.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=839000, episode_reward=-91.54 +/- 10.11\n",
      "Episode length: 26.60 +/- 34.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.6     |\n",
      "|    mean_reward     | -91.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 839000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.6     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 410      |\n",
      "|    time_elapsed    | 2469     |\n",
      "|    total_timesteps | 839680   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=-94.72 +/- 4.42\n",
      "Episode length: 16.60 +/- 16.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 16.6         |\n",
      "|    mean_reward          | -94.7        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 840000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0115266675 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | -0.00215     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 31.6         |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -0.021       |\n",
      "|    value_loss           | 76.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=841000, episode_reward=-91.20 +/- 8.56\n",
      "Episode length: 32.40 +/- 26.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 32.4     |\n",
      "|    mean_reward     | -91.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 841000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.4     |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 2474     |\n",
      "|    total_timesteps | 841728   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=842000, episode_reward=-94.03 +/- 3.31\n",
      "Episode length: 14.80 +/- 6.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 14.8         |\n",
      "|    mean_reward          | -94          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 842000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104910415 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0907       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.8         |\n",
      "|    n_updates            | 4110         |\n",
      "|    policy_gradient_loss | -0.018       |\n",
      "|    value_loss           | 77.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=843000, episode_reward=-96.34 +/- 2.34\n",
      "Episode length: 9.40 +/- 5.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 843000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.1     |\n",
      "|    ep_rew_mean     | -91.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 412      |\n",
      "|    time_elapsed    | 2479     |\n",
      "|    total_timesteps | 843776   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=844000, episode_reward=-90.29 +/- 3.31\n",
      "Episode length: 27.00 +/- 9.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27          |\n",
      "|    mean_reward          | -90.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 844000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010363048 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.7        |\n",
      "|    n_updates            | 4120        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 77.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=-95.65 +/- 2.65\n",
      "Episode length: 9.80 +/- 4.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | -95.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 845000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.6     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 413      |\n",
      "|    time_elapsed    | 2484     |\n",
      "|    total_timesteps | 845824   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=846000, episode_reward=-93.22 +/- 5.69\n",
      "Episode length: 22.00 +/- 24.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22          |\n",
      "|    mean_reward          | -93.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 846000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010443304 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.4        |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 88.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=847000, episode_reward=-95.74 +/- 3.39\n",
      "Episode length: 10.60 +/- 6.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 847000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21       |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 414      |\n",
      "|    time_elapsed    | 2489     |\n",
      "|    total_timesteps | 847872   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=848000, episode_reward=-93.95 +/- 7.36\n",
      "Episode length: 17.00 +/- 21.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 17          |\n",
      "|    mean_reward          | -94         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 848000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013503853 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | -0.263      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40          |\n",
      "|    n_updates            | 4140        |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 87.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=849000, episode_reward=-92.87 +/- 7.90\n",
      "Episode length: 20.60 +/- 23.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.6     |\n",
      "|    mean_reward     | -92.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 849000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 415      |\n",
      "|    time_elapsed    | 2494     |\n",
      "|    total_timesteps | 849920   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=-94.31 +/- 4.70\n",
      "Episode length: 17.00 +/- 8.97\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17         |\n",
      "|    mean_reward          | -94.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 850000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01349926 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.11       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 37.8       |\n",
      "|    n_updates            | 4150       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 94.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=851000, episode_reward=-94.47 +/- 3.52\n",
      "Episode length: 18.20 +/- 11.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.2     |\n",
      "|    mean_reward     | -94.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 851000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.5     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 416      |\n",
      "|    time_elapsed    | 2499     |\n",
      "|    total_timesteps | 851968   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=852000, episode_reward=-93.78 +/- 7.48\n",
      "Episode length: 24.60 +/- 34.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.6        |\n",
      "|    mean_reward          | -93.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 852000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012904799 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.0108      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 82.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=853000, episode_reward=-94.09 +/- 6.30\n",
      "Episode length: 14.20 +/- 12.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | -94.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 853000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=854000, episode_reward=-97.75 +/- 3.12\n",
      "Episode length: 8.00 +/- 4.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8        |\n",
      "|    mean_reward     | -97.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 854000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.5     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 417      |\n",
      "|    time_elapsed    | 2504     |\n",
      "|    total_timesteps | 854016   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=-98.18 +/- 1.23\n",
      "Episode length: 9.00 +/- 7.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -98.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 855000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011876946 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0846      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 94.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=856000, episode_reward=-95.15 +/- 3.64\n",
      "Episode length: 13.00 +/- 9.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13       |\n",
      "|    mean_reward     | -95.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 856000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 2511     |\n",
      "|    total_timesteps | 856064   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=857000, episode_reward=-93.95 +/- 5.03\n",
      "Episode length: 21.20 +/- 8.75\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 21.2       |\n",
      "|    mean_reward          | -94        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 857000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01308991 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.0663     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 34.4       |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 86.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=858000, episode_reward=-94.06 +/- 4.58\n",
      "Episode length: 13.80 +/- 10.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | -94.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 858000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.4     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 419      |\n",
      "|    time_elapsed    | 2519     |\n",
      "|    total_timesteps | 858112   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=859000, episode_reward=-96.99 +/- 1.94\n",
      "Episode length: 9.00 +/- 4.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -97         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 859000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011762818 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.1        |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 83.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=860000, episode_reward=-95.74 +/- 3.23\n",
      "Episode length: 12.20 +/- 10.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.9     |\n",
      "|    ep_rew_mean     | -94.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 2525     |\n",
      "|    total_timesteps | 860160   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=861000, episode_reward=-88.27 +/- 7.61\n",
      "Episode length: 31.60 +/- 18.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 31.6        |\n",
      "|    mean_reward          | -88.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 861000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012869557 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.8        |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 82.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=862000, episode_reward=-90.80 +/- 9.57\n",
      "Episode length: 41.80 +/- 49.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 41.8     |\n",
      "|    mean_reward     | -90.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 862000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 421      |\n",
      "|    time_elapsed    | 2530     |\n",
      "|    total_timesteps | 862208   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=863000, episode_reward=-86.91 +/- 11.46\n",
      "Episode length: 38.40 +/- 39.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 38.4        |\n",
      "|    mean_reward          | -86.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 863000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009671366 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.2        |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 96.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=864000, episode_reward=-96.25 +/- 5.04\n",
      "Episode length: 14.80 +/- 20.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.8     |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 864000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.7     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 2536     |\n",
      "|    total_timesteps | 864256   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=-90.95 +/- 7.66\n",
      "Episode length: 24.40 +/- 20.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.4        |\n",
      "|    mean_reward          | -90.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 865000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009795733 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.6        |\n",
      "|    n_updates            | 4220        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 87          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=866000, episode_reward=-85.36 +/- 17.09\n",
      "Episode length: 43.20 +/- 51.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 43.2     |\n",
      "|    mean_reward     | -85.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 866000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 423      |\n",
      "|    time_elapsed    | 2541     |\n",
      "|    total_timesteps | 866304   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=867000, episode_reward=-90.94 +/- 7.89\n",
      "Episode length: 24.20 +/- 22.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | -90.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 867000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010556019 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.3        |\n",
      "|    n_updates            | 4230        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    value_loss           | 91.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=868000, episode_reward=-93.76 +/- 6.62\n",
      "Episode length: 16.60 +/- 18.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | -93.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 868000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 2548     |\n",
      "|    total_timesteps | 868352   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=869000, episode_reward=-95.16 +/- 3.90\n",
      "Episode length: 12.60 +/- 8.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.6        |\n",
      "|    mean_reward          | -95.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 869000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012790849 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.4        |\n",
      "|    n_updates            | 4240        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 71.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=-94.00 +/- 10.24\n",
      "Episode length: 20.00 +/- 34.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20       |\n",
      "|    mean_reward     | -94      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.8     |\n",
      "|    ep_rew_mean     | -92      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 425      |\n",
      "|    time_elapsed    | 2555     |\n",
      "|    total_timesteps | 870400   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=871000, episode_reward=-92.75 +/- 4.20\n",
      "Episode length: 22.40 +/- 14.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 22.4         |\n",
      "|    mean_reward          | -92.8        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 871000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118022915 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.992       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.4         |\n",
      "|    n_updates            | 4250         |\n",
      "|    policy_gradient_loss | -0.0206      |\n",
      "|    value_loss           | 71.4         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=872000, episode_reward=-84.92 +/- 16.16\n",
      "Episode length: 54.00 +/- 62.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 54       |\n",
      "|    mean_reward     | -84.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 872000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.8     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 2564     |\n",
      "|    total_timesteps | 872448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=873000, episode_reward=-96.48 +/- 3.61\n",
      "Episode length: 11.60 +/- 8.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 11.6        |\n",
      "|    mean_reward          | -96.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 873000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014665732 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.00347     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 4260        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 75.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=874000, episode_reward=-88.35 +/- 13.98\n",
      "Episode length: 30.20 +/- 32.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 30.2     |\n",
      "|    mean_reward     | -88.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 874000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 427      |\n",
      "|    time_elapsed    | 2570     |\n",
      "|    total_timesteps | 874496   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=-87.18 +/- 11.96\n",
      "Episode length: 53.20 +/- 37.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 53.2        |\n",
      "|    mean_reward          | -87.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 875000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013085203 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.7        |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 85.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=876000, episode_reward=-93.27 +/- 4.93\n",
      "Episode length: 16.20 +/- 11.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.2     |\n",
      "|    mean_reward     | -93.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 876000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 2575     |\n",
      "|    total_timesteps | 876544   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=877000, episode_reward=-94.74 +/- 5.33\n",
      "Episode length: 16.80 +/- 16.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 16.8        |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 877000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012168201 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.01        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.7        |\n",
      "|    n_updates            | 4280        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 81.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=878000, episode_reward=-93.52 +/- 2.50\n",
      "Episode length: 19.80 +/- 9.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.8     |\n",
      "|    mean_reward     | -93.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 878000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 429      |\n",
      "|    time_elapsed    | 2580     |\n",
      "|    total_timesteps | 878592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=879000, episode_reward=-94.81 +/- 3.89\n",
      "Episode length: 14.20 +/- 11.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.2        |\n",
      "|    mean_reward          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 879000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012333946 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | -0.0147     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.4        |\n",
      "|    n_updates            | 4290        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 79.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=-91.94 +/- 3.02\n",
      "Episode length: 20.80 +/- 7.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 20.8     |\n",
      "|    mean_reward     | -91.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.1     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 2585     |\n",
      "|    total_timesteps | 880640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=881000, episode_reward=-97.10 +/- 2.45\n",
      "Episode length: 7.80 +/- 5.84\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.8        |\n",
      "|    mean_reward          | -97.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 881000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01237886 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.0343     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 39.3       |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    value_loss           | 84.6       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=882000, episode_reward=-95.26 +/- 1.56\n",
      "Episode length: 13.40 +/- 6.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.4     |\n",
      "|    mean_reward     | -95.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 882000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.6     |\n",
      "|    ep_rew_mean     | -91.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 431      |\n",
      "|    time_elapsed    | 2590     |\n",
      "|    total_timesteps | 882688   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=883000, episode_reward=-93.55 +/- 6.29\n",
      "Episode length: 17.40 +/- 18.29\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 17.4       |\n",
      "|    mean_reward          | -93.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 883000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01011036 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.05      |\n",
      "|    explained_variance   | 0.207      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 38.3       |\n",
      "|    n_updates            | 4310       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 80.2       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=884000, episode_reward=-93.66 +/- 4.84\n",
      "Episode length: 14.80 +/- 10.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.8     |\n",
      "|    mean_reward     | -93.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 884000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.6     |\n",
      "|    ep_rew_mean     | -92.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 2595     |\n",
      "|    total_timesteps | 884736   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=-92.32 +/- 5.00\n",
      "Episode length: 22.80 +/- 20.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -92.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 885000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011730493 |\n",
      "|    clip_fraction        | 0.0984      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31          |\n",
      "|    n_updates            | 4320        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    value_loss           | 81.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=886000, episode_reward=-95.30 +/- 1.71\n",
      "Episode length: 14.60 +/- 4.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.6     |\n",
      "|    mean_reward     | -95.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 886000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | -92.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 433      |\n",
      "|    time_elapsed    | 2601     |\n",
      "|    total_timesteps | 886784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=887000, episode_reward=-96.93 +/- 3.09\n",
      "Episode length: 10.40 +/- 10.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 10.4        |\n",
      "|    mean_reward          | -96.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 887000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009615384 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27          |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 82.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=888000, episode_reward=-96.54 +/- 3.89\n",
      "Episode length: 10.60 +/- 11.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.6     |\n",
      "|    mean_reward     | -96.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 888000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.3     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 2608     |\n",
      "|    total_timesteps | 888832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=889000, episode_reward=-98.99 +/- 0.77\n",
      "Episode length: 9.00 +/- 4.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9           |\n",
      "|    mean_reward          | -99         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 889000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010672602 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | 0.0786      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.6        |\n",
      "|    n_updates            | 4340        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 98.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=-90.21 +/- 6.22\n",
      "Episode length: 23.00 +/- 14.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23       |\n",
      "|    mean_reward     | -90.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 435      |\n",
      "|    time_elapsed    | 2615     |\n",
      "|    total_timesteps | 890880   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=891000, episode_reward=-95.41 +/- 4.23\n",
      "Episode length: 9.80 +/- 8.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 891000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012422238 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.1        |\n",
      "|    n_updates            | 4350        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 91.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=892000, episode_reward=-97.22 +/- 2.63\n",
      "Episode length: 10.40 +/- 11.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.4     |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 892000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28       |\n",
      "|    ep_rew_mean     | -90.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 2619     |\n",
      "|    total_timesteps | 892928   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=893000, episode_reward=-90.04 +/- 8.52\n",
      "Episode length: 29.80 +/- 26.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.8        |\n",
      "|    mean_reward          | -90         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 893000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012970768 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.0381      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 4360        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    value_loss           | 81.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=894000, episode_reward=-89.95 +/- 11.61\n",
      "Episode length: 35.80 +/- 48.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 35.8     |\n",
      "|    mean_reward     | -90      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 894000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.6     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 437      |\n",
      "|    time_elapsed    | 2625     |\n",
      "|    total_timesteps | 894976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=-96.12 +/- 2.59\n",
      "Episode length: 9.20 +/- 5.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.2         |\n",
      "|    mean_reward          | -96.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 895000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014956252 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.0891      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 4370        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 88          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=896000, episode_reward=-97.60 +/- 2.56\n",
      "Episode length: 9.80 +/- 12.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | -97.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 896000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=897000, episode_reward=-83.13 +/- 22.53\n",
      "Episode length: 76.60 +/- 112.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 76.6     |\n",
      "|    mean_reward     | -83.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 897000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.7     |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 2630     |\n",
      "|    total_timesteps | 897024   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=898000, episode_reward=-91.63 +/- 5.83\n",
      "Episode length: 22.80 +/- 17.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | -91.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 898000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013892178 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.7        |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 76.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=899000, episode_reward=-89.58 +/- 8.61\n",
      "Episode length: 39.40 +/- 28.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 39.4     |\n",
      "|    mean_reward     | -89.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 899000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.7     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 439      |\n",
      "|    time_elapsed    | 2636     |\n",
      "|    total_timesteps | 899072   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-95.36 +/- 3.43\n",
      "Episode length: 15.80 +/- 9.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.8        |\n",
      "|    mean_reward          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 900000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010256904 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.2        |\n",
      "|    n_updates            | 4390        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 87          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=901000, episode_reward=-98.85 +/- 1.63\n",
      "Episode length: 5.20 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5.2      |\n",
      "|    mean_reward     | -98.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 901000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20       |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 2641     |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=902000, episode_reward=-97.17 +/- 2.09\n",
      "Episode length: 12.60 +/- 8.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.6        |\n",
      "|    mean_reward          | -97.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 902000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013530051 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.4        |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 76.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=903000, episode_reward=-95.53 +/- 2.52\n",
      "Episode length: 21.60 +/- 15.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.6     |\n",
      "|    mean_reward     | -95.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 903000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 441      |\n",
      "|    time_elapsed    | 2646     |\n",
      "|    total_timesteps | 903168   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=904000, episode_reward=-90.94 +/- 6.06\n",
      "Episode length: 26.40 +/- 20.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 26.4         |\n",
      "|    mean_reward          | -90.9        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 904000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131687205 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.253        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.3         |\n",
      "|    n_updates            | 4410         |\n",
      "|    policy_gradient_loss | -0.0211      |\n",
      "|    value_loss           | 94.6         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=905000, episode_reward=-96.41 +/- 3.82\n",
      "Episode length: 11.60 +/- 13.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | -96.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 905000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -92.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 2653     |\n",
      "|    total_timesteps | 905216   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=906000, episode_reward=-97.21 +/- 2.79\n",
      "Episode length: 12.20 +/- 8.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.2        |\n",
      "|    mean_reward          | -97.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 906000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009962605 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 4420        |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    value_loss           | 94.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=907000, episode_reward=-86.80 +/- 7.13\n",
      "Episode length: 36.60 +/- 20.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 36.6     |\n",
      "|    mean_reward     | -86.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 907000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.7     |\n",
      "|    ep_rew_mean     | -91.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 443      |\n",
      "|    time_elapsed    | 2663     |\n",
      "|    total_timesteps | 907264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=908000, episode_reward=-96.93 +/- 2.16\n",
      "Episode length: 9.80 +/- 7.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.8         |\n",
      "|    mean_reward          | -96.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 908000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012175619 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 67.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=909000, episode_reward=-96.64 +/- 1.99\n",
      "Episode length: 10.00 +/- 6.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -96.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 909000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 27.2     |\n",
      "|    ep_rew_mean     | -91.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 2671     |\n",
      "|    total_timesteps | 909312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-96.89 +/- 2.46\n",
      "Episode length: 8.60 +/- 5.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | -96.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 910000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010533793 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.6        |\n",
      "|    n_updates            | 4440        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 82.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=911000, episode_reward=-92.97 +/- 3.21\n",
      "Episode length: 23.00 +/- 14.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 23       |\n",
      "|    mean_reward     | -93      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 911000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.6     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 445      |\n",
      "|    time_elapsed    | 2676     |\n",
      "|    total_timesteps | 911360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=912000, episode_reward=-92.86 +/- 7.06\n",
      "Episode length: 21.60 +/- 21.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.6        |\n",
      "|    mean_reward          | -92.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 912000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012478076 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.00445     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.5        |\n",
      "|    n_updates            | 4450        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 87.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=913000, episode_reward=-98.17 +/- 2.97\n",
      "Episode length: 6.40 +/- 4.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 6.4      |\n",
      "|    mean_reward     | -98.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 913000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.3     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 2681     |\n",
      "|    total_timesteps | 913408   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=914000, episode_reward=-91.76 +/- 3.41\n",
      "Episode length: 21.20 +/- 8.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21.2        |\n",
      "|    mean_reward          | -91.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 914000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012152418 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | -0.05       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.7        |\n",
      "|    n_updates            | 4460        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 83.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=-95.16 +/- 3.82\n",
      "Episode length: 12.80 +/- 9.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.8     |\n",
      "|    mean_reward     | -95.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 915000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 447      |\n",
      "|    time_elapsed    | 2686     |\n",
      "|    total_timesteps | 915456   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=916000, episode_reward=-83.85 +/- 15.23\n",
      "Episode length: 46.00 +/- 43.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 46          |\n",
      "|    mean_reward          | -83.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 916000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014415618 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.8        |\n",
      "|    n_updates            | 4470        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 66.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=917000, episode_reward=-87.69 +/- 9.99\n",
      "Episode length: 38.40 +/- 25.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 38.4     |\n",
      "|    mean_reward     | -87.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 917000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21       |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 2691     |\n",
      "|    total_timesteps | 917504   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=918000, episode_reward=-94.70 +/- 4.88\n",
      "Episode length: 18.40 +/- 14.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.4        |\n",
      "|    mean_reward          | -94.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 918000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011721516 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30          |\n",
      "|    n_updates            | 4480        |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 73.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=919000, episode_reward=-94.44 +/- 3.36\n",
      "Episode length: 13.80 +/- 7.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 13.8     |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 919000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.3     |\n",
      "|    ep_rew_mean     | -91.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 449      |\n",
      "|    time_elapsed    | 2697     |\n",
      "|    total_timesteps | 919552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=-98.05 +/- 3.71\n",
      "Episode length: 7.40 +/- 7.61\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 7.4        |\n",
      "|    mean_reward          | -98.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 920000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01220669 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.04      |\n",
      "|    explained_variance   | 0.0938     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 33.8       |\n",
      "|    n_updates            | 4490       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    value_loss           | 74.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=921000, episode_reward=-91.89 +/- 4.01\n",
      "Episode length: 18.80 +/- 8.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | -91.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 921000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 341      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 2702     |\n",
      "|    total_timesteps | 921600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=922000, episode_reward=-87.94 +/- 10.32\n",
      "Episode length: 41.60 +/- 42.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 41.6        |\n",
      "|    mean_reward          | -87.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 922000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011883013 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.5        |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 76.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=923000, episode_reward=-96.13 +/- 2.05\n",
      "Episode length: 9.00 +/- 4.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -96.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 923000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.5     |\n",
      "|    ep_rew_mean     | -93      |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 451      |\n",
      "|    time_elapsed    | 2710     |\n",
      "|    total_timesteps | 923648   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=924000, episode_reward=-91.82 +/- 5.13\n",
      "Episode length: 19.80 +/- 13.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19.8        |\n",
      "|    mean_reward          | -91.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 924000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012690991 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.6        |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 94.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=-88.38 +/- 5.84\n",
      "Episode length: 33.20 +/- 21.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.2     |\n",
      "|    mean_reward     | -88.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 925000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 2718     |\n",
      "|    total_timesteps | 925696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=926000, episode_reward=-92.65 +/- 3.73\n",
      "Episode length: 18.80 +/- 10.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.8        |\n",
      "|    mean_reward          | -92.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 926000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011034248 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.0992      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36          |\n",
      "|    n_updates            | 4520        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    value_loss           | 88.5        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=927000, episode_reward=-91.40 +/- 7.29\n",
      "Episode length: 22.40 +/- 19.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.4     |\n",
      "|    mean_reward     | -91.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 927000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -93.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 453      |\n",
      "|    time_elapsed    | 2725     |\n",
      "|    total_timesteps | 927744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=928000, episode_reward=-92.47 +/- 3.34\n",
      "Episode length: 20.40 +/- 9.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -92.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 928000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013980352 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.0875      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 4530        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 77.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=929000, episode_reward=-92.24 +/- 4.31\n",
      "Episode length: 22.60 +/- 15.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22.6     |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 929000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.1     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 2732     |\n",
      "|    total_timesteps | 929792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=-89.29 +/- 12.77\n",
      "Episode length: 32.40 +/- 38.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 32.4        |\n",
      "|    mean_reward          | -89.3       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 930000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014144353 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 75.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=931000, episode_reward=-91.46 +/- 8.29\n",
      "Episode length: 24.80 +/- 25.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.8     |\n",
      "|    mean_reward     | -91.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 931000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 455      |\n",
      "|    time_elapsed    | 2737     |\n",
      "|    total_timesteps | 931840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=932000, episode_reward=-90.56 +/- 9.53\n",
      "Episode length: 30.00 +/- 28.99\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 30         |\n",
      "|    mean_reward          | -90.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 932000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01311542 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.16      |\n",
      "|    explained_variance   | 0.0515     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 40.9       |\n",
      "|    n_updates            | 4550       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    value_loss           | 84.5       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=933000, episode_reward=-98.14 +/- 1.29\n",
      "Episode length: 7.60 +/- 4.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.6      |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 933000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 2741     |\n",
      "|    total_timesteps | 933888   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=934000, episode_reward=-96.48 +/- 3.21\n",
      "Episode length: 11.20 +/- 9.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 11.2       |\n",
      "|    mean_reward          | -96.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 934000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01142734 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.16      |\n",
      "|    explained_variance   | 0.128      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 33.3       |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    value_loss           | 90.4       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=-96.89 +/- 2.98\n",
      "Episode length: 7.80 +/- 6.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7.8      |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 935000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.4     |\n",
      "|    ep_rew_mean     | -93.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 457      |\n",
      "|    time_elapsed    | 2746     |\n",
      "|    total_timesteps | 935936   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=936000, episode_reward=-93.20 +/- 3.31\n",
      "Episode length: 19.60 +/- 10.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 19.6       |\n",
      "|    mean_reward          | -93.2      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 936000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01144604 |\n",
      "|    clip_fraction        | 0.0998     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.11      |\n",
      "|    explained_variance   | 0.0426     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 37         |\n",
      "|    n_updates            | 4570       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    value_loss           | 89.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=937000, episode_reward=-96.63 +/- 2.88\n",
      "Episode length: 8.80 +/- 6.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | -96.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 937000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.3     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 2752     |\n",
      "|    total_timesteps | 937984   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=938000, episode_reward=-92.80 +/- 7.84\n",
      "Episode length: 18.20 +/- 21.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | -92.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 938000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012175077 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -0.0518     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.2        |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 69          |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=939000, episode_reward=-94.61 +/- 2.51\n",
      "Episode length: 16.60 +/- 8.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 16.6     |\n",
      "|    mean_reward     | -94.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 939000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=-88.78 +/- 13.97\n",
      "Episode length: 36.40 +/- 50.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 36.4     |\n",
      "|    mean_reward     | -88.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 2760     |\n",
      "|    total_timesteps | 940032   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=941000, episode_reward=-86.20 +/- 7.38\n",
      "Episode length: 38.60 +/- 21.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 38.6        |\n",
      "|    mean_reward          | -86.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 941000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014411714 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.00756     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.9        |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 70          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=942000, episode_reward=-89.55 +/- 6.52\n",
      "Episode length: 24.80 +/- 15.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 24.8     |\n",
      "|    mean_reward     | -89.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 942000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.8     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 460      |\n",
      "|    time_elapsed    | 2768     |\n",
      "|    total_timesteps | 942080   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=943000, episode_reward=-91.45 +/- 5.01\n",
      "Episode length: 21.00 +/- 12.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 21          |\n",
      "|    mean_reward          | -91.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 943000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012005409 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.9        |\n",
      "|    n_updates            | 4600        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 82          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=944000, episode_reward=-92.78 +/- 6.97\n",
      "Episode length: 18.60 +/- 13.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.6     |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 944000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 461      |\n",
      "|    time_elapsed    | 2773     |\n",
      "|    total_timesteps | 944128   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=-90.03 +/- 8.51\n",
      "Episode length: 27.60 +/- 22.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 27.6         |\n",
      "|    mean_reward          | -90          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 945000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0108099915 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.0871       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 51.2         |\n",
      "|    n_updates            | 4610         |\n",
      "|    policy_gradient_loss | -0.0198      |\n",
      "|    value_loss           | 90.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=946000, episode_reward=-92.82 +/- 6.42\n",
      "Episode length: 18.40 +/- 18.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | -92.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 946000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.3     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 462      |\n",
      "|    time_elapsed    | 2779     |\n",
      "|    total_timesteps | 946176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=947000, episode_reward=-84.97 +/- 13.98\n",
      "Episode length: 45.80 +/- 40.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 45.8        |\n",
      "|    mean_reward          | -85         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 947000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013397591 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.1        |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 79.7        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=948000, episode_reward=-94.72 +/- 5.17\n",
      "Episode length: 15.80 +/- 15.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.8     |\n",
      "|    mean_reward     | -94.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 948000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | -93.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 463      |\n",
      "|    time_elapsed    | 2784     |\n",
      "|    total_timesteps | 948224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=949000, episode_reward=-91.46 +/- 6.94\n",
      "Episode length: 24.20 +/- 24.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | -91.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 949000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013676863 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.0511      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.1        |\n",
      "|    n_updates            | 4630        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    value_loss           | 79.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=-95.70 +/- 5.40\n",
      "Episode length: 12.20 +/- 9.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 12.2     |\n",
      "|    mean_reward     | -95.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.5     |\n",
      "|    ep_rew_mean     | -91.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 464      |\n",
      "|    time_elapsed    | 2789     |\n",
      "|    total_timesteps | 950272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=951000, episode_reward=-91.96 +/- 5.70\n",
      "Episode length: 23.40 +/- 20.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.4        |\n",
      "|    mean_reward          | -92         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 951000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015197333 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.0202      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.1        |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 60.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=952000, episode_reward=-94.94 +/- 5.37\n",
      "Episode length: 17.00 +/- 15.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 17       |\n",
      "|    mean_reward     | -94.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 952000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.2     |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 465      |\n",
      "|    time_elapsed    | 2795     |\n",
      "|    total_timesteps | 952320   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=953000, episode_reward=-93.39 +/- 5.38\n",
      "Episode length: 19.60 +/- 17.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19.6        |\n",
      "|    mean_reward          | -93.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 953000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012633105 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43          |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 79.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=954000, episode_reward=-86.11 +/- 11.25\n",
      "Episode length: 40.00 +/- 33.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 40       |\n",
      "|    mean_reward     | -86.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 954000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.8     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 466      |\n",
      "|    time_elapsed    | 2803     |\n",
      "|    total_timesteps | 954368   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=-92.64 +/- 4.31\n",
      "Episode length: 23.00 +/- 20.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23          |\n",
      "|    mean_reward          | -92.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 955000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015160108 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | -0.069      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.6        |\n",
      "|    n_updates            | 4660        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 72.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=956000, episode_reward=-96.35 +/- 3.80\n",
      "Episode length: 10.80 +/- 11.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.8     |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 956000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -93.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 467      |\n",
      "|    time_elapsed    | 2811     |\n",
      "|    total_timesteps | 956416   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=957000, episode_reward=-90.17 +/- 13.34\n",
      "Episode length: 29.00 +/- 37.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29          |\n",
      "|    mean_reward          | -90.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 957000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013900921 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.0701      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.7        |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 73.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=958000, episode_reward=-90.63 +/- 18.31\n",
      "Episode length: 49.60 +/- 66.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 49.6     |\n",
      "|    mean_reward     | -90.6    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 958000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.5     |\n",
      "|    ep_rew_mean     | -92.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 468      |\n",
      "|    time_elapsed    | 2817     |\n",
      "|    total_timesteps | 958464   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=959000, episode_reward=-91.44 +/- 5.87\n",
      "Episode length: 27.60 +/- 22.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 27.6        |\n",
      "|    mean_reward          | -91.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 959000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011527118 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.6        |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 79          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=-89.16 +/- 9.22\n",
      "Episode length: 33.20 +/- 30.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.2     |\n",
      "|    mean_reward     | -89.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21       |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 2823     |\n",
      "|    total_timesteps | 960512   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=961000, episode_reward=-94.63 +/- 4.66\n",
      "Episode length: 12.60 +/- 10.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12.6        |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 961000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010579208 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.9        |\n",
      "|    n_updates            | 4690        |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 88.7        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=962000, episode_reward=-97.76 +/- 1.16\n",
      "Episode length: 7.00 +/- 3.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -97.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 962000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.2     |\n",
      "|    ep_rew_mean     | -92.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 470      |\n",
      "|    time_elapsed    | 2827     |\n",
      "|    total_timesteps | 962560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=963000, episode_reward=-84.11 +/- 11.48\n",
      "Episode length: 49.40 +/- 42.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 49.4        |\n",
      "|    mean_reward          | -84.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 963000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012297545 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 38.8        |\n",
      "|    n_updates            | 4700        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 83.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=964000, episode_reward=-96.26 +/- 3.12\n",
      "Episode length: 14.60 +/- 18.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.6     |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 964000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.1     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 471      |\n",
      "|    time_elapsed    | 2833     |\n",
      "|    total_timesteps | 964608   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=-91.87 +/- 8.52\n",
      "Episode length: 26.00 +/- 27.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 26         |\n",
      "|    mean_reward          | -91.9      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 965000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01243354 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.21      |\n",
      "|    explained_variance   | 0.197      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 41.3       |\n",
      "|    n_updates            | 4710       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    value_loss           | 91.9       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=966000, episode_reward=-92.20 +/- 9.24\n",
      "Episode length: 19.00 +/- 21.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19       |\n",
      "|    mean_reward     | -92.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 966000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | -92.9    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 472      |\n",
      "|    time_elapsed    | 2838     |\n",
      "|    total_timesteps | 966656   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=967000, episode_reward=-92.04 +/- 8.41\n",
      "Episode length: 30.60 +/- 40.64\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 30.6     |\n",
      "|    mean_reward          | -92      |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 967000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.013065 |\n",
      "|    clip_fraction        | 0.133    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.14    |\n",
      "|    explained_variance   | 0.0247   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 27.2     |\n",
      "|    n_updates            | 4720     |\n",
      "|    policy_gradient_loss | -0.0218  |\n",
      "|    value_loss           | 77.9     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=968000, episode_reward=-90.40 +/- 5.00\n",
      "Episode length: 28.60 +/- 14.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.6     |\n",
      "|    mean_reward     | -90.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 968000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.5     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 473      |\n",
      "|    time_elapsed    | 2845     |\n",
      "|    total_timesteps | 968704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=969000, episode_reward=-88.40 +/- 12.39\n",
      "Episode length: 35.80 +/- 43.43\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 35.8       |\n",
      "|    mean_reward          | -88.4      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 969000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01152313 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.26      |\n",
      "|    explained_variance   | 0.125      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 45.7       |\n",
      "|    n_updates            | 4730       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    value_loss           | 91.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=-89.11 +/- 14.55\n",
      "Episode length: 27.00 +/- 34.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 27       |\n",
      "|    mean_reward     | -89.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.2     |\n",
      "|    ep_rew_mean     | -93.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 340      |\n",
      "|    iterations      | 474      |\n",
      "|    time_elapsed    | 2854     |\n",
      "|    total_timesteps | 970752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=971000, episode_reward=-96.37 +/- 2.30\n",
      "Episode length: 14.20 +/- 13.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.2        |\n",
      "|    mean_reward          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 971000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012121055 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 4740        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 83.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=972000, episode_reward=-85.70 +/- 15.70\n",
      "Episode length: 45.60 +/- 48.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 45.6     |\n",
      "|    mean_reward     | -85.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 972000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 26.1     |\n",
      "|    ep_rew_mean     | -92.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 475      |\n",
      "|    time_elapsed    | 2863     |\n",
      "|    total_timesteps | 972800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=973000, episode_reward=-90.94 +/- 7.69\n",
      "Episode length: 29.40 +/- 30.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 29.4        |\n",
      "|    mean_reward          | -90.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 973000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014591733 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | 0.0097      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.4        |\n",
      "|    n_updates            | 4750        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 73.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=974000, episode_reward=-92.12 +/- 5.33\n",
      "Episode length: 26.20 +/- 11.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 26.2     |\n",
      "|    mean_reward     | -92.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 974000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.7     |\n",
      "|    ep_rew_mean     | -92.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 476      |\n",
      "|    time_elapsed    | 2872     |\n",
      "|    total_timesteps | 974848   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=-94.91 +/- 3.96\n",
      "Episode length: 13.40 +/- 9.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.4        |\n",
      "|    mean_reward          | -94.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 975000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012485314 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.0503      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.9        |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    value_loss           | 82.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=976000, episode_reward=-83.95 +/- 9.63\n",
      "Episode length: 50.80 +/- 30.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 50.8     |\n",
      "|    mean_reward     | -84      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 976000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | -93.2    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 477      |\n",
      "|    time_elapsed    | 2879     |\n",
      "|    total_timesteps | 976896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=977000, episode_reward=-92.36 +/- 5.44\n",
      "Episode length: 24.20 +/- 18.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 24.2        |\n",
      "|    mean_reward          | -92.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 977000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013565484 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | -0.0847     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 4770        |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 76.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=978000, episode_reward=-87.67 +/- 6.11\n",
      "Episode length: 40.60 +/- 23.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 40.6     |\n",
      "|    mean_reward     | -87.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 978000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.1     |\n",
      "|    ep_rew_mean     | -93.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 478      |\n",
      "|    time_elapsed    | 2887     |\n",
      "|    total_timesteps | 978944   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=979000, episode_reward=-92.93 +/- 4.57\n",
      "Episode length: 18.00 +/- 9.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | -92.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 979000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013158678 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.5        |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 71.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-96.88 +/- 2.07\n",
      "Episode length: 9.60 +/- 6.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | -96.9    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.2     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 479      |\n",
      "|    time_elapsed    | 2894     |\n",
      "|    total_timesteps | 980992   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=981000, episode_reward=-96.01 +/- 3.67\n",
      "Episode length: 14.60 +/- 6.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 14.6        |\n",
      "|    mean_reward          | -96         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 981000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013695581 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | -0.166      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.3        |\n",
      "|    n_updates            | 4790        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 75.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=982000, episode_reward=-89.02 +/- 11.20\n",
      "Episode length: 63.80 +/- 66.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 63.8     |\n",
      "|    mean_reward     | -89      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 982000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=983000, episode_reward=-92.66 +/- 8.82\n",
      "Episode length: 28.20 +/- 34.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.2     |\n",
      "|    mean_reward     | -92.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 983000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.9     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 480      |\n",
      "|    time_elapsed    | 2904     |\n",
      "|    total_timesteps | 983040   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=984000, episode_reward=-94.46 +/- 2.17\n",
      "Episode length: 15.40 +/- 8.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15.4        |\n",
      "|    mean_reward          | -94.5       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 984000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012355908 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.8        |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 61.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=-91.19 +/- 11.39\n",
      "Episode length: 28.80 +/- 41.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.8     |\n",
      "|    mean_reward     | -91.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 985000   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -94.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 481      |\n",
      "|    time_elapsed    | 2913     |\n",
      "|    total_timesteps | 985088   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=986000, episode_reward=-96.12 +/- 3.65\n",
      "Episode length: 15.20 +/- 10.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 15.2       |\n",
      "|    mean_reward          | -96.1      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 986000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01221045 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | -0.0851    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 25.4       |\n",
      "|    n_updates            | 4810       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    value_loss           | 71.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=987000, episode_reward=-95.78 +/- 2.76\n",
      "Episode length: 11.40 +/- 2.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.4     |\n",
      "|    mean_reward     | -95.8    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 987000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -93.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 482      |\n",
      "|    time_elapsed    | 2923     |\n",
      "|    total_timesteps | 987136   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=988000, episode_reward=-95.87 +/- 6.34\n",
      "Episode length: 18.20 +/- 19.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18.2        |\n",
      "|    mean_reward          | -95.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 988000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013799482 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.0358      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 4820        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    value_loss           | 78.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=989000, episode_reward=-97.16 +/- 2.13\n",
      "Episode length: 7.00 +/- 4.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -97.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 989000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -94      |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 483      |\n",
      "|    time_elapsed    | 2930     |\n",
      "|    total_timesteps | 989184   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=-97.43 +/- 4.10\n",
      "Episode length: 13.80 +/- 12.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 13.8        |\n",
      "|    mean_reward          | -97.4       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 990000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015692849 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.3        |\n",
      "|    n_updates            | 4830        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 81.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=991000, episode_reward=-96.48 +/- 3.51\n",
      "Episode length: 14.00 +/- 11.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14       |\n",
      "|    mean_reward     | -96.5    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 991000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.5     |\n",
      "|    ep_rew_mean     | -94.1    |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 2936     |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=992000, episode_reward=-94.58 +/- 5.12\n",
      "Episode length: 20.40 +/- 25.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 20.4        |\n",
      "|    mean_reward          | -94.6       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 992000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011711741 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | -0.000603   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 4840        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    value_loss           | 71.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=993000, episode_reward=-98.08 +/- 3.97\n",
      "Episode length: 10.20 +/- 9.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | -98.1    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 993000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.4     |\n",
      "|    ep_rew_mean     | -92.8    |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 485      |\n",
      "|    time_elapsed    | 2941     |\n",
      "|    total_timesteps | 993280   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=994000, episode_reward=-97.15 +/- 1.52\n",
      "Episode length: 8.00 +/- 3.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | -97.2       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 994000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011519114 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.0757      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.1        |\n",
      "|    n_updates            | 4850        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 72.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=-96.31 +/- 2.91\n",
      "Episode length: 10.40 +/- 8.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.4     |\n",
      "|    mean_reward     | -96.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 995000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.4     |\n",
      "|    ep_rew_mean     | -94.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 337      |\n",
      "|    iterations      | 486      |\n",
      "|    time_elapsed    | 2945     |\n",
      "|    total_timesteps | 995328   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=996000, episode_reward=-91.81 +/- 6.76\n",
      "Episode length: 23.20 +/- 22.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 23.2        |\n",
      "|    mean_reward          | -91.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 996000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013225882 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    value_loss           | 71.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=997000, episode_reward=-94.41 +/- 2.10\n",
      "Episode length: 14.20 +/- 5.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 14.2     |\n",
      "|    mean_reward     | -94.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 997000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 487      |\n",
      "|    time_elapsed    | 2950     |\n",
      "|    total_timesteps | 997376   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=998000, episode_reward=-93.96 +/- 3.96\n",
      "Episode length: 18.00 +/- 11.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 18          |\n",
      "|    mean_reward          | -94         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 998000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011183888 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.00401     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 69.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=999000, episode_reward=-74.30 +/- 20.91\n",
      "Episode length: 120.00 +/- 89.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 120      |\n",
      "|    mean_reward     | -74.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 999000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | -93.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 488      |\n",
      "|    time_elapsed    | 2956     |\n",
      "|    total_timesteps | 999424   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-96.66 +/- 2.08\n",
      "Episode length: 8.60 +/- 4.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8.6         |\n",
      "|    mean_reward          | -96.7       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013735818 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.0791      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.4        |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 71.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1001000, episode_reward=-92.36 +/- 3.78\n",
      "Episode length: 22.00 +/- 9.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 22       |\n",
      "|    mean_reward     | -92.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1001000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 28       |\n",
      "|    ep_rew_mean     | -91.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 338      |\n",
      "|    iterations      | 489      |\n",
      "|    time_elapsed    | 2961     |\n",
      "|    total_timesteps | 1001472  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7c78143e3ff45273\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7c78143e3ff45273\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Train agent for 1M timesteps\n",
    "agent.learn(total_timesteps=1000000, n_eval_episodes=1, callback=eval_callback)\n",
    "%tensorboard --logdir ./logs/ppo_YT_initial_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd7df4ad-7ee7-46b2-871e-5550c5c334a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save trained agent\n",
    "agent_name = \"ppo-6PC\"\n",
    "agent.save(agent_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38314341-31dd-4012-8d23-dfab67eea1b1",
   "metadata": {},
   "source": [
    "## Agent evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c174d6-1c79-4b4c-bd57-4430c5509488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 17:32:31,069::WARNING::py.warnings::_showwarnmsg::109::C:\\Users\\olive\\yawning_titan\\.venv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-86.93 +/- 12.216967764534838\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a new environment for evaluation\n",
    "eval_env =GenericNetworkEnv(red, blue, network_interface)\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward = evaluate_policy(agent, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b2cdf8-f53c-42d4-9b80-fef20a53ebeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[   action    rewards                                               info\n",
       " 0      17   0.300000  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " 1      20   0.464600  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " 2      23   0.100000  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " 3      11   0.300000  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " 4       9   0.300000  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " ..    ...        ...                                                ...\n",
       " 66     14   0.397600  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " 67      0   0.000000  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " 68      0   0.000000  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " 69      0   0.436364  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " 70     14 -93.000000  {'initial_state': {'e8b8d6b4-9078-4fff-b039-b9...\n",
       " \n",
       " [71 rows x 3 columns]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loop = ActionLoop(eval_env, agent, episode_count=1)\n",
    "loop.gif_action_loop(save_gif=False, render_network=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
