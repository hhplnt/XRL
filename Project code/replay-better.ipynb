{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa23c94-7412-40c4-9b60-b22d5f852b7f",
   "metadata": {},
   "source": [
    "## First create network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f9dff2-fc35-43db-afb8-f824d6c96e52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\yawning_titan\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from yawning_titan.networks.node import Node\n",
    "from yawning_titan.networks.network import Network\n",
    "\n",
    "#Import packages - SB3\n",
    "import time\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOMlp\n",
    "\n",
    "from yawning_titan.envs.generic.core.blue_interface import BlueInterface\n",
    "from yawning_titan.envs.generic.core.red_interface import RedInterface\n",
    "from yawning_titan.envs.generic.generic_env import GenericNetworkEnv\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "from yawning_titan.envs.generic.core.network_interface import NetworkInterface\n",
    "from yawning_titan.networks.network_db import default_18_node_network\n",
    "import yawning_titan.game_modes\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89cb44c-2dba-40c1-9036-60f8ecd0aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UUID                                  Name    High Value Node    Entry Node      Vulnerability  Position (x,y)\n",
      "------------------------------------  ------  -----------------  ------------  ---------------  ----------------\n",
      "688308b1-8c7b-4fc0-9bef-f21af31eab8e  PC 1    False              True                      0.5  -1.00, 0.01\n",
      "54cf8097-6b21-4437-87c8-f7f01c539274  PC 2    False              False                     0.5  -0.50, 0.01\n",
      "37d7fb0d-5f6c-4abe-892f-49fbf75e37eb  PC 3    False              False                     0.5  0.00, 0.01\n",
      "3f4cc577-52b8-45c7-8159-3354ef0a4fee  PC 4    False              False                     0.5  0.50, 0.01\n",
      "27893026-be4c-4120-a82b-833e3eb4abed  PC 5    True               False                     0.5  1.00, 0.01\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Network\n",
    "network = Network()\n",
    "\n",
    "# Instantiate the Node's and add them to the Network\n",
    " \n",
    "pc_1 = Node(\"PC 1\")\n",
    "network.add_node(pc_1)\n",
    "pc_1.x_pos = -1.00\n",
    "pc_1.y_pos = 0.01\n",
    "pc_1.entry_node = True\n",
    "pc_1.vulnerability = 0.5\n",
    "\n",
    "pc_2 = Node(\"PC 2\")\n",
    "network.add_node(pc_2)\n",
    "pc_2.x_pos = -0.50\n",
    "pc_2.y_pos = 0.01\n",
    "pc_2.vulnerability = 0.5\n",
    "\n",
    "pc_3 = Node(\"PC 3\")\n",
    "network.add_node(pc_3)\n",
    "pc_3.x_pos = 0.00\n",
    "pc_3.y_pos = 0.01\n",
    "pc_3.vulnerability = 0.5\n",
    "\n",
    "pc_4 = Node(\"PC 4\")\n",
    "network.add_node(pc_4)\n",
    "pc_4.x_pos = 0.50\n",
    "pc_4.y_pos = 0.01\n",
    "pc_4.vulnerability = 0.5\n",
    "\n",
    "pc_5 = Node(\"PC 5\")\n",
    "network.add_node(pc_5)\n",
    "pc_5.x_pos = 1.00\n",
    "pc_5.y_pos = 0.01\n",
    "pc_5.high_value_node = True\n",
    "pc_5.vulnerability = 0.5\n",
    "\n",
    "\n",
    "# Add the edges between Node's\n",
    "network.add_edge(pc_1, pc_2)\n",
    "network.add_edge(pc_2, pc_3)\n",
    "network.add_edge(pc_3, pc_4)\n",
    "network.add_edge(pc_4, pc_5)\n",
    "\n",
    "\n",
    "\n",
    "# Reset the entry nodes, high value nodes, and vulnerability scores by calling .setup()\n",
    "# network.reset()\n",
    "\n",
    "# View the Networks Node Details\n",
    "network.show(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0781bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using DB\n",
    "from yawning_titan.game_modes.game_mode_db import GameModeDB, GameModeSchema\n",
    "from yawning_titan.db.doc_metadata import DocMetadataSchema\n",
    "db = GameModeDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70f8ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name               author              locked    uuid\n",
      "-----------------  ------------------  --------  ------------------------------------\n",
      "DCBO Agent Config  dstl/YAWNING-TITAN  True      bac2cb9d-b24b-426c-88a5-5edd0c2de413\n",
      "Default Game Mode  dstl/YAWNING-TITAN  True      900a704f-6271-4994-ade7-40b74d3199b1\n",
      "Low skill red      dstl/YAWNING-TITAN  True      3ccd9988-8781-4c3e-9c75-44cc987ae6af\n",
      "simple_mode        Hannah Harrison     False     919da33c-7bc9-4d29-99eb-097a7e9bb016\n",
      "no_zero_day        Hannah Harrison     False     fe76bb6c-4806-41af-aaf3-ac78d2942021\n"
     ]
    }
   ],
   "source": [
    "db.show(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b5bb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_mode = db.get(\"919da33c-7bc9-4d29-99eb-097a7e9bb016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66e1add-8ee8-4d54-81e2-71e1fa62cfa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Build network interface\n",
    "s_network_interface = NetworkInterface(game_mode=simple_mode, network=network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c014789-0765-48b4-831b-46860bec4fee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Name agents\n",
    "red = RedInterface(s_network_interface)\n",
    "blue = BlueInterface(s_network_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f43d14-9df2-4e17-857b-9b6bbbf1d140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create environment\n",
    "s_env = GenericNetworkEnv(red, blue, s_network_interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6fd1a8-512e-45ac-8697-4a060d912cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Check compliant with OpenAI gym\n",
    "check_env(s_env, warn=True)\n",
    "_ = s_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373754fa",
   "metadata": {},
   "source": [
    "## Creating a dataset of states and actions\n",
    "https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pretraining.ipynb#scrollTo=Tgx4AMZo8anP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c852bdf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(f\"{gym.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48ecb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "793e3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "177fd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "agent = PPO.load('./ppo-s-linear.zip', env= s_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b23aa9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward = -88.50666600763797 +/- 0.3711846905218861\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(agent, s_env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebcd1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_agent(env, agent, max_steps=30):\n",
    "    \"\"\"\n",
    "    Interact with a Stable Baselines 3 PPO agent in an environment.\n",
    "\n",
    "    Parameters:\n",
    "        env (gym.Env): The environment to interact with.\n",
    "        agent (BaseAlgorithm): The Stable Baselines 3 PPO agent.\n",
    "        max_steps (int): The maximum number of steps to interact with the agent.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: Each tuple contains (action, prev_state, new_state, reward)\n",
    "                        for each interaction step.\n",
    "    \"\"\"\n",
    "    interactions = []\n",
    "\n",
    "    # Reset the environment to get the initial state\n",
    "    state = env.reset()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Store the previous state\n",
    "        prev_state = np.array(state)\n",
    "\n",
    "        # Get the action from the agent\n",
    "        action, _ = agent.predict(state, deterministic=True)\n",
    "\n",
    "        # Perform the action in the environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Store the interaction tuple (action, previous state, current state, reward)\n",
    "        interactions.append((action, prev_state, np.array(new_state), reward, info))\n",
    "\n",
    "        # Update the state for the next step\n",
    "        state = new_state\n",
    "\n",
    "        # If the episode is done, break out of the loop\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c2f810c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prev State: [0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.\n",
      " 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.5 0.5 0.5 0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  1.  0.7], New State: [0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.\n",
      " 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.5 0.5 0.5 0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  1.  0.7], Reward: 0.3000000000000007\n",
      "Action: 0, Prev State: [0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.\n",
      " 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.5 0.5 0.5 0.3 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  1.  0.7], New State: [0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.\n",
      " 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.5 0.5 0.5 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  1.  0.7], Reward: 0.29999999999999893\n",
      "Action: 0, Prev State: [0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0.  1.\n",
      " 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
      " 0.5 0.5 0.5 0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.  0.  0.  0.  0.  1.  0.7], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -0.1399999999999988\n",
      "Action: 0, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -1.0\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   0.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   0.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   0.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -3.5\n",
      "Action: 1, Prev State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], New State: [0.   1.   0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   1.\n",
      " 0.   0.   0.   1.   0.   1.   0.   0.   0.   1.   0.   0.   0.   0.\n",
      " 0.   0.   1.   1.   1.   1.   0.   0.5  0.5  0.5  0.5  0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   1.   0.7 ], Reward: -50.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    interactions = interact_with_agent(s_env, agent)\n",
    "\n",
    "    for action, prev_state, new_state, reward in interactions:\n",
    "        print(f\"Action: {action}, Prev State: {prev_state}, New State: {new_state}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05baabce",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "Using network_interface.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebdd75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2f390e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "        complete_results = []\n",
    "        for i in range(100):\n",
    "            results = pd.DataFrame(\n",
    "                columns=[\"action\", \"rewards\", \"info\"]\n",
    "            )  \n",
    "            obs = s_env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                # gets the agents prediction for the best next action to take\n",
    "                action, _states = agent.predict(obs, deterministic= True)\n",
    "\n",
    "                # step the env\n",
    "                obs, rewards, done, info = s_env.step(action)\n",
    "\n",
    "                results.loc[len(results.index)] = [action, rewards, info]\n",
    "\n",
    "            complete_results.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7251c059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action</th>\n",
       "      <th>rewards</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.140000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>-53.333333</td>\n",
       "      <td>{'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action    rewards                                               info\n",
       "0       0   0.300000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "1       0   0.300000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "2       0  -0.140000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "3       0  -1.000000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "4       1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "5       1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "6       1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "7       1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "8       1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "9       1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "10      1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "11      1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "12      1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "13      1  -3.500000  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2...\n",
       "14      1 -53.333333  {'initial_state': {'688308b1-8c7b-4fc0-9bef-f2..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd3699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_state': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0},\n",
       " 'initial_blue_view': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0},\n",
       " 'initial_vulnerabilities': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0.5,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0.5,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0.5,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0.5,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0.09999999999999998},\n",
       " 'initial_red_location': None,\n",
       " 'initial_graph': {Node(uuid='688308b1-8c7b-4fc0-9bef-f21af31eab8e', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.5, x_pos=-1.0, y_pos=0.01): {Node(uuid='54cf8097-6b21-4437-87c8-f7f01c539274', name='PC 2', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=-0.5, y_pos=0.01): {}},\n",
       "  Node(uuid='54cf8097-6b21-4437-87c8-f7f01c539274', name='PC 2', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=-0.5, y_pos=0.01): {Node(uuid='688308b1-8c7b-4fc0-9bef-f21af31eab8e', name='PC 1', high_value_node=False, entry_node=True, vulnerability=0.5, x_pos=-1.0, y_pos=0.01): {},\n",
       "   Node(uuid='37d7fb0d-5f6c-4abe-892f-49fbf75e37eb', name='PC 3', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=0.0, y_pos=0.01): {}},\n",
       "  Node(uuid='37d7fb0d-5f6c-4abe-892f-49fbf75e37eb', name='PC 3', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=0.0, y_pos=0.01): {Node(uuid='54cf8097-6b21-4437-87c8-f7f01c539274', name='PC 2', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=-0.5, y_pos=0.01): {},\n",
       "   Node(uuid='3f4cc577-52b8-45c7-8159-3354ef0a4fee', name='PC 4', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=0.5, y_pos=0.01): {}},\n",
       "  Node(uuid='3f4cc577-52b8-45c7-8159-3354ef0a4fee', name='PC 4', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=0.5, y_pos=0.01): {Node(uuid='37d7fb0d-5f6c-4abe-892f-49fbf75e37eb', name='PC 3', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=0.0, y_pos=0.01): {},\n",
       "   Node(uuid='27893026-be4c-4120-a82b-833e3eb4abed', name='PC 5', high_value_node=True, entry_node=False, vulnerability=0.5, x_pos=1.0, y_pos=0.01): {}},\n",
       "  Node(uuid='27893026-be4c-4120-a82b-833e3eb4abed', name='PC 5', high_value_node=True, entry_node=False, vulnerability=0.5, x_pos=1.0, y_pos=0.01): {Node(uuid='3f4cc577-52b8-45c7-8159-3354ef0a4fee', name='PC 4', high_value_node=False, entry_node=False, vulnerability=0.5, x_pos=0.5, y_pos=0.01): {}}},\n",
       " 'current_step': 2,\n",
       " 'red_info': {0: {'Action': 'do_nothing',\n",
       "   'Attacking_Nodes': [],\n",
       "   'Target_Nodes': [],\n",
       "   'Successes': [True]}},\n",
       " 'post_red_state': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0},\n",
       " 'post_red_blue_view': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0},\n",
       " 'post_red_vulnerabilities': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0.5,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0.5,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0.5,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0.5,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0.09999999999999998},\n",
       " 'post_red_isolation': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': False,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': False,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': False,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': False,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': False},\n",
       " 'post_red_red_location': None,\n",
       " 'end_blue_view': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0},\n",
       " 'end_state': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0},\n",
       " 'final_vulnerabilities': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': 0.5,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': 0.5,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': 0.5,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': 0.5,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': 0.01},\n",
       " 'final_red_location': None,\n",
       " 'safe_nodes': 5,\n",
       " 'blue_action': 'reduce_vulnerability',\n",
       " 'blue_node': Node(uuid='27893026-be4c-4120-a82b-833e3eb4abed', name='PC 5', high_value_node=True, entry_node=False, vulnerability=0.5, x_pos=1.0, y_pos=0.01),\n",
       " 'attacks': [],\n",
       " 'end_isolation': {'688308b1-8c7b-4fc0-9bef-f21af31eab8e': False,\n",
       "  '54cf8097-6b21-4437-87c8-f7f01c539274': False,\n",
       "  '37d7fb0d-5f6c-4abe-892f-49fbf75e37eb': False,\n",
       "  '3f4cc577-52b8-45c7-8159-3354ef0a4fee': False,\n",
       "  '27893026-be4c-4120-a82b-833e3eb4abed': False}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_results[1]['info'][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
